{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":59094,"databundleVersionId":7010844,"sourceType":"competition"},{"sourceId":7190562,"sourceType":"datasetVersion","datasetId":4157666}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils import clip_grad_norm_\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer\nimport json","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-25T10:10:18.529569Z","iopub.execute_input":"2024-04-25T10:10:18.529847Z","iopub.status.idle":"2024-04-25T10:10:24.250637Z","shell.execute_reply.started":"2024-04-25T10:10:18.529823Z","shell.execute_reply":"2024-04-25T10:10:24.249641Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Seed Everything","metadata":{}},{"cell_type":"code","source":"def seed_everything():\n    seed = 42\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    print('-----Seed Set!-----') ","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:10:24.252278Z","iopub.execute_input":"2024-04-25T10:10:24.252689Z","iopub.status.idle":"2024-04-25T10:10:24.259052Z","shell.execute_reply.started":"2024-04-25T10:10:24.252663Z","shell.execute_reply":"2024-04-25T10:10:24.258058Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"seed_everything()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:10:24.260057Z","iopub.execute_input":"2024-04-25T10:10:24.260293Z","iopub.status.idle":"2024-04-25T10:10:24.294826Z","shell.execute_reply.started":"2024-04-25T10:10:24.260272Z","shell.execute_reply":"2024-04-25T10:10:24.293997Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"-----Seed Set!-----\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Download the pretrained ChemBERTa model","metadata":{}},{"cell_type":"markdown","source":"## Read Competition Datasets","metadata":{}},{"cell_type":"code","source":"de_train = pd.read_parquet('../input/open-problems-single-cell-perturbations/de_train.parquet')\nid_map = pd.read_csv('../input/open-problems-single-cell-perturbations/id_map.csv')\nsample_submission = pd.read_csv('../input/open-problems-single-cell-perturbations/sample_submission.csv', index_col='id')","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:10:24.297246Z","iopub.execute_input":"2024-04-25T10:10:24.297590Z","iopub.status.idle":"2024-04-25T10:10:29.539158Z","shell.execute_reply.started":"2024-04-25T10:10:24.297558Z","shell.execute_reply":"2024-04-25T10:10:29.538327Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"xlist  = ['cell_type','sm_name']\n_ylist = ['cell_type','sm_name','sm_lincs_id','SMILES','control']\n\ny = de_train.drop(columns=_ylist)\ny.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:10:29.540275Z","iopub.execute_input":"2024-04-25T10:10:29.540570Z","iopub.status.idle":"2024-04-25T10:10:29.586118Z","shell.execute_reply.started":"2024-04-25T10:10:29.540545Z","shell.execute_reply":"2024-04-25T10:10:29.585154Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(614, 18211)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Use Scikit-Learn's One Hot Encoder\nThis helps encode each pair (cell_type, sm_name) as a multi-dimensional binary vector","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder()\nencoder.fit(de_train[xlist])\none_hot_encode_features = encoder.transform(de_train[xlist])\none_hot_test = encoder.transform(id_map[xlist])\n\nX = pd.DataFrame(one_hot_encode_features.toarray().astype(float))\ntest = pd.DataFrame(one_hot_test.toarray().astype(float))","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:10:29.587409Z","iopub.execute_input":"2024-04-25T10:10:29.587775Z","iopub.status.idle":"2024-04-25T10:10:29.964904Z","shell.execute_reply.started":"2024-04-25T10:10:29.587742Z","shell.execute_reply":"2024-04-25T10:10:29.964140Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## ","metadata":{}},{"cell_type":"markdown","source":"## First Data Augmentation\nCompute the mean and std differential expression for each cell type and each small molecule name. Take this as additional input features to the model to be built.","metadata":{}},{"cell_type":"code","source":"de_cell_type = de_train.iloc[:, [0] + list(range(5, de_train.shape[1]))]\nde_sm_name = de_train.iloc[:, [1] + list(range(5, de_train.shape[1]))]\nmean_cell_type = de_cell_type.groupby('cell_type').mean().reset_index()\nmean_sm_name = de_sm_name.groupby('sm_name').mean().reset_index()\n\nstd_cell_type = de_cell_type.groupby('cell_type').std().reset_index()\nstd_sm_name = de_sm_name.groupby('sm_name').std().reset_index()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:10:29.965994Z","iopub.execute_input":"2024-04-25T10:10:29.966292Z","iopub.status.idle":"2024-04-25T10:10:30.721257Z","shell.execute_reply.started":"2024-04-25T10:10:29.966267Z","shell.execute_reply":"2024-04-25T10:10:30.720089Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"I also consider the 25%, 50%, and 75% percentiles (see below). Not that I explored different combinations of these features. In particular, when using all additional features (i.e., mean, std, 25%, 50%, and 75% percentiles) I refer to the corresponding models as \"heavy\".","metadata":{}},{"cell_type":"code","source":"cell_types = de_cell_type.groupby('cell_type').quantile(0.1).reset_index()['cell_type']\ndesc_cell_type = pd.concat([pd.DataFrame(cell_types)]+[de_cell_type.groupby('cell_type')[col]\\\n.quantile([0.25, 0.50, 0.75], interpolation='linear').unstack().reset_index(drop=True) for col in list(de_train.columns)[5:]], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:10:30.722517Z","iopub.execute_input":"2024-04-25T10:10:30.722820Z","iopub.status.idle":"2024-04-25T10:11:04.719545Z","shell.execute_reply.started":"2024-04-25T10:10:30.722793Z","shell.execute_reply":"2024-04-25T10:11:04.718495Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"sm_name2smiles = {smname:smiles for smname, smiles in zip(de_train['sm_name'], de_train['SMILES'])}\ntest_smiles = list(map(sm_name2smiles.get, id_map['sm_name'].values))","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:11:04.721341Z","iopub.execute_input":"2024-04-25T10:11:04.721643Z","iopub.status.idle":"2024-04-25T10:11:04.727371Z","shell.execute_reply.started":"2024-04-25T10:11:04.721620Z","shell.execute_reply":"2024-04-25T10:11:04.726346Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# ChemBERTa Features\nThanks to ALEKSEY TREPETSKY (I upvoted) https://www.kaggle.com/code/alekseytrepetsky/create-chemberta-embed/notebook, I could either build my own ChemBERTa features or use the ones she/he has created and shared publicly.","metadata":{}},{"cell_type":"code","source":"def build_ChemBERTa_features(smiles_list):\n    chemberta = AutoModelForMaskedLM.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n    tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n    chemberta.eval()\n    embeddings = torch.zeros(len(smiles_list), 600)\n    embeddings_mean = torch.zeros(len(smiles_list), 600)\n\n    with torch.no_grad():\n        for i, smiles in enumerate(tqdm(smiles_list)):\n            encoded_input = tokenizer(smiles, return_tensors=\"pt\", padding=False, truncation=True)\n            model_output = chemberta(**encoded_input)\n            \n            embedding = model_output[0][::,0,::]\n            embeddings[i] = embedding\n            \n            embedding = torch.mean(model_output[0], 1)\n            embeddings_mean[i] = embedding\n            \n    return embeddings.numpy(), embeddings_mean.numpy()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:11:04.731186Z","iopub.execute_input":"2024-04-25T10:11:04.731506Z","iopub.status.idle":"2024-04-25T10:11:04.739648Z","shell.execute_reply.started":"2024-04-25T10:11:04.731475Z","shell.execute_reply":"2024-04-25T10:11:04.738782Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_chem_feat, train_chem_feat_mean = build_ChemBERTa_features(de_train.SMILES)\ntest_chem_feat, test_chem_feat_mean = build_ChemBERTa_features(test_smiles)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:11:04.740854Z","iopub.execute_input":"2024-04-25T10:11:04.741248Z","iopub.status.idle":"2024-04-25T10:11:17.076910Z","shell.execute_reply.started":"2024-04-25T10:11:04.741215Z","shell.execute_reply":"2024-04-25T10:11:17.075993Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/17.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"319604ff20fa4a7eae680fd7d93f6308"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/14.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b493f3f9401b4411bb7acc4d1957bb3f"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MTR and are newly initialized: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/1.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3d705f6576548ba9de1b511d2477b4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading vocab.json:   0%|          | 0.00/6.96k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b5493c3f52146c59236da16d0f420b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading merges.txt:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c7fd9b8f7c24a488154395fad07fd0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/8.26k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af6ad3b004b14b14bcb47a43fe731180"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading added_tokens.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abed7356c9f947619bd73c38a5c851a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/420 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec33c90b8379476a8f3ae46402ef1592"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n100%|██████████| 614/614 [00:05<00:00, 120.11it/s]\nSome weights of RobertaForMaskedLM were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MTR and are newly initialized: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n100%|██████████| 255/255 [00:01<00:00, 136.60it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Now Define the Function to Combine the Created Features in Different Ways\nFor each combination, we create and train 3 deep learning architectures (see below)","metadata":{}},{"cell_type":"code","source":"def combine_features(data_aug_dfs, chem_feats, main_df, one_hot_dfs=None, quantiles_df=None):\n    \"\"\"\n    This function concatenates the provided vectors, matrices and data frames (i.e., one hot, std, mean, etc) into a single long vector. This is done for each pair (cell_type, sm_name)\n    \"\"\"\n    new_vecs = []\n    chem_feat_dim = 600\n    if len(data_aug_dfs) > 0:\n        add_len = sum(aug_df.shape[1]-1 for aug_df in data_aug_dfs)+chem_feat_dim*len(chem_feats)+one_hot_dfs.shape[1] if\\\n        one_hot_dfs is not None else sum(aug_df.shape[1]-1 for aug_df in data_aug_dfs)+chem_feat_dim*len(chem_feats)\n    else:\n        add_len = chem_feat_dim*len(chem_feats)+one_hot_dfs.shape[1] if\\\n        one_hot_dfs is not None else chem_feat_dim*len(chem_feats)\n    if quantiles_df is not None:\n        add_len += (quantiles_df.shape[1]-1)//3\n    for i in range(len(main_df)):\n        if one_hot_dfs is not None:\n            vec_ = (one_hot_dfs.iloc[i,:].values).copy()\n        else:\n            vec_ = np.array([])\n        for df in data_aug_dfs:\n            if 'cell_type' in df.columns:\n                values = df[df['cell_type']==main_df.iloc[i]['cell_type']].values.squeeze()[1:].astype(float)\n                vec_ = np.concatenate([vec_, values])\n            else:\n                assert 'sm_name' in df.columns\n                values = df[df['sm_name']==main_df.iloc[i]['sm_name']].values.squeeze()[1:].astype(float)\n                vec_ = np.concatenate([vec_, values])\n        for chem_feat in chem_feats:\n            vec_ = np.concatenate([vec_, chem_feat[i]])\n        final_vec = np.concatenate([vec_,np.zeros(add_len-vec_.shape[0],)])\n        new_vecs.append(final_vec)\n    return np.stack(new_vecs, axis=0).astype(float).reshape(len(main_df), 1, add_len)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:11:17.078385Z","iopub.execute_input":"2024-04-25T10:11:17.079164Z","iopub.status.idle":"2024-04-25T10:11:17.091915Z","shell.execute_reply.started":"2024-04-25T10:11:17.079126Z","shell.execute_reply":"2024-04-25T10:11:17.090978Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"test_vec = combine_features([mean_cell_type,std_cell_type,mean_sm_name,std_sm_name],\\\n                   [test_chem_feat, test_chem_feat_mean], id_map, test)\ntest_vec_light = combine_features([mean_cell_type,mean_sm_name],\\\n                   [test_chem_feat, test_chem_feat_mean], id_map, test)\ntest_vec_heavy = combine_features([desc_cell_type,mean_cell_type,mean_sm_name],\\\n                   [test_chem_feat,test_chem_feat_mean], id_map, test, desc_cell_type)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:11:17.092941Z","iopub.execute_input":"2024-04-25T10:11:17.093214Z","iopub.status.idle":"2024-04-25T10:13:29.400319Z","shell.execute_reply.started":"2024-04-25T10:11:17.093191Z","shell.execute_reply":"2024-04-25T10:13:29.399286Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation Metric Function","metadata":{}},{"cell_type":"code","source":"def mrrmse_np(y_pred, y_true):\n    return np.sqrt(np.square(y_true - y_pred).mean(axis=1)).mean()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:13:29.401609Z","iopub.execute_input":"2024-04-25T10:13:29.401909Z","iopub.status.idle":"2024-04-25T10:13:29.406656Z","shell.execute_reply.started":"2024-04-25T10:13:29.401884Z","shell.execute_reply":"2024-04-25T10:13:29.405813Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Additional Loss Functions\nI discovered experimentally that by combining different loss functions (others defined in the models), we can achieve a better performance. In particular, I used the binary cross entropy loss to push each predicted value in the target to a value other than 0 (i.e., push the value to a strictly positive or negative value). This is motivated by the fact that several values in the target are close to zero, and I wanted to make sure models do not learn this naively. The rest of the loss functions are suited for regression tasks and used normally to enforce the predicted value to be close to the target.","metadata":{}},{"cell_type":"code","source":"class LogCoshLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, y_prime_t, y_t):\n        ey_t = (y_t - y_prime_t)/3 # divide by 3 to avoid numerical overflow in cosh\n        return torch.mean(torch.log(torch.cosh(ey_t + 1e-12)))","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:13:29.407840Z","iopub.execute_input":"2024-04-25T10:13:29.408112Z","iopub.status.idle":"2024-04-25T10:13:29.418568Z","shell.execute_reply.started":"2024-04-25T10:13:29.408089Z","shell.execute_reply":"2024-04-25T10:13:29.417759Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"dims_dict = {'conv': {'heavy': 13400, 'light': 4576, 'initial': 8992},\n                                    'rnn': {'linear': {'heavy': 99968, 'light': 24192, 'initial': 29568},\n                                           'input_shape': {'heavy': [779,142], 'light': [187,202], 'initial': [229,324]}\n                                           }}","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:13:29.419537Z","iopub.execute_input":"2024-04-25T10:13:29.419795Z","iopub.status.idle":"2024-04-25T10:13:29.430868Z","shell.execute_reply.started":"2024-04-25T10:13:29.419773Z","shell.execute_reply":"2024-04-25T10:13:29.430038Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"code","source":"class Conv(nn.Module):\n    def __init__(self, scheme):\n        super(Conv, self).__init__()\n        self.name = 'Conv'\n        self.conv_block = nn.Sequential(nn.Conv1d(1, 8, 5, stride=1, padding=0),\n                                        nn.Dropout(0.3),\n                                        nn.Conv1d(8, 8, 5, stride=1, padding=0),\n                                        nn.ReLU(),\n                                        nn.Conv1d(8, 16, 5, stride=2, padding=0),\n                                        nn.Dropout(0.3),\n                                        nn.AvgPool1d(11),\n                                        nn.Conv1d(16, 8, 3, stride=3, padding=0),\n                                        nn.Flatten())\n        self.scheme = scheme\n        self.linear = nn.Sequential(\n                nn.Linear(dims_dict['conv'][self.scheme], 1024),\n                nn.Dropout(0.3),\n                nn.ReLU(),\n                nn.Linear(1024, 512),\n                nn.Dropout(0.3),\n                nn.ReLU())\n        self.head1 = nn.Linear(512, 18211)\n        \n        self.loss1 = nn.MSELoss()\n        self.loss2 = LogCoshLoss()\n        self.loss3 = nn.L1Loss()\n        self.loss4 = nn.BCELoss()\n        \n    def forward(self, x, y=None):\n        if y is None:\n            out = self.conv_block(x)\n            out = self.head1(self.linear(out))\n            return out\n        else:\n            out = self.conv_block(x)\n            out = self.head1(self.linear(out))\n            loss1 = 0.4*self.loss1(out, y) + 0.3*self.loss2(out, y) + 0.3*self.loss3(out, y)\n            yhat = torch.sigmoid(out)\n            yy = torch.sigmoid(y)\n            loss2 = self.loss4(yhat, yy)\n            return 0.8*loss1 + 0.2*loss2\n        \n\nclass LSTM(nn.Module):\n    def __init__(self, scheme):\n        super(LSTM, self).__init__()\n        self.name = 'LSTM'\n        self.scheme = scheme\n        self.lstm = nn.LSTM(dims_dict['rnn']['input_shape'][self.scheme][1], 128, num_layers=2, batch_first=True)\n        self.linear = nn.Sequential(\n            nn.Linear(dims_dict['rnn']['linear'][self.scheme], 1024),\n            nn.Dropout(0.3),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.Dropout(0.3),\n            nn.ReLU())\n        self.head1 = nn.Linear(512, 18211)\n        \n        self.loss1 = nn.MSELoss()\n        self.loss2 = LogCoshLoss()\n        self.loss3 = nn.L1Loss()\n        self.loss4 = nn.BCELoss()\n        \n    def forward(self, x, y=None):\n        shape1, shape2 = dims_dict['rnn']['input_shape'][self.scheme]\n        x = x.reshape(x.shape[0],shape1,shape2)\n        if y is None:\n            out, (hn, cn) = self.lstm(x)\n            out = out.reshape(out.shape[0],-1)\n            out = torch.cat([out, hn.reshape(hn.shape[1], -1)], dim=1)\n            out = self.head1(self.linear(out))\n            return out\n        else:\n            out, (hn, cn) = self.lstm(x)\n            out = out.reshape(out.shape[0],-1)\n            out = torch.cat([out, hn.reshape(hn.shape[1], -1)], dim=1)\n            out = self.head1(self.linear(out))\n            loss1 = 0.4*self.loss1(out, y) + 0.3*self.loss2(out, y) + 0.3*self.loss3(out, y)\n            yhat = torch.sigmoid(out)\n            yy = torch.sigmoid(y)\n            loss2 = self.loss4(yhat, yy)\n            return 0.8*loss1 + 0.2*loss2\n        \n        \nclass GRU(nn.Module):\n    def __init__(self, scheme):\n        super(GRU, self).__init__()\n        self.name = 'GRU'\n        self.scheme = scheme\n        self.gru = nn.GRU(dims_dict['rnn']['input_shape'][self.scheme][1], 128, num_layers=2, batch_first=True)\n        self.linear = nn.Sequential(\n            nn.Linear(dims_dict['rnn']['linear'][self.scheme], 1024),\n            nn.Dropout(0.3),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.Dropout(0.3),\n            nn.ReLU())\n        self.head1 = nn.Linear(512, 18211)\n        \n        self.loss1 = nn.MSELoss()\n        self.loss2 = LogCoshLoss()\n        self.loss3 = nn.L1Loss()\n        self.loss4 = nn.BCELoss()\n        \n    def forward(self, x, y=None):\n        shape1, shape2 = dims_dict['rnn']['input_shape'][self.scheme]\n        x = x.reshape(x.shape[0],shape1,shape2)\n        if y is None:\n            out, hn = self.gru(x)\n            out = out.reshape(out.shape[0],-1)\n            out = torch.cat([out, hn.reshape(hn.shape[1], -1)], dim=1)\n            out = self.head1(self.linear(out))\n            return out\n        else:\n            out, hn = self.gru(x)\n            out = out.reshape(out.shape[0],-1)\n            out = torch.cat([out, hn.reshape(hn.shape[1], -1)], dim=1)\n            out = self.head1(self.linear(out))\n            loss1 = 0.4*self.loss1(out, y) + 0.3*self.loss2(out, y) + 0.3*self.loss3(out, y)\n            yhat = torch.sigmoid(out)\n            yy = torch.sigmoid(y)\n            loss2 = self.loss4(yhat, yy)\n            return 0.8*loss1 + 0.2*loss2","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:13:29.432183Z","iopub.execute_input":"2024-04-25T10:13:29.432447Z","iopub.status.idle":"2024-04-25T10:13:29.462000Z","shell.execute_reply.started":"2024-04-25T10:13:29.432424Z","shell.execute_reply":"2024-04-25T10:13:29.461279Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Create Dataset Class","metadata":{}},{"cell_type":"code","source":"class Dataset:\n    def __init__(self, data_x, data_y=None):\n        super(Dataset, self).__init__()\n        self.data_x = data_x\n        self.data_y = data_y\n\n    def __len__(self):\n        return len(self.data_x)\n    \n    def __getitem__(self, idx):\n        if self.data_y is not None:\n            return self.data_x[idx], self.data_y[idx]\n        else:\n            return self.data_x[idx]","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:13:29.463086Z","iopub.execute_input":"2024-04-25T10:13:29.463415Z","iopub.status.idle":"2024-04-25T10:13:29.475953Z","shell.execute_reply.started":"2024-04-25T10:13:29.463385Z","shell.execute_reply":"2024-04-25T10:13:29.475098Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Define 2nd Data Augmentation Function\nIn the following function, we augment the training data by randomly dropping 30% of our 1-dimensional input feature vectors' entries. Input features are of shape (batch, 1, d)","metadata":{}},{"cell_type":"code","source":"import random\ndef augment_data(x_, y_):\n    copy_x = x_.copy()\n    new_x = []\n    new_y = y_.copy()\n    dim = x_.shape[2]\n    k = int(0.3*dim)\n    for i in range(x_.shape[0]):\n        idx = random.sample(range(dim), k=k)\n        copy_x[i,:,idx] = 0\n        new_x.append(copy_x[i])\n    return np.stack(new_x, axis=0), new_y","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:13:29.476869Z","iopub.execute_input":"2024-04-25T10:13:29.477332Z","iopub.status.idle":"2024-04-25T10:13:29.489839Z","shell.execute_reply.started":"2024-04-25T10:13:29.477308Z","shell.execute_reply":"2024-04-25T10:13:29.488980Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Define Helper and Main Training Functions\nGRU experienced numerical overflow with a learning rate of 0.001, so I used 0.0003 instead","metadata":{}},{"cell_type":"code","source":"def train_step(dataloader, model, opt, clip_norm):\n    model.train()\n    train_losses = []\n    for x, target in dataloader:\n        if torch.cuda.is_available():\n            model.cuda()\n            x = x.cuda()\n            target = target.cuda()\n        loss = model(x, target)\n        train_losses.append(loss.item())\n        opt.zero_grad()\n        loss.backward()\n        clip_grad_norm_(model.parameters(), clip_norm)\n        opt.step()\n    return np.mean(train_losses)\n\ndef validation_step(dataloader, model):\n    model.eval()\n    val_losses = []\n    val_mrrmse = []\n    for x, target in dataloader:\n        if torch.cuda.is_available():\n            model.cuda()\n            x = x.cuda()\n            target = target.cuda()\n        loss = model(x,target)\n        pred = model(x).detach().cpu().numpy()\n        val_mrrmse.append(mrrmse_np(pred, target.cpu().numpy()))\n        val_losses.append(loss.item())\n    return np.mean(val_losses), np.mean(val_mrrmse)\n\n\ndef train_function(model, x_train, y_train, x_val, y_val, epochs=20, clip_norm=1.0):\n    if model.name in ['GRU']:\n        print('lr', 0.0003)\n        opt = torch.optim.Adam(model.parameters(), lr=0.0003)\n    else:\n        opt = torch.optim.Adam(model.parameters(), lr=0.001)\n    model.cuda()\n    x_train_aug, y_train_aug = augment_data(x_train, y_train)\n    x_train_aug = np.concatenate([x_train, x_train_aug], axis=0)\n    y_train_aug = np.concatenate([y_train, y_train_aug], axis=0)\n    data_x_train = torch.FloatTensor(x_train_aug)\n    data_y_train = torch.FloatTensor(y_train_aug)\n    data_x_val = torch.FloatTensor(x_val)\n    data_y_val = torch.FloatTensor(y_val)\n    train_dataloader = DataLoader(Dataset(data_x_train, data_y_train), num_workers=4, batch_size=16, shuffle=True)\n    val_dataloader = DataLoader(Dataset(data_x_val, data_y_val), num_workers=4, batch_size=32, shuffle=False)\n    best_loss = np.inf\n    best_weights = None\n    train_losses = []\n    val_losses = []\n    for e in range(epochs):\n        loss = train_step(train_dataloader, model, opt, clip_norm)\n        val_losses.append(loss.item())\n        val_loss, val_mrrmse = validation_step(val_dataloader, model)\n        if val_mrrmse < best_loss:\n            best_loss = val_mrrmse\n            best_weights = model.state_dict()\n            print('BEST ----> ')\n        print(f\"{model.name} Epoch {e}, train_loss {round(loss,3)}, val_loss {round(val_loss, 3)}, val_mrrmse {val_mrrmse}\")\n    model.load_state_dict(best_weights)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:13:29.491134Z","iopub.execute_input":"2024-04-25T10:13:29.491468Z","iopub.status.idle":"2024-04-25T10:13:29.507234Z","shell.execute_reply.started":"2024-04-25T10:13:29.491438Z","shell.execute_reply":"2024-04-25T10:13:29.506304Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold as KF\nsplits = 5\nkf_cv = KF(n_splits=splits, shuffle=True, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:13:29.508423Z","iopub.execute_input":"2024-04-25T10:13:29.509271Z","iopub.status.idle":"2024-04-25T10:13:29.599584Z","shell.execute_reply.started":"2024-04-25T10:13:29.509238Z","shell.execute_reply":"2024-04-25T10:13:29.598641Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def cross_validate_models(X, y, epochs=120, scheme='initial', clip_norm=1.0):\n    trained_models = []\n    for i, (train_idx,val_idx) in enumerate(kf_cv.split(X)):\n        print(f\"\\nSplit {i+1}/{splits}...\")\n        x_train, x_val = X[train_idx], X[val_idx]\n        y_train, y_val = y.values[train_idx], y.values[val_idx]\n        for Model in [LSTM, Conv, GRU]:\n            model = Model(scheme)\n            model = train_function(model, x_train, y_train, x_val, y_val, epochs=epochs, clip_norm=clip_norm)\n            model.to('cpu')\n            trained_models.append(model)\n            torch.cuda.empty_cache()\n            torch.save(model.state_dict(), f'pytorch_{model.name}_{scheme}_fold{i}.pt')\n    return trained_models","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:13:29.600694Z","iopub.execute_input":"2024-04-25T10:13:29.601040Z","iopub.status.idle":"2024-04-25T10:13:29.608790Z","shell.execute_reply.started":"2024-04-25T10:13:29.601007Z","shell.execute_reply":"2024-04-25T10:13:29.607908Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Define Inference Functions","metadata":{}},{"cell_type":"code","source":"def inference_pytorch(model, dataloader):\n    model.eval()\n    preds = []\n    for x in dataloader:\n        if torch.cuda.is_available():\n            model.cuda()\n            x = x.cuda()\n        pred = model(x).detach().cpu().numpy()\n        preds.append(pred)\n    model.to('cpu')\n    torch.cuda.empty_cache()\n    return np.concatenate(preds, axis=0)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:13:29.609865Z","iopub.execute_input":"2024-04-25T10:13:29.610197Z","iopub.status.idle":"2024-04-25T10:13:29.619050Z","shell.execute_reply.started":"2024-04-25T10:13:29.610166Z","shell.execute_reply":"2024-04-25T10:13:29.618204Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def average_prediction(X_test, trained_models):\n    all_preds = []\n    test_dataloader = DataLoader(Dataset(torch.FloatTensor(X_test)), num_workers=4, batch_size=64, shuffle=False)\n    for i,model in enumerate(trained_models):\n        #if model.name == \"Conv\": continue\n        current_pred = inference_pytorch(model, test_dataloader)\n        all_preds.append(current_pred)\n    return np.stack(all_preds, axis=1).mean(axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:13:29.620094Z","iopub.execute_input":"2024-04-25T10:13:29.620338Z","iopub.status.idle":"2024-04-25T10:13:29.631911Z","shell.execute_reply.started":"2024-04-25T10:13:29.620317Z","shell.execute_reply":"2024-04-25T10:13:29.630970Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def weighted_average_prediction(X_test, trained_models, model_wise=[0.25, 0.35, 0.40], fold_wise=None):\n    all_preds = []\n    test_dataloader = DataLoader(Dataset(torch.FloatTensor(X_test)), num_workers=4, batch_size=64, shuffle=False)\n    for i,model in enumerate(trained_models):\n        current_pred = inference_pytorch(model, test_dataloader)\n        current_pred = model_wise[i%3]*current_pred\n        if fold_wise:\n            current_pred = fold_wise[i//3]*current_pred\n        all_preds.append(current_pred)\n    return np.stack(all_preds, axis=1).sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:13:29.633052Z","iopub.execute_input":"2024-04-25T10:13:29.633315Z","iopub.status.idle":"2024-04-25T10:13:29.641831Z","shell.execute_reply.started":"2024-04-25T10:13:29.633293Z","shell.execute_reply":"2024-04-25T10:13:29.640995Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def reproduce(epochs=1):\n    trained_models = {'initial': [], 'light': [], 'heavy': []}\n    for scheme, clip_norm, input_features in zip(['initial', 'light', 'heavy'], [5.0, 1.0, 1.0], [X_vec, X_vec_light, X_vec_heavy]):\n        seed_everything()\n        models = cross_validate_models(input_features, y, epochs=epochs, scheme=scheme, clip_norm=clip_norm)\n        trained_models[scheme].extend(models)\n    return trained_models","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:13:29.643070Z","iopub.execute_input":"2024-04-25T10:13:29.643733Z","iopub.status.idle":"2024-04-25T10:13:29.651880Z","shell.execute_reply.started":"2024-04-25T10:13:29.643699Z","shell.execute_reply":"2024-04-25T10:13:29.651090Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def load_trained_models(path=\"/kaggle/input/best-models-single-cell/\", kf_n_splits=5):\n    trained_models = {'initial': [], 'light': [], 'heavy': []}\n    for scheme in ['initial', 'light', 'heavy']:\n        for fold in range(kf_n_splits):\n            for Model in [LSTM, Conv, GRU]:\n                model = Model(scheme)\n                for weights_path in os.listdir(path):\n                    if model.name in weights_path and scheme in weights_path and f'fold{fold}' in weights_path:\n                        model.load_state_dict(torch.load(f'{path}{weights_path}', map_location='cpu'))\n                        trained_models[scheme].append(model)\n    return trained_models","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:13:29.652993Z","iopub.execute_input":"2024-04-25T10:13:29.653846Z","iopub.status.idle":"2024-04-25T10:13:29.661879Z","shell.execute_reply.started":"2024-04-25T10:13:29.653821Z","shell.execute_reply":"2024-04-25T10:13:29.661001Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"trained_models = load_trained_models()#reproduce(epochs=250)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:13:29.667654Z","iopub.execute_input":"2024-04-25T10:13:29.667920Z","iopub.status.idle":"2024-04-25T10:14:42.638338Z","shell.execute_reply.started":"2024-04-25T10:13:29.667891Z","shell.execute_reply":"2024-04-25T10:14:42.637276Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"model_weights = [0.29, 0.33, 0.38]\nfold_weights = [0.25, 0.15, 0.2, 0.15, 0.25]","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:14:42.639608Z","iopub.execute_input":"2024-04-25T10:14:42.640001Z","iopub.status.idle":"2024-04-25T10:14:42.645161Z","shell.execute_reply.started":"2024-04-25T10:14:42.639966Z","shell.execute_reply":"2024-04-25T10:14:42.644082Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"pred1 = average_prediction(test_vec_light, trained_models['light'])\npred2 = weighted_average_prediction(test_vec_light, trained_models['light'],\\\n                                        model_wise=model_weights, fold_wise=fold_weights)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:14:42.646217Z","iopub.execute_input":"2024-04-25T10:14:42.646480Z","iopub.status.idle":"2024-04-25T10:15:08.947587Z","shell.execute_reply.started":"2024-04-25T10:14:42.646458Z","shell.execute_reply":"2024-04-25T10:15:08.946452Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"pred3 = average_prediction(test_vec, trained_models['initial'])\npred4 = weighted_average_prediction(test_vec, trained_models['initial'],\\\n                                        model_wise=model_weights, fold_wise=fold_weights)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:15:08.949419Z","iopub.execute_input":"2024-04-25T10:15:08.950294Z","iopub.status.idle":"2024-04-25T10:15:37.355214Z","shell.execute_reply.started":"2024-04-25T10:15:08.950250Z","shell.execute_reply":"2024-04-25T10:15:37.353972Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"pred5 = average_prediction(test_vec_heavy, trained_models['heavy'])\npred6 = weighted_average_prediction(test_vec_heavy, trained_models['heavy'],\\\n                                    model_wise=model_weights, fold_wise=fold_weights)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:15:37.356943Z","iopub.execute_input":"2024-04-25T10:15:37.357830Z","iopub.status.idle":"2024-04-25T10:16:14.921540Z","shell.execute_reply.started":"2024-04-25T10:15:37.357790Z","shell.execute_reply":"2024-04-25T10:16:14.920229Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## Read Submission Sample File","metadata":{}},{"cell_type":"code","source":"col = list(de_train.columns[5:])\nsubmission = sample_submission.copy()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:16:14.923205Z","iopub.execute_input":"2024-04-25T10:16:14.923545Z","iopub.status.idle":"2024-04-25T10:16:14.939571Z","shell.execute_reply.started":"2024-04-25T10:16:14.923516Z","shell.execute_reply":"2024-04-25T10:16:14.938588Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## Ensemble Prediction","metadata":{}},{"cell_type":"code","source":"submission[col] = 0.23*pred1 + 0.15*pred2 + 0.18*pred3 + 0.15*pred4 + 0.15*pred5 + 0.14*pred6\ndf1 = submission.copy()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:16:14.940799Z","iopub.execute_input":"2024-04-25T10:16:14.941090Z","iopub.status.idle":"2024-04-25T10:16:21.897124Z","shell.execute_reply.started":"2024-04-25T10:16:14.941067Z","shell.execute_reply":"2024-04-25T10:16:21.896327Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"submission[col] =  0.13*pred1 + 0.15*pred2 + 0.23*pred3 + 0.15*pred4 + 0.20*pred5 + 0.14*pred6\ndf2 = submission.copy()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:16:21.898310Z","iopub.execute_input":"2024-04-25T10:16:21.898611Z","iopub.status.idle":"2024-04-25T10:16:30.222416Z","shell.execute_reply.started":"2024-04-25T10:16:21.898586Z","shell.execute_reply":"2024-04-25T10:16:30.221595Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"submission[col] = 0.17*pred1 + 0.16*pred2 + 0.17*pred3 + 0.16*pred4 + 0.18*pred5 + 0.16*pred6\ndf3 = submission.copy()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:16:30.223593Z","iopub.execute_input":"2024-04-25T10:16:30.223880Z","iopub.status.idle":"2024-04-25T10:16:38.208017Z","shell.execute_reply.started":"2024-04-25T10:16:30.223856Z","shell.execute_reply":"2024-04-25T10:16:38.207209Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"df_sub = 0.34*df1 + 0.33*df2 + 0.33*df3","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:16:38.209261Z","iopub.execute_input":"2024-04-25T10:16:38.209922Z","iopub.status.idle":"2024-04-25T10:16:38.242722Z","shell.execute_reply.started":"2024-04-25T10:16:38.209883Z","shell.execute_reply":"2024-04-25T10:16:38.241912Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"## Save Submission Dataframe","metadata":{}},{"cell_type":"code","source":"df_sub","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:16:38.243971Z","iopub.execute_input":"2024-04-25T10:16:38.244557Z","iopub.status.idle":"2024-04-25T10:16:38.281046Z","shell.execute_reply.started":"2024-04-25T10:16:38.244523Z","shell.execute_reply":"2024-04-25T10:16:38.280216Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"         A1BG  A1BG-AS1       A2M   A2M-AS1     A2MP1    A4GALT      AAAS  \\\nid                                                                          \n0    0.004891  0.257333  0.350504  0.783664  0.991103  0.424397 -0.072302   \n1    0.086987  0.100221  0.116760  0.225081  0.657403  0.378152 -0.036609   \n2    0.570574  0.200264  0.552647  0.946792  2.579547  1.750001 -0.010679   \n3    0.027916  0.101314  0.084691  0.190825  0.442343  0.189132 -0.066829   \n4    0.011895  0.100863  0.114376  0.293312  0.614760  0.323282 -0.060771   \n..        ...       ...       ...       ...       ...       ...       ...   \n250  0.071806 -0.015070 -0.270762 -0.305674  0.309278  0.096855  0.105902   \n251  0.353880  0.058280  0.146316  0.051660  2.291123  0.885869  0.209340   \n252  0.070027  0.009405 -0.093254 -0.231917  0.406057  0.149935  0.069441   \n253  0.769563  1.120203 -4.138731  0.050159  3.966650  2.874411  0.706871   \n254  0.202520 -0.029769 -1.635844 -0.481743  0.906814  0.248161  0.162045   \n\n         AACS     AAGAB      AAK1  ...      ZUP1      ZW10    ZWILCH  \\\nid                                 ...                                 \n0    0.096655 -0.235352  0.644939  ... -1.231125 -0.238978 -0.240077   \n1    0.145430  0.148688  0.112047  ... -0.192942 -0.010191 -0.171862   \n2    0.386336  0.392121  0.505168  ... -0.422660  0.097118 -0.036983   \n3    0.086405  0.063511  0.107463  ... -0.258952 -0.022202 -0.199640   \n4    0.133719  0.081242  0.245043  ... -0.485432 -0.061439 -0.269130   \n..        ...       ...       ...  ...       ...       ...       ...   \n250  0.133263  0.088458  0.152961  ...  0.041829 -0.111343 -0.377440   \n251 -0.039648  0.161889  0.259124  ... -0.476890 -0.376750 -0.120565   \n252  0.123252  0.105856  0.171478  ... -0.045171 -0.102367 -0.168883   \n253 -0.120767  0.327062  1.393089  ... -0.077082 -0.236828 -4.114982   \n254  0.115002  0.286919  0.429017  ...  0.036484 -0.084490 -0.118096   \n\n        ZWINT      ZXDA      ZXDB      ZXDC    ZYG11B       ZYX     ZZEF1  \nid                                                                         \n0    0.218761  0.024762  0.433813  0.391118  0.399233 -0.637794  0.134152  \n1    0.278123  0.139727  0.177368  0.199524  0.093824 -0.228720 -0.072083  \n2    0.778453  0.439742  0.460379  0.526051  0.231570 -0.365261  0.043398  \n3    0.189844  0.081758  0.166348  0.170276  0.114126 -0.152267 -0.062795  \n4    0.220721  0.064593  0.232333  0.234596  0.163050 -0.304614 -0.021434  \n..        ...       ...       ...       ...       ...       ...       ...  \n250 -0.090412  0.057765 -0.008004 -0.155536 -0.185297 -0.375359 -0.172615  \n251 -0.399312  0.457138  0.096086 -0.215959 -0.071947 -0.139824 -0.009215  \n252 -0.061508  0.109178  0.048182 -0.106359 -0.132385 -0.179049 -0.112557  \n253 -0.373702  0.234885 -0.191915 -1.683750 -0.063814  0.515996 -0.404022  \n254 -0.230267  0.223750  0.044718 -0.218275 -0.484879 -0.398333 -0.115411  \n\n[255 rows x 18211 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>A1BG</th>\n      <th>A1BG-AS1</th>\n      <th>A2M</th>\n      <th>A2M-AS1</th>\n      <th>A2MP1</th>\n      <th>A4GALT</th>\n      <th>AAAS</th>\n      <th>AACS</th>\n      <th>AAGAB</th>\n      <th>AAK1</th>\n      <th>...</th>\n      <th>ZUP1</th>\n      <th>ZW10</th>\n      <th>ZWILCH</th>\n      <th>ZWINT</th>\n      <th>ZXDA</th>\n      <th>ZXDB</th>\n      <th>ZXDC</th>\n      <th>ZYG11B</th>\n      <th>ZYX</th>\n      <th>ZZEF1</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.004891</td>\n      <td>0.257333</td>\n      <td>0.350504</td>\n      <td>0.783664</td>\n      <td>0.991103</td>\n      <td>0.424397</td>\n      <td>-0.072302</td>\n      <td>0.096655</td>\n      <td>-0.235352</td>\n      <td>0.644939</td>\n      <td>...</td>\n      <td>-1.231125</td>\n      <td>-0.238978</td>\n      <td>-0.240077</td>\n      <td>0.218761</td>\n      <td>0.024762</td>\n      <td>0.433813</td>\n      <td>0.391118</td>\n      <td>0.399233</td>\n      <td>-0.637794</td>\n      <td>0.134152</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.086987</td>\n      <td>0.100221</td>\n      <td>0.116760</td>\n      <td>0.225081</td>\n      <td>0.657403</td>\n      <td>0.378152</td>\n      <td>-0.036609</td>\n      <td>0.145430</td>\n      <td>0.148688</td>\n      <td>0.112047</td>\n      <td>...</td>\n      <td>-0.192942</td>\n      <td>-0.010191</td>\n      <td>-0.171862</td>\n      <td>0.278123</td>\n      <td>0.139727</td>\n      <td>0.177368</td>\n      <td>0.199524</td>\n      <td>0.093824</td>\n      <td>-0.228720</td>\n      <td>-0.072083</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.570574</td>\n      <td>0.200264</td>\n      <td>0.552647</td>\n      <td>0.946792</td>\n      <td>2.579547</td>\n      <td>1.750001</td>\n      <td>-0.010679</td>\n      <td>0.386336</td>\n      <td>0.392121</td>\n      <td>0.505168</td>\n      <td>...</td>\n      <td>-0.422660</td>\n      <td>0.097118</td>\n      <td>-0.036983</td>\n      <td>0.778453</td>\n      <td>0.439742</td>\n      <td>0.460379</td>\n      <td>0.526051</td>\n      <td>0.231570</td>\n      <td>-0.365261</td>\n      <td>0.043398</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.027916</td>\n      <td>0.101314</td>\n      <td>0.084691</td>\n      <td>0.190825</td>\n      <td>0.442343</td>\n      <td>0.189132</td>\n      <td>-0.066829</td>\n      <td>0.086405</td>\n      <td>0.063511</td>\n      <td>0.107463</td>\n      <td>...</td>\n      <td>-0.258952</td>\n      <td>-0.022202</td>\n      <td>-0.199640</td>\n      <td>0.189844</td>\n      <td>0.081758</td>\n      <td>0.166348</td>\n      <td>0.170276</td>\n      <td>0.114126</td>\n      <td>-0.152267</td>\n      <td>-0.062795</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.011895</td>\n      <td>0.100863</td>\n      <td>0.114376</td>\n      <td>0.293312</td>\n      <td>0.614760</td>\n      <td>0.323282</td>\n      <td>-0.060771</td>\n      <td>0.133719</td>\n      <td>0.081242</td>\n      <td>0.245043</td>\n      <td>...</td>\n      <td>-0.485432</td>\n      <td>-0.061439</td>\n      <td>-0.269130</td>\n      <td>0.220721</td>\n      <td>0.064593</td>\n      <td>0.232333</td>\n      <td>0.234596</td>\n      <td>0.163050</td>\n      <td>-0.304614</td>\n      <td>-0.021434</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>250</th>\n      <td>0.071806</td>\n      <td>-0.015070</td>\n      <td>-0.270762</td>\n      <td>-0.305674</td>\n      <td>0.309278</td>\n      <td>0.096855</td>\n      <td>0.105902</td>\n      <td>0.133263</td>\n      <td>0.088458</td>\n      <td>0.152961</td>\n      <td>...</td>\n      <td>0.041829</td>\n      <td>-0.111343</td>\n      <td>-0.377440</td>\n      <td>-0.090412</td>\n      <td>0.057765</td>\n      <td>-0.008004</td>\n      <td>-0.155536</td>\n      <td>-0.185297</td>\n      <td>-0.375359</td>\n      <td>-0.172615</td>\n    </tr>\n    <tr>\n      <th>251</th>\n      <td>0.353880</td>\n      <td>0.058280</td>\n      <td>0.146316</td>\n      <td>0.051660</td>\n      <td>2.291123</td>\n      <td>0.885869</td>\n      <td>0.209340</td>\n      <td>-0.039648</td>\n      <td>0.161889</td>\n      <td>0.259124</td>\n      <td>...</td>\n      <td>-0.476890</td>\n      <td>-0.376750</td>\n      <td>-0.120565</td>\n      <td>-0.399312</td>\n      <td>0.457138</td>\n      <td>0.096086</td>\n      <td>-0.215959</td>\n      <td>-0.071947</td>\n      <td>-0.139824</td>\n      <td>-0.009215</td>\n    </tr>\n    <tr>\n      <th>252</th>\n      <td>0.070027</td>\n      <td>0.009405</td>\n      <td>-0.093254</td>\n      <td>-0.231917</td>\n      <td>0.406057</td>\n      <td>0.149935</td>\n      <td>0.069441</td>\n      <td>0.123252</td>\n      <td>0.105856</td>\n      <td>0.171478</td>\n      <td>...</td>\n      <td>-0.045171</td>\n      <td>-0.102367</td>\n      <td>-0.168883</td>\n      <td>-0.061508</td>\n      <td>0.109178</td>\n      <td>0.048182</td>\n      <td>-0.106359</td>\n      <td>-0.132385</td>\n      <td>-0.179049</td>\n      <td>-0.112557</td>\n    </tr>\n    <tr>\n      <th>253</th>\n      <td>0.769563</td>\n      <td>1.120203</td>\n      <td>-4.138731</td>\n      <td>0.050159</td>\n      <td>3.966650</td>\n      <td>2.874411</td>\n      <td>0.706871</td>\n      <td>-0.120767</td>\n      <td>0.327062</td>\n      <td>1.393089</td>\n      <td>...</td>\n      <td>-0.077082</td>\n      <td>-0.236828</td>\n      <td>-4.114982</td>\n      <td>-0.373702</td>\n      <td>0.234885</td>\n      <td>-0.191915</td>\n      <td>-1.683750</td>\n      <td>-0.063814</td>\n      <td>0.515996</td>\n      <td>-0.404022</td>\n    </tr>\n    <tr>\n      <th>254</th>\n      <td>0.202520</td>\n      <td>-0.029769</td>\n      <td>-1.635844</td>\n      <td>-0.481743</td>\n      <td>0.906814</td>\n      <td>0.248161</td>\n      <td>0.162045</td>\n      <td>0.115002</td>\n      <td>0.286919</td>\n      <td>0.429017</td>\n      <td>...</td>\n      <td>0.036484</td>\n      <td>-0.084490</td>\n      <td>-0.118096</td>\n      <td>-0.230267</td>\n      <td>0.223750</td>\n      <td>0.044718</td>\n      <td>-0.218275</td>\n      <td>-0.484879</td>\n      <td>-0.398333</td>\n      <td>-0.115411</td>\n    </tr>\n  </tbody>\n</table>\n<p>255 rows × 18211 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_sub.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:18:52.510642Z","iopub.execute_input":"2024-04-25T10:18:52.511479Z","iopub.status.idle":"2024-04-25T10:19:03.661280Z","shell.execute_reply.started":"2024-04-25T10:18:52.511442Z","shell.execute_reply":"2024-04-25T10:19:03.660109Z"},"trusted":true},"execution_count":40,"outputs":[]}]}