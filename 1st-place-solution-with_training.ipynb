{"cells":[{"cell_type":"markdown","metadata":{},"source":["Reproduced first place solution from: https://www.kaggle.com/competitions/open-problems-single-cell-perturbations/leaderboard\n","\n","All credit goes to Jean!\n","\n","Steps involved here:\n","\n","1. Preprocessing:\n","a) ChemBERT --- for features of small molecules’ SMILES (taken from here: https://huggingface.co/DeepChem/ChemBERTa-77M-MTR)\n","b) Additional augmentation: mean, standard deviation, and (25%, 50%, 75%) percentiles of differential expressions per cell type and small molecule in the training\n","\n","Combined and validated produces following pre-processed data:\n","\n","“initial”: ChemBERTa embeddings, 1 hot encoding of cell_type/sm_name pairs, mean, std, percentiles of targets per cell_type and sm_name\n","\n","“light”: ChemBERTa embeddings, 1 hot encoding of cell_type/sm_name pairs, mean targets per cell_type and sm_name\n","\n","“heavy”: ChemBERTa embeddings, 1 hot encoding of cell_type/sm_name pairs, mean, 25%, 50%, 75% percentiles of targets per cell_type and sm_name\n","\n","This data was validated with 5 fold cross validation across folds. Where ''T regulatory cells’’, ''B cells’’, and ''Nk cells’’ were easiest to predict, while ''T cells CD8+’’ and ''Myeloid cells’’ are the hardest to predict\n","\n","2. Model: Weighted average between 3 \"simple\" trained models: LSTM, GRU, and 1-d CNN \n","\n","3. Adding noise and making it robust: Randomly replacing 30% of the input features’ entries with zeros, and adding the resulting input feature together with the correct target as a new training datapoint. This has proven to improve the predictive performance of models. In this sense, models are robust to the noise as their performance is not hindered but rather improved. The biological motivation here is that we might not need to know the complete chemical structure of a molecule (assuming the dropped input features are from sm_name) to know its impact on a cell:\n","\n","'>  def augment_data(x_, y_):\n",">        copy_x = x_.copy()\n",">        new_x = []\n",">        new_y = y_.copy()\n",">        dim = x_.shape[2]\n",">        k = int(0.3*dim)\n",">        for i in range(x_.shape[0]):\n",">           idx = random.sample(range(dim), k=k)\n",">           copy_x[i,:,idx] = 0\n",">           new_x.append(copy_x[i])\n",">       return np.stack(new_x, axis=0), new_y\n","\n","\n","4. Post processing (all credict goes here: https://www.kaggle.com/code/raki21/4th-place-magic-postprocessing)\n","\n","\n","5. Reproduced result on the leaderboard:"]},{"cell_type":"markdown","metadata":{},"source":["Things left to do:\n","\n","1. Abelation study \n","2. categories (genes, cell types, compounds) predicted well and predicted poorly\n","3. Viash PR (with both train and predict component)\n","4. Write it all up in overleaf"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-25T08:01:53.366680Z","iopub.status.busy":"2024-04-25T08:01:53.366406Z","iopub.status.idle":"2024-04-25T08:01:59.166294Z","shell.execute_reply":"2024-04-25T08:01:59.165427Z","shell.execute_reply.started":"2024-04-25T08:01:53.366655Z"},"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","import os\n","import random\n","import numpy as np \n","import pandas as pd\n","from tqdm import tqdm\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils import clip_grad_norm_\n","from torch.utils.data import DataLoader\n","from transformers import AutoModelForMaskedLM, AutoTokenizer\n","import json"]},{"cell_type":"markdown","metadata":{},"source":["## Seed Everything"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:01:59.168594Z","iopub.status.busy":"2024-04-25T08:01:59.168040Z","iopub.status.idle":"2024-04-25T08:01:59.175117Z","shell.execute_reply":"2024-04-25T08:01:59.174205Z","shell.execute_reply.started":"2024-04-25T08:01:59.168554Z"},"trusted":true},"outputs":[],"source":["def seed_everything():\n","    seed = 42\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","    print('-----Seed Set!-----') "]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:01:59.176561Z","iopub.status.busy":"2024-04-25T08:01:59.176135Z","iopub.status.idle":"2024-04-25T08:01:59.211955Z","shell.execute_reply":"2024-04-25T08:01:59.211103Z","shell.execute_reply.started":"2024-04-25T08:01:59.176536Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["-----Seed Set!-----\n"]}],"source":["seed_everything()"]},{"cell_type":"markdown","metadata":{},"source":["## Download the pretrained ChemBERTa model"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:01:59.214235Z","iopub.status.busy":"2024-04-25T08:01:59.213961Z","iopub.status.idle":"2024-04-25T08:02:01.672492Z","shell.execute_reply":"2024-04-25T08:02:01.671667Z","shell.execute_reply.started":"2024-04-25T08:01:59.214210Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"222842125f114dbe9d460d2a3ecbb7f1","version_major":2,"version_minor":0},"text/plain":["Downloading config.json:   0%|          | 0.00/17.7k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"617ca23da0f94e8a9130c73a3ee6bd63","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/14.0M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MTR and are newly initialized: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ddf30f1a7a764eda8e2631d85909eb5b","version_major":2,"version_minor":0},"text/plain":["Downloading tokenizer_config.json:   0%|          | 0.00/1.27k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c3498acee6984c44a711cc11114b4b81","version_major":2,"version_minor":0},"text/plain":["Downloading vocab.json:   0%|          | 0.00/6.96k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b808e59dc75043769ff21e2dd67d9508","version_major":2,"version_minor":0},"text/plain":["Downloading merges.txt:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a37fe89ef62049a7a98f6fc57a5d29b8","version_major":2,"version_minor":0},"text/plain":["Downloading tokenizer.json:   0%|          | 0.00/8.26k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4ed7f15938994a8cae8211632989ba23","version_major":2,"version_minor":0},"text/plain":["Downloading added_tokens.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4dda3c3920744e86bf2fbb9d3776195b","version_major":2,"version_minor":0},"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/420 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["chemberta = AutoModelForMaskedLM.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n","tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n","chemberta._modules[\"lm_head\"] = nn.Identity()"]},{"cell_type":"markdown","metadata":{},"source":["## Read Competition Datasets"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:02:01.673958Z","iopub.status.busy":"2024-04-25T08:02:01.673547Z","iopub.status.idle":"2024-04-25T08:02:07.441042Z","shell.execute_reply":"2024-04-25T08:02:07.440270Z","shell.execute_reply.started":"2024-04-25T08:02:01.673933Z"},"trusted":true},"outputs":[],"source":["de_train = pd.read_parquet('../input/open-problems-single-cell-perturbations/de_train.parquet')\n","id_map = pd.read_csv('../input/open-problems-single-cell-perturbations/id_map.csv')\n","sample_submission = pd.read_csv('../input/open-problems-single-cell-perturbations/sample_submission.csv', index_col='id')"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:02:07.442388Z","iopub.status.busy":"2024-04-25T08:02:07.442103Z","iopub.status.idle":"2024-04-25T08:02:07.486380Z","shell.execute_reply":"2024-04-25T08:02:07.485269Z","shell.execute_reply.started":"2024-04-25T08:02:07.442365Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(614, 18211)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["xlist  = ['cell_type','sm_name']\n","_ylist = ['cell_type','sm_name','sm_lincs_id','SMILES','control']\n","\n","y = de_train.drop(columns=_ylist)\n","y.shape"]},{"cell_type":"markdown","metadata":{},"source":["## Use Scikit-Learn's One Hot Encoder\n","This helps encode each pair (cell_type, sm_name) as a multi-dimensional binary vector"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:02:07.490693Z","iopub.status.busy":"2024-04-25T08:02:07.490339Z","iopub.status.idle":"2024-04-25T08:02:08.014323Z","shell.execute_reply":"2024-04-25T08:02:08.013376Z","shell.execute_reply.started":"2024-04-25T08:02:07.490668Z"},"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import OneHotEncoder\n","\n","encoder = OneHotEncoder()\n","encoder.fit(de_train[xlist])\n","one_hot_encode_features = encoder.transform(de_train[xlist])\n","one_hot_test = encoder.transform(id_map[xlist])\n","\n","X = pd.DataFrame(one_hot_encode_features.toarray().astype(float))\n","test = pd.DataFrame(one_hot_test.toarray().astype(float))"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["## "]},{"cell_type":"markdown","metadata":{},"source":["## First Data Augmentation\n","Compute the mean and std differential expression for each cell type and each small molecule name. Take this as additional input features to the model to be built."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:02:08.015817Z","iopub.status.busy":"2024-04-25T08:02:08.015557Z","iopub.status.idle":"2024-04-25T08:02:08.776787Z","shell.execute_reply":"2024-04-25T08:02:08.775989Z","shell.execute_reply.started":"2024-04-25T08:02:08.015794Z"},"trusted":true},"outputs":[],"source":["de_cell_type = de_train.iloc[:, [0] + list(range(5, de_train.shape[1]))]\n","de_sm_name = de_train.iloc[:, [1] + list(range(5, de_train.shape[1]))]\n","mean_cell_type = de_cell_type.groupby('cell_type').mean().reset_index()\n","mean_sm_name = de_sm_name.groupby('sm_name').mean().reset_index()\n","\n","std_cell_type = de_cell_type.groupby('cell_type').std().reset_index()\n","std_sm_name = de_sm_name.groupby('sm_name').std().reset_index()"]},{"cell_type":"markdown","metadata":{},"source":["I also consider the 25%, 50%, and 75% percentiles (see below). Not that I explored different combinations of these features. In particular, when using all additional features (i.e., mean, std, 25%, 50%, and 75% percentiles) I refer to the corresponding models as \"heavy\"."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:02:08.778154Z","iopub.status.busy":"2024-04-25T08:02:08.777854Z","iopub.status.idle":"2024-04-25T08:04:51.471651Z","shell.execute_reply":"2024-04-25T08:04:51.470845Z","shell.execute_reply.started":"2024-04-25T08:02:08.778129Z"},"trusted":true},"outputs":[],"source":["desc_cell_type = de_cell_type.groupby('cell_type').describe().reset_index()\n","desc_cell_type.drop(['mean', 'count', 'std', 'min', 'max'], axis=1, level=1, inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Build a Mapping Between Each SM Name and Its SMILES\n","This helps build Chem BERTa features for molecule SMILES"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:04:51.476930Z","iopub.status.busy":"2024-04-25T08:04:51.476631Z","iopub.status.idle":"2024-04-25T08:04:51.482155Z","shell.execute_reply":"2024-04-25T08:04:51.481298Z","shell.execute_reply.started":"2024-04-25T08:04:51.476901Z"},"trusted":true},"outputs":[],"source":["sm_name2smiles = {smname:smiles for smname, smiles in zip(de_train['sm_name'], de_train['SMILES'])}\n","test_smiles = list(map(sm_name2smiles.get, id_map['sm_name'].values))"]},{"cell_type":"markdown","metadata":{},"source":["Below, we define a variable to control whether we want to reproduce the leaderboard or train different, potentially better instances of our proposed models"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:04:51.484188Z","iopub.status.busy":"2024-04-25T08:04:51.483548Z","iopub.status.idle":"2024-04-25T08:04:51.496469Z","shell.execute_reply":"2024-04-25T08:04:51.495333Z","shell.execute_reply.started":"2024-04-25T08:04:51.484155Z"},"trusted":true},"outputs":[],"source":["reproduce_leaderboard_score = True # Either reproduce the leaderboard or build new (different) models to predict"]},{"cell_type":"markdown","metadata":{},"source":["Thanks to ALEKSEY TREPETSKY (I upvoted) https://www.kaggle.com/code/alekseytrepetsky/create-chemberta-embed/notebook, I could either build my own ChemBERTa features or use the ones she/he has created and shared publicly."]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:04:51.498699Z","iopub.status.busy":"2024-04-25T08:04:51.498413Z","iopub.status.idle":"2024-04-25T08:04:51.509666Z","shell.execute_reply":"2024-04-25T08:04:51.508806Z","shell.execute_reply.started":"2024-04-25T08:04:51.498675Z"},"trusted":true},"outputs":[],"source":["chemberta.eval()\n","def build_ChemBERTa_features(smiles_list):\n","    embeddings = torch.zeros(len(smiles_list), 384)\n","    embeddings_mean = torch.zeros(len(smiles_list), 384)\n","\n","    with torch.no_grad():\n","        for i, smiles in enumerate(tqdm(smiles_list)):\n","            encoded_input = tokenizer(smiles, return_tensors=\"pt\", padding=False, truncation=True)\n","            model_output = chemberta(**encoded_input)\n","            \n","            embedding = model_output[0][::,0,::]\n","            embeddings[i] = embedding\n","            \n","            embedding = torch.mean(model_output[0], 1)\n","            embeddings_mean[i] = embedding\n","            \n","    return embeddings.numpy(), embeddings_mean.numpy()"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:04:51.511603Z","iopub.status.busy":"2024-04-25T08:04:51.510970Z","iopub.status.idle":"2024-04-25T08:04:51.609047Z","shell.execute_reply":"2024-04-25T08:04:51.608311Z","shell.execute_reply.started":"2024-04-25T08:04:51.511572Z"},"trusted":true},"outputs":[],"source":["if reproduce_leaderboard_score:\n","    train_chem_feat = np.load('../input/create-chemberta-embed/train_ChemBERTa_v2_77MTR_cls_pad_True.npy')\n","    test_chem_feat = np.load('../input/create-chemberta-embed/test_ChemBERTa_v2_77MTR_cls_pad_True.npy')\n","    train_chem_feat_mean = np.load('../input/create-chemberta-embed/train_ChemBERTa_v2_77MTR_mean_pad_True.npy')\n","    test_chem_feat_mean = np.load('../input/create-chemberta-embed/test_ChemBERTa_v2_77MTR_mean_pad_True.npy')\n","else:\n","    train_chem_feat, train_chem_feat_mean = build_ChemBERTa_features(de_train.SMILES)\n","    test_chem_feat, test_chem_feat_mean = build_ChemBERTa_features(test_smiles)"]},{"cell_type":"markdown","metadata":{},"source":["## Now Define the Function to Combine the Created Features in Different Ways\n","For each combination, we create and train 3 deep learning architectures (see below)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:04:51.610331Z","iopub.status.busy":"2024-04-25T08:04:51.610057Z","iopub.status.idle":"2024-04-25T08:04:51.622669Z","shell.execute_reply":"2024-04-25T08:04:51.621673Z","shell.execute_reply.started":"2024-04-25T08:04:51.610307Z"},"trusted":true},"outputs":[],"source":["def combine_features(add_dfs, chem_feats, main_df, add_vecs=None, use_description=False):\n","    new_vecs = []\n","    chem_feat_dim = 600 if reproduce_leaderboard_score else 384\n","    if len(add_dfs) > 0:\n","        add_len = sum(add_df.shape[1]-1 for add_df in add_dfs)+chem_feat_dim*len(chem_feats)+add_vecs.shape[1] if\\\n","        add_vecs is not None else sum(add_df.shape[1]-1 for add_df in add_dfs)+chem_feat_dim*len(chem_feats)\n","    else:\n","        add_len = chem_feat_dim*len(chem_feats)+add_vecs.shape[1] if\\\n","        add_vecs is not None else chem_feat_dim*len(chem_feats)\n","    if use_description and reproduce_leaderboard_score:\n","        add_len += (desc_cell_type.shape[1]-1)//3\n","    for i in range(len(main_df)):\n","        if add_vecs is not None:\n","            vec_ = (add_vecs.iloc[i,:].values).copy()\n","        else:\n","            vec_ = np.array([])\n","        for df in add_dfs:\n","            if 'cell_type' in df.columns:\n","                values = df[df['cell_type']==main_df.iloc[i]['cell_type']].values.squeeze()[1:].astype(float)\n","                vec_ = np.concatenate([vec_, values])\n","            else:\n","                assert 'sm_name' in df.columns\n","                values = df[df['sm_name']==main_df.iloc[i]['sm_name']].values.squeeze()[1:].astype(float)\n","                vec_ = np.concatenate([vec_, values])\n","        for chem_feat in chem_feats:\n","            vec_ = np.concatenate([vec_, chem_feat[i]])\n","        final_vec = np.concatenate([vec_,np.zeros(add_len-vec_.shape[0],)])\n","        new_vecs.append(final_vec)\n","    return np.stack(new_vecs, axis=0).astype(float).reshape(len(main_df), 1, add_len)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:04:51.624292Z","iopub.status.busy":"2024-04-25T08:04:51.623936Z","iopub.status.idle":"2024-04-25T08:05:51.894780Z","shell.execute_reply":"2024-04-25T08:05:51.893953Z","shell.execute_reply.started":"2024-04-25T08:04:51.624239Z"},"trusted":true},"outputs":[],"source":["X_vec = combine_features([mean_cell_type,std_cell_type,mean_sm_name,std_sm_name],\\\n","                [train_chem_feat, train_chem_feat_mean], de_train, X)\n","test_vec = combine_features([mean_cell_type,std_cell_type,mean_sm_name,std_sm_name],\\\n","                   [test_chem_feat, test_chem_feat_mean], id_map, test)\n","\n","X_vec_light = combine_features([mean_cell_type,mean_sm_name],\\\n","                [train_chem_feat, train_chem_feat_mean], de_train, X)\n","test_vec_light = combine_features([mean_cell_type,mean_sm_name],\\\n","                   [test_chem_feat, test_chem_feat_mean], id_map, test)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:05:51.896857Z","iopub.status.busy":"2024-04-25T08:05:51.896502Z","iopub.status.idle":"2024-04-25T08:05:51.903827Z","shell.execute_reply":"2024-04-25T08:05:51.902907Z","shell.execute_reply.started":"2024-04-25T08:05:51.896825Z"},"trusted":true},"outputs":[],"source":["if not reproduce_leaderboard_score:\n","    X_vec = np.concatenate([X_vec, np.zeros((X_vec.shape[0], 1, 2))], axis=-1)\n","    test_vec = np.concatenate([test_vec, np.zeros((test_vec.shape[0], 1, 2))], axis=-1)\n","    X_vec_light = np.concatenate([X_vec_light, np.zeros((X_vec_light.shape[0], 1, 1))], axis=-1)\n","    test_vec_light = np.concatenate([test_vec_light, np.zeros((test_vec_light.shape[0], 1, 1))], axis=-1)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:05:51.905461Z","iopub.status.busy":"2024-04-25T08:05:51.905093Z","iopub.status.idle":"2024-04-25T08:05:51.920129Z","shell.execute_reply":"2024-04-25T08:05:51.919184Z","shell.execute_reply.started":"2024-04-25T08:05:51.905427Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(614, 1, 37774)"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["X_vec_light.shape"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:05:51.921712Z","iopub.status.busy":"2024-04-25T08:05:51.921363Z","iopub.status.idle":"2024-04-25T08:06:26.326047Z","shell.execute_reply":"2024-04-25T08:06:26.325153Z","shell.execute_reply.started":"2024-04-25T08:05:51.921680Z"},"trusted":true},"outputs":[],"source":["X_vec_heavy = combine_features([desc_cell_type,mean_cell_type,mean_sm_name],\\\n","                [train_chem_feat,train_chem_feat_mean], de_train, X, use_description=True)\n","test_vec_heavy = combine_features([desc_cell_type,mean_cell_type,mean_sm_name],\\\n","                   [test_chem_feat,test_chem_feat], id_map, test, use_description=True)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:06:26.327421Z","iopub.status.busy":"2024-04-25T08:06:26.327126Z","iopub.status.idle":"2024-04-25T08:06:26.333003Z","shell.execute_reply":"2024-04-25T08:06:26.332136Z","shell.execute_reply.started":"2024-04-25T08:06:26.327396Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(255, 1, 110618)"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["test_vec_heavy.shape"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation Metric Function"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:06:26.334548Z","iopub.status.busy":"2024-04-25T08:06:26.334284Z","iopub.status.idle":"2024-04-25T08:06:26.344580Z","shell.execute_reply":"2024-04-25T08:06:26.343820Z","shell.execute_reply.started":"2024-04-25T08:06:26.334524Z"},"trusted":true},"outputs":[],"source":["def mrrmse_np(y_pred, y_true):\n","    return np.sqrt(np.square(y_true - y_pred).mean(axis=1)).mean()"]},{"cell_type":"markdown","metadata":{},"source":["## Additional Loss Functions\n","I discovered experimentally that by combining different loss functions (others defined in the models), we can achieve a better performance. In particular, I used the binary cross entropy loss to push each predicted value in the target to a value other than 0 (i.e., push the value to a strictly positive or negative value). This is motivated by the fact that several values in the target are close to zero, and I wanted to make sure models do not learn this naively. The rest of the loss functions are suited for regression tasks and used normally to enforce the predicted value to be close to the target."]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:06:26.345852Z","iopub.status.busy":"2024-04-25T08:06:26.345584Z","iopub.status.idle":"2024-04-25T08:06:26.356914Z","shell.execute_reply":"2024-04-25T08:06:26.356195Z","shell.execute_reply.started":"2024-04-25T08:06:26.345829Z"},"trusted":true},"outputs":[],"source":["class LogCoshLoss(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, y_prime_t, y_t):\n","        ey_t = (y_t - y_prime_t)/3 # divide by 3 to avoid numerical overflow in cosh\n","        return torch.mean(torch.log(torch.cosh(ey_t + 1e-12)))"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:06:26.360363Z","iopub.status.busy":"2024-04-25T08:06:26.359641Z","iopub.status.idle":"2024-04-25T08:06:26.370430Z","shell.execute_reply":"2024-04-25T08:06:26.369726Z","shell.execute_reply.started":"2024-04-25T08:06:26.360337Z"},"trusted":true},"outputs":[],"source":["hidden_dims_reproduce_leaderboard = {'conv': {'heavy': 13400, 'light': 4576, 'initial': 8992},\n","                                    'rnn': {'linear': {'heavy': 99968, 'light': 24192, 'initial': 29568},\n","                                           'input_shape': {'heavy': [779,142], 'light': [187,202], 'initial': [229,324]}\n","                                           }}\n","\n","hidden_dims_new = {'conv': {'heavy': 11144, 'light': 4520, 'initial': 8936},\n","                                    'rnn': {'linear': {'heavy': 36480, 'light': 13952, 'initial': 19968},\n","                                           'input_shape': {'heavy': [283,325], 'light': [107,349], 'initial': [154,479]}\n","                                           }}\n","\n","dims_dict = hidden_dims_reproduce_leaderboard if reproduce_leaderboard_score else hidden_dims_new"]},{"cell_type":"markdown","metadata":{},"source":["## Modeling"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:06:26.371877Z","iopub.status.busy":"2024-04-25T08:06:26.371609Z","iopub.status.idle":"2024-04-25T08:06:26.401059Z","shell.execute_reply":"2024-04-25T08:06:26.400220Z","shell.execute_reply.started":"2024-04-25T08:06:26.371854Z"},"trusted":true},"outputs":[],"source":["class Conv(nn.Module):\n","    def __init__(self, scheme):\n","        super(Conv, self).__init__()\n","        self.name = 'Conv'\n","        self.conv_block = nn.Sequential(nn.Conv1d(1, 8, 5, stride=1, padding=0),\n","                                        nn.Dropout(0.3),\n","                                        nn.Conv1d(8, 8, 5, stride=1, padding=0),\n","                                        nn.ReLU(),\n","                                        nn.Conv1d(8, 16, 5, stride=2, padding=0),\n","                                        nn.Dropout(0.3),\n","                                        nn.AvgPool1d(11),\n","                                        nn.Conv1d(16, 8, 3, stride=3, padding=0),\n","                                        nn.Flatten())\n","        self.scheme = scheme\n","        self.linear = nn.Sequential(\n","                nn.Linear(dims_dict['conv'][self.scheme], 1024),\n","                nn.Dropout(0.3),\n","                nn.ReLU(),\n","                nn.Linear(1024, 512),\n","                nn.Dropout(0.3),\n","                nn.ReLU())\n","        self.head1 = nn.Linear(512, 18211)\n","        \n","        self.loss1 = nn.MSELoss()\n","        self.loss2 = LogCoshLoss()\n","        self.loss3 = nn.L1Loss()\n","        self.loss4 = nn.BCELoss()\n","        \n","    def forward(self, x, y=None):\n","        if y is None:\n","            out = self.conv_block(x)\n","            out = self.head1(self.linear(out))\n","            return out\n","        else:\n","            out = self.conv_block(x)\n","            out = self.head1(self.linear(out))\n","            loss1 = 0.4*self.loss1(out, y) + 0.3*self.loss2(out, y) + 0.3*self.loss3(out, y)\n","            yhat = torch.sigmoid(out)\n","            yy = torch.sigmoid(y)\n","            loss2 = self.loss4(yhat, yy)\n","            return 0.8*loss1 + 0.2*loss2\n","        \n","\n","class LSTM(nn.Module):\n","    def __init__(self, scheme):\n","        super(LSTM, self).__init__()\n","        self.name = 'LSTM'\n","        self.scheme = scheme\n","        self.lstm = nn.LSTM(dims_dict['rnn']['input_shape'][self.scheme][1], 128, num_layers=2, batch_first=True)\n","        self.linear = nn.Sequential(\n","            nn.Linear(dims_dict['rnn']['linear'][self.scheme], 1024),\n","            nn.Dropout(0.3),\n","            nn.ReLU(),\n","            nn.Linear(1024, 512),\n","            nn.Dropout(0.3),\n","            nn.ReLU())\n","        self.head1 = nn.Linear(512, 18211)\n","        \n","        self.loss1 = nn.MSELoss()\n","        self.loss2 = LogCoshLoss()\n","        self.loss3 = nn.L1Loss()\n","        self.loss4 = nn.BCELoss()\n","        \n","    def forward(self, x, y=None):\n","        shape1, shape2 = dims_dict['rnn']['input_shape'][self.scheme]\n","        x = x.reshape(x.shape[0],shape1,shape2)\n","        if y is None:\n","            out, (hn, cn) = self.lstm(x)\n","            out = out.reshape(out.shape[0],-1)\n","            out = torch.cat([out, hn.reshape(hn.shape[1], -1)], dim=1)\n","            out = self.head1(self.linear(out))\n","            return out\n","        else:\n","            out, (hn, cn) = self.lstm(x)\n","            out = out.reshape(out.shape[0],-1)\n","            out = torch.cat([out, hn.reshape(hn.shape[1], -1)], dim=1)\n","            out = self.head1(self.linear(out))\n","            loss1 = 0.4*self.loss1(out, y) + 0.3*self.loss2(out, y) + 0.3*self.loss3(out, y)\n","            yhat = torch.sigmoid(out)\n","            yy = torch.sigmoid(y)\n","            loss2 = self.loss4(yhat, yy)\n","            return 0.8*loss1 + 0.2*loss2\n","        \n","        \n","class GRU(nn.Module):\n","    def __init__(self, scheme):\n","        super(GRU, self).__init__()\n","        self.name = 'GRU'\n","        self.scheme = scheme\n","        self.gru = nn.GRU(dims_dict['rnn']['input_shape'][self.scheme][1], 128, num_layers=2, batch_first=True)\n","        self.linear = nn.Sequential(\n","            nn.Linear(dims_dict['rnn']['linear'][self.scheme], 1024),\n","            nn.Dropout(0.3),\n","            nn.ReLU(),\n","            nn.Linear(1024, 512),\n","            nn.Dropout(0.3),\n","            nn.ReLU())\n","        self.head1 = nn.Linear(512, 18211)\n","        \n","        self.loss1 = nn.MSELoss()\n","        self.loss2 = LogCoshLoss()\n","        self.loss3 = nn.L1Loss()\n","        self.loss4 = nn.BCELoss()\n","        \n","    def forward(self, x, y=None):\n","        shape1, shape2 = dims_dict['rnn']['input_shape'][self.scheme]\n","        x = x.reshape(x.shape[0],shape1,shape2)\n","        if y is None:\n","            out, hn = self.gru(x)\n","            out = out.reshape(out.shape[0],-1)\n","            out = torch.cat([out, hn.reshape(hn.shape[1], -1)], dim=1)\n","            out = self.head1(self.linear(out))\n","            return out\n","        else:\n","            out, hn = self.gru(x)\n","            out = out.reshape(out.shape[0],-1)\n","            out = torch.cat([out, hn.reshape(hn.shape[1], -1)], dim=1)\n","            out = self.head1(self.linear(out))\n","            loss1 = 0.4*self.loss1(out, y) + 0.3*self.loss2(out, y) + 0.3*self.loss3(out, y)\n","            yhat = torch.sigmoid(out)\n","            yy = torch.sigmoid(y)\n","            loss2 = self.loss4(yhat, yy)\n","            return 0.8*loss1 + 0.2*loss2"]},{"cell_type":"markdown","metadata":{},"source":["## Create Dataset Class"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:06:26.402367Z","iopub.status.busy":"2024-04-25T08:06:26.402072Z","iopub.status.idle":"2024-04-25T08:06:26.414692Z","shell.execute_reply":"2024-04-25T08:06:26.413932Z","shell.execute_reply.started":"2024-04-25T08:06:26.402344Z"},"trusted":true},"outputs":[],"source":["class Dataset:\n","    def __init__(self, data_x, data_y=None):\n","        super(Dataset, self).__init__()\n","        self.data_x = data_x\n","        self.data_y = data_y\n","\n","    def __len__(self):\n","        return len(self.data_x)\n","    \n","    def __getitem__(self, idx):\n","        if self.data_y is not None:\n","            return self.data_x[idx], self.data_y[idx]\n","        else:\n","            return self.data_x[idx]"]},{"cell_type":"markdown","metadata":{},"source":["## Define Data Augmentation Function\n","In the following function, we augment the training data by randomly dropping 30% of our 1-dimensional input feature vectors' entries. Input features are of shape (batch, 1, d)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:06:26.415932Z","iopub.status.busy":"2024-04-25T08:06:26.415664Z","iopub.status.idle":"2024-04-25T08:06:26.424855Z","shell.execute_reply":"2024-04-25T08:06:26.424089Z","shell.execute_reply.started":"2024-04-25T08:06:26.415909Z"},"trusted":true},"outputs":[],"source":["import random\n","def augment_data(x_, y_):\n","    copy_x = x_.copy()\n","    new_x = []\n","    new_y = y_.copy()\n","    dim = x_.shape[2]\n","    k = int(0.3*dim)\n","    for i in range(x_.shape[0]):\n","        idx = random.sample(range(dim), k=k)\n","        copy_x[i,:,idx] = 0\n","        new_x.append(copy_x[i])\n","    return np.stack(new_x, axis=0), new_y"]},{"cell_type":"markdown","metadata":{},"source":["## Define Helper and Main Training Functions\n","GRU experienced numerical overflow with a learning rate of 0.001, so I used 0.0003 instead"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:06:26.426505Z","iopub.status.busy":"2024-04-25T08:06:26.426162Z","iopub.status.idle":"2024-04-25T08:06:26.442090Z","shell.execute_reply":"2024-04-25T08:06:26.441295Z","shell.execute_reply.started":"2024-04-25T08:06:26.426475Z"},"trusted":true},"outputs":[],"source":["def train_step(dataloader, model, opt, clip_norm):\n","    model.train()\n","    train_losses = []\n","    for x, target in dataloader:\n","        if torch.cuda.is_available():\n","            model.cuda()\n","            x = x.cuda()\n","            target = target.cuda()\n","        loss = model(x, target)\n","        train_losses.append(loss.item())\n","        opt.zero_grad()\n","        loss.backward()\n","        clip_grad_norm_(model.parameters(), clip_norm)\n","        opt.step()\n","    return np.mean(train_losses)\n","\n","def validation_step(dataloader, model):\n","    model.eval()\n","    val_losses = []\n","    val_mrrmse = []\n","    for x, target in dataloader:\n","        if torch.cuda.is_available():\n","            model.cuda()\n","            x = x.cuda()\n","            target = target.cuda()\n","        loss = model(x,target)\n","        pred = model(x).detach().cpu().numpy()\n","        val_mrrmse.append(mrrmse_np(pred, target.cpu().numpy()))\n","        val_losses.append(loss.item())\n","    return np.mean(val_losses), np.mean(val_mrrmse)\n","\n","\n","def train_function(model, x_train, y_train, x_val, y_val, epochs=20, clip_norm=1.0):\n","    if model.name in ['GRU'] or not reproduce_leaderboard_score:\n","        print('lr', 0.0003)\n","        opt = torch.optim.Adam(model.parameters(), lr=0.0003)\n","    else:\n","        opt = torch.optim.Adam(model.parameters(), lr=0.001)\n","    model.cuda()\n","    x_train_aug, y_train_aug = augment_data(x_train, y_train)\n","    x_train_aug = np.concatenate([x_train, x_train_aug], axis=0)\n","    y_train_aug = np.concatenate([y_train, y_train_aug], axis=0)\n","    data_x_train = torch.FloatTensor(x_train_aug)\n","    data_y_train = torch.FloatTensor(y_train_aug)\n","    data_x_val = torch.FloatTensor(x_val)\n","    data_y_val = torch.FloatTensor(y_val)\n","    train_dataloader = DataLoader(Dataset(data_x_train, data_y_train), num_workers=4, batch_size=16, shuffle=True)\n","    val_dataloader = DataLoader(Dataset(data_x_val, data_y_val), num_workers=4, batch_size=32, shuffle=False)\n","    best_loss = np.inf\n","    best_weights = None\n","    train_losses = []\n","    val_losses = []\n","    for e in range(epochs):\n","        loss = train_step(train_dataloader, model, opt, clip_norm)\n","        val_losses.append(loss.item())\n","        val_loss, val_mrrmse = validation_step(val_dataloader, model)\n","        if val_mrrmse < best_loss:\n","            best_loss = val_mrrmse\n","            best_weights = model.state_dict()\n","            print('BEST ----> ')\n","        print(f\"{model.name} Epoch {e}, train_loss {round(loss,3)}, val_loss {round(val_loss, 3)}, val_mrrmse {val_mrrmse}\")\n","    model.load_state_dict(best_weights)\n","    return model"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:06:26.443394Z","iopub.status.busy":"2024-04-25T08:06:26.443061Z","iopub.status.idle":"2024-04-25T08:06:26.539945Z","shell.execute_reply":"2024-04-25T08:06:26.539282Z","shell.execute_reply.started":"2024-04-25T08:06:26.443364Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import KFold as KF\n","splits = 5\n","kf_cv = KF(n_splits=splits, shuffle=True, random_state=42)"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:06:26.547059Z","iopub.status.busy":"2024-04-25T08:06:26.546771Z","iopub.status.idle":"2024-04-25T08:06:26.553704Z","shell.execute_reply":"2024-04-25T08:06:26.552822Z","shell.execute_reply.started":"2024-04-25T08:06:26.547034Z"},"trusted":true},"outputs":[],"source":["def cross_validate_models(X, y, epochs=120, scheme='initial', clip_norm=1.0):\n","    trained_models = []\n","    for i,(train_idx,val_idx) in enumerate(kf_cv.split(X)):\n","        print(f\"\\nSplit {i+1}/{splits}...\")\n","        x_train, x_val = X[train_idx], X[val_idx]\n","        y_train, y_val = y.values[train_idx], y.values[val_idx]\n","        for Model in [LSTM, Conv, GRU]:\n","            model = Model(scheme)\n","            model = train_function(model, x_train, y_train, x_val, y_val, epochs=epochs, clip_norm=clip_norm)\n","            model.to('cpu')\n","            trained_models.append(model)\n","            torch.cuda.empty_cache()\n","    return trained_models"]},{"cell_type":"markdown","metadata":{},"source":["## Define Inference Functions"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:06:26.555145Z","iopub.status.busy":"2024-04-25T08:06:26.554809Z","iopub.status.idle":"2024-04-25T08:06:26.563104Z","shell.execute_reply":"2024-04-25T08:06:26.562132Z","shell.execute_reply.started":"2024-04-25T08:06:26.555113Z"},"trusted":true},"outputs":[],"source":["def inference_pytorch(model, dataloader):\n","    model.eval()\n","    preds = []\n","    for x in dataloader:\n","        if torch.cuda.is_available():\n","            model.cuda()\n","            x = x.cuda()\n","        pred = model(x).detach().cpu().numpy()\n","        preds.append(pred)\n","    model.to('cpu')\n","    torch.cuda.empty_cache()\n","    return np.concatenate(preds, axis=0)"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:06:26.564737Z","iopub.status.busy":"2024-04-25T08:06:26.564278Z","iopub.status.idle":"2024-04-25T08:06:26.573332Z","shell.execute_reply":"2024-04-25T08:06:26.572564Z","shell.execute_reply.started":"2024-04-25T08:06:26.564700Z"},"trusted":true},"outputs":[],"source":["def average_prediction(X_test, trained_models):\n","    all_preds = []\n","    test_dataloader = DataLoader(Dataset(torch.FloatTensor(X_test)), num_workers=4, batch_size=64, shuffle=False)\n","    for i,model in enumerate(trained_models):\n","        current_pred = inference_pytorch(model, test_dataloader)\n","        all_preds.append(current_pred)\n","    return np.stack(all_preds, axis=1).mean(axis=1)"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:06:26.575172Z","iopub.status.busy":"2024-04-25T08:06:26.574384Z","iopub.status.idle":"2024-04-25T08:06:26.586876Z","shell.execute_reply":"2024-04-25T08:06:26.586022Z","shell.execute_reply.started":"2024-04-25T08:06:26.575135Z"},"trusted":true},"outputs":[],"source":["def weighted_average_prediction(X_test, trained_models, model_wise=[0.25, 0.35, 0.40], fold_wise=None):\n","    all_preds = []\n","    test_dataloader = DataLoader(Dataset(torch.FloatTensor(X_test)), num_workers=4, batch_size=64, shuffle=False)\n","    for i,model in enumerate(trained_models):\n","        current_pred = inference_pytorch(model, test_dataloader)\n","        current_pred = model_wise[i%3]*current_pred\n","        if fold_wise:\n","            current_pred = fold_wise[i//3]*current_pred\n","        all_preds.append(current_pred)\n","    return np.stack(all_preds, axis=1).sum(axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["## Train and Reproduce the Leaderboard!"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:06:26.588348Z","iopub.status.busy":"2024-04-25T08:06:26.588011Z","iopub.status.idle":"2024-04-25T08:06:26.596778Z","shell.execute_reply":"2024-04-25T08:06:26.595896Z","shell.execute_reply.started":"2024-04-25T08:06:26.588318Z"},"trusted":true},"outputs":[],"source":["def reproduce(epochs=1):\n","    trained_models = {'initial': [], 'light': [], 'heavy': []}\n","    for scheme, clip_norm, input_features in zip(['initial','light','heavy'], [5.0, 1.0, 1.0], [X_vec, X_vec_light, X_vec_heavy]):\n","        seed_everything()\n","        models = cross_validate_models(input_features, y, epochs=epochs, scheme=scheme, clip_norm=clip_norm)\n","        trained_models[scheme].extend(models)\n","    return trained_models"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:06:26.598279Z","iopub.status.busy":"2024-04-25T08:06:26.597945Z","iopub.status.idle":"2024-04-25T09:00:56.008647Z","shell.execute_reply":"2024-04-25T09:00:56.007215Z","shell.execute_reply.started":"2024-04-25T08:06:26.598234Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["-----Seed Set!-----\n","\n","Split 1/5...\n","BEST ----> \n","LSTM Epoch 0, train_loss 2.096, val_loss 1.16, val_mrrmse 0.991895318031311\n","LSTM Epoch 1, train_loss 1.872, val_loss 2.23, val_mrrmse 1.210561990737915\n","LSTM Epoch 2, train_loss 2.011, val_loss 1.283, val_mrrmse 1.0085457563400269\n","LSTM Epoch 3, train_loss 1.703, val_loss 1.212, val_mrrmse 0.9939535856246948\n","LSTM Epoch 4, train_loss 1.68, val_loss 1.234, val_mrrmse 1.0307337045669556\n","LSTM Epoch 5, train_loss 1.671, val_loss 1.444, val_mrrmse 1.0314505100250244\n","LSTM Epoch 6, train_loss 1.577, val_loss 1.361, val_mrrmse 1.0260246992111206\n","LSTM Epoch 7, train_loss 1.44, val_loss 1.774, val_mrrmse 1.0883088111877441\n","LSTM Epoch 8, train_loss 1.448, val_loss 1.181, val_mrrmse 0.9931333065032959\n","LSTM Epoch 9, train_loss 1.412, val_loss 1.966, val_mrrmse 1.090728998184204\n","LSTM Epoch 10, train_loss 1.378, val_loss 1.568, val_mrrmse 1.0396913290023804\n","LSTM Epoch 11, train_loss 1.301, val_loss 1.515, val_mrrmse 1.0244438648223877\n","LSTM Epoch 12, train_loss 1.326, val_loss 1.475, val_mrrmse 1.016708493232727\n","LSTM Epoch 13, train_loss 1.278, val_loss 1.361, val_mrrmse 1.0028759241104126\n","LSTM Epoch 14, train_loss 1.235, val_loss 1.332, val_mrrmse 1.002417802810669\n","LSTM Epoch 15, train_loss 1.257, val_loss 1.213, val_mrrmse 0.9923996925354004\n","LSTM Epoch 16, train_loss 1.345, val_loss 1.202, val_mrrmse 1.0020945072174072\n","LSTM Epoch 17, train_loss 1.142, val_loss 3.098, val_mrrmse 1.2225182056427002\n","LSTM Epoch 18, train_loss 1.135, val_loss 1.614, val_mrrmse 1.0409750938415527\n","LSTM Epoch 19, train_loss 1.059, val_loss 2.084, val_mrrmse 1.077584981918335\n","BEST ----> \n","Conv Epoch 0, train_loss 2.122, val_loss 1.251, val_mrrmse 1.0194735527038574\n","BEST ----> \n","Conv Epoch 1, train_loss 1.797, val_loss 1.172, val_mrrmse 1.008052945137024\n","BEST ----> \n","Conv Epoch 2, train_loss 2.081, val_loss 1.19, val_mrrmse 1.0069645643234253\n","BEST ----> \n","Conv Epoch 3, train_loss 1.958, val_loss 1.162, val_mrrmse 1.0030732154846191\n","BEST ----> \n","Conv Epoch 4, train_loss 1.733, val_loss 1.086, val_mrrmse 0.9950807690620422\n","BEST ----> \n","Conv Epoch 5, train_loss 1.702, val_loss 1.034, val_mrrmse 0.9806634783744812\n","BEST ----> \n","Conv Epoch 6, train_loss 1.661, val_loss 1.086, val_mrrmse 0.9634734392166138\n","Conv Epoch 7, train_loss 1.455, val_loss 1.134, val_mrrmse 0.965014636516571\n","BEST ----> \n","Conv Epoch 8, train_loss 1.721, val_loss 1.039, val_mrrmse 0.9447356462478638\n","BEST ----> \n","Conv Epoch 9, train_loss 1.354, val_loss 0.996, val_mrrmse 0.9306866526603699\n","Conv Epoch 10, train_loss 1.238, val_loss 1.465, val_mrrmse 1.0046436786651611\n","BEST ----> \n","Conv Epoch 11, train_loss 1.178, val_loss 0.9, val_mrrmse 0.9131044149398804\n","Conv Epoch 12, train_loss 1.196, val_loss 1.512, val_mrrmse 1.0193369388580322\n","Conv Epoch 13, train_loss 1.349, val_loss 0.963, val_mrrmse 0.9214411973953247\n","Conv Epoch 14, train_loss 1.051, val_loss 1.078, val_mrrmse 0.9593471884727478\n","Conv Epoch 15, train_loss 1.05, val_loss 1.029, val_mrrmse 0.9255969524383545\n","BEST ----> \n","Conv Epoch 16, train_loss 0.966, val_loss 0.892, val_mrrmse 0.9036749601364136\n","Conv Epoch 17, train_loss 1.01, val_loss 0.978, val_mrrmse 0.9292845726013184\n","Conv Epoch 18, train_loss 0.904, val_loss 1.584, val_mrrmse 1.0080833435058594\n","Conv Epoch 19, train_loss 0.912, val_loss 0.992, val_mrrmse 0.9130842089653015\n","lr 0.0003\n","BEST ----> \n","GRU Epoch 0, train_loss 2.079, val_loss 1.238, val_mrrmse 1.0296754837036133\n","BEST ----> \n","GRU Epoch 1, train_loss 1.764, val_loss 1.235, val_mrrmse 1.003372311592102\n","GRU Epoch 2, train_loss 1.62, val_loss 1.288, val_mrrmse 1.0102499723434448\n","BEST ----> \n","GRU Epoch 3, train_loss 1.512, val_loss 1.22, val_mrrmse 0.9988157153129578\n","BEST ----> \n","GRU Epoch 4, train_loss 1.419, val_loss 1.192, val_mrrmse 0.9871514439582825\n","GRU Epoch 5, train_loss 1.459, val_loss 1.289, val_mrrmse 0.9885467290878296\n","BEST ----> \n","GRU Epoch 6, train_loss 1.316, val_loss 1.151, val_mrrmse 0.9542250037193298\n","GRU Epoch 7, train_loss 1.186, val_loss 1.176, val_mrrmse 0.9931831359863281\n","BEST ----> \n","GRU Epoch 8, train_loss 1.118, val_loss 0.95, val_mrrmse 0.9199455976486206\n","BEST ----> \n","GRU Epoch 9, train_loss 1.021, val_loss 0.936, val_mrrmse 0.9077581167221069\n","GRU Epoch 10, train_loss 1.03, val_loss 1.034, val_mrrmse 0.9307596683502197\n","BEST ----> \n","GRU Epoch 11, train_loss 0.95, val_loss 0.934, val_mrrmse 0.9071052670478821\n","GRU Epoch 12, train_loss 0.93, val_loss 1.007, val_mrrmse 0.9169155955314636\n","BEST ----> \n","GRU Epoch 13, train_loss 0.894, val_loss 0.835, val_mrrmse 0.8781462907791138\n","GRU Epoch 14, train_loss 0.818, val_loss 1.044, val_mrrmse 0.9384871125221252\n","GRU Epoch 15, train_loss 0.824, val_loss 0.874, val_mrrmse 0.8895221948623657\n","GRU Epoch 16, train_loss 0.81, val_loss 1.217, val_mrrmse 0.942813515663147\n","GRU Epoch 17, train_loss 0.818, val_loss 1.103, val_mrrmse 0.9537676572799683\n","GRU Epoch 18, train_loss 0.782, val_loss 0.898, val_mrrmse 0.9080785512924194\n","GRU Epoch 19, train_loss 0.761, val_loss 0.938, val_mrrmse 0.9181933403015137\n","\n","Split 2/5...\n","BEST ----> \n","LSTM Epoch 0, train_loss 1.587, val_loss 3.106, val_mrrmse 1.4352118968963623\n","LSTM Epoch 1, train_loss 1.399, val_loss 3.735, val_mrrmse 1.5255457162857056\n","BEST ----> \n","LSTM Epoch 2, train_loss 1.435, val_loss 3.032, val_mrrmse 1.4285304546356201\n","LSTM Epoch 3, train_loss 1.285, val_loss 3.261, val_mrrmse 1.44629967212677\n","LSTM Epoch 4, train_loss 1.315, val_loss 3.075, val_mrrmse 1.4379905462265015\n","LSTM Epoch 5, train_loss 1.243, val_loss 3.303, val_mrrmse 1.4595664739608765\n","BEST ----> \n","LSTM Epoch 6, train_loss 1.356, val_loss 2.96, val_mrrmse 1.4047160148620605\n","LSTM Epoch 7, train_loss 1.188, val_loss 3.114, val_mrrmse 1.4416389465332031\n","LSTM Epoch 8, train_loss 1.105, val_loss 3.016, val_mrrmse 1.418192744255066\n","LSTM Epoch 9, train_loss 1.114, val_loss 3.1, val_mrrmse 1.4482346773147583\n","LSTM Epoch 10, train_loss 1.112, val_loss 2.873, val_mrrmse 1.4063713550567627\n","LSTM Epoch 11, train_loss 1.109, val_loss 3.067, val_mrrmse 1.4404170513153076\n","LSTM Epoch 12, train_loss 0.964, val_loss 3.096, val_mrrmse 1.4169961214065552\n","BEST ----> \n","LSTM Epoch 13, train_loss 0.97, val_loss 2.825, val_mrrmse 1.3853280544281006\n","LSTM Epoch 14, train_loss 0.942, val_loss 3.003, val_mrrmse 1.4168555736541748\n","LSTM Epoch 15, train_loss 0.903, val_loss 2.995, val_mrrmse 1.4102455377578735\n","LSTM Epoch 16, train_loss 0.895, val_loss 2.889, val_mrrmse 1.387531042098999\n","BEST ----> \n","LSTM Epoch 17, train_loss 0.886, val_loss 2.765, val_mrrmse 1.3772258758544922\n","BEST ----> \n","LSTM Epoch 18, train_loss 0.907, val_loss 2.812, val_mrrmse 1.3698792457580566\n","BEST ----> \n","LSTM Epoch 19, train_loss 0.898, val_loss 2.783, val_mrrmse 1.355738878250122\n","BEST ----> \n","Conv Epoch 0, train_loss 1.547, val_loss 3.038, val_mrrmse 1.423509120941162\n","BEST ----> \n","Conv Epoch 1, train_loss 1.464, val_loss 2.683, val_mrrmse 1.3796286582946777\n","Conv Epoch 2, train_loss 1.623, val_loss 3.086, val_mrrmse 1.4345853328704834\n","Conv Epoch 3, train_loss 1.372, val_loss 3.186, val_mrrmse 1.431779384613037\n","Conv Epoch 4, train_loss 1.374, val_loss 3.057, val_mrrmse 1.4305824041366577\n","BEST ----> \n","Conv Epoch 5, train_loss 1.363, val_loss 2.676, val_mrrmse 1.363358736038208\n","Conv Epoch 6, train_loss 1.226, val_loss 2.875, val_mrrmse 1.3956186771392822\n","BEST ----> \n","Conv Epoch 7, train_loss 1.19, val_loss 2.77, val_mrrmse 1.3456306457519531\n","BEST ----> \n","Conv Epoch 8, train_loss 1.097, val_loss 2.582, val_mrrmse 1.3338721990585327\n","Conv Epoch 9, train_loss 1.083, val_loss 2.586, val_mrrmse 1.3364853858947754\n","Conv Epoch 10, train_loss 1.08, val_loss 3.085, val_mrrmse 1.3881516456604004\n","Conv Epoch 11, train_loss 1.063, val_loss 2.7, val_mrrmse 1.3488068580627441\n","Conv Epoch 12, train_loss 0.954, val_loss 2.885, val_mrrmse 1.3687989711761475\n","Conv Epoch 13, train_loss 1.0, val_loss 2.698, val_mrrmse 1.3388713598251343\n","Conv Epoch 14, train_loss 1.007, val_loss 2.764, val_mrrmse 1.3849172592163086\n","Conv Epoch 15, train_loss 0.92, val_loss 3.187, val_mrrmse 1.4262678623199463\n","BEST ----> \n","Conv Epoch 16, train_loss 0.936, val_loss 2.587, val_mrrmse 1.3337180614471436\n","BEST ----> \n","Conv Epoch 17, train_loss 0.847, val_loss 2.741, val_mrrmse 1.322175145149231\n","Conv Epoch 18, train_loss 0.795, val_loss 2.627, val_mrrmse 1.3260958194732666\n","Conv Epoch 19, train_loss 0.804, val_loss 2.757, val_mrrmse 1.3250062465667725\n","lr 0.0003\n","BEST ----> \n","GRU Epoch 0, train_loss 1.514, val_loss 3.113, val_mrrmse 1.4256587028503418\n","BEST ----> \n","GRU Epoch 1, train_loss 1.294, val_loss 2.862, val_mrrmse 1.3940856456756592\n","GRU Epoch 2, train_loss 1.215, val_loss 3.144, val_mrrmse 1.432917833328247\n","GRU Epoch 3, train_loss 1.121, val_loss 3.451, val_mrrmse 1.4764375686645508\n","BEST ----> \n","GRU Epoch 4, train_loss 1.112, val_loss 2.765, val_mrrmse 1.3491811752319336\n","GRU Epoch 5, train_loss 1.055, val_loss 2.768, val_mrrmse 1.3532028198242188\n","GRU Epoch 6, train_loss 1.005, val_loss 2.876, val_mrrmse 1.3732190132141113\n","GRU Epoch 7, train_loss 0.982, val_loss 2.872, val_mrrmse 1.4054772853851318\n","GRU Epoch 8, train_loss 0.886, val_loss 2.795, val_mrrmse 1.3611358404159546\n","BEST ----> \n","GRU Epoch 9, train_loss 0.87, val_loss 2.562, val_mrrmse 1.333201289176941\n","BEST ----> \n","GRU Epoch 10, train_loss 0.872, val_loss 2.479, val_mrrmse 1.2784836292266846\n","GRU Epoch 11, train_loss 0.802, val_loss 2.693, val_mrrmse 1.3129146099090576\n","BEST ----> \n","GRU Epoch 12, train_loss 0.771, val_loss 2.525, val_mrrmse 1.2729806900024414\n","BEST ----> \n","GRU Epoch 13, train_loss 0.745, val_loss 2.496, val_mrrmse 1.238578200340271\n","GRU Epoch 14, train_loss 0.728, val_loss 2.557, val_mrrmse 1.2825136184692383\n","BEST ----> \n","GRU Epoch 15, train_loss 0.708, val_loss 2.392, val_mrrmse 1.223365068435669\n","GRU Epoch 16, train_loss 0.692, val_loss 3.065, val_mrrmse 1.3760367631912231\n","GRU Epoch 17, train_loss 0.703, val_loss 2.748, val_mrrmse 1.305528998374939\n","GRU Epoch 18, train_loss 0.702, val_loss 2.408, val_mrrmse 1.2530517578125\n","GRU Epoch 19, train_loss 0.664, val_loss 2.375, val_mrrmse 1.2301936149597168\n","\n","Split 3/5...\n","BEST ----> \n","LSTM Epoch 0, train_loss 2.174, val_loss 1.477, val_mrrmse 1.1436232328414917\n","BEST ----> \n","LSTM Epoch 1, train_loss 1.822, val_loss 1.075, val_mrrmse 1.063647747039795\n","LSTM Epoch 2, train_loss 1.76, val_loss 1.351, val_mrrmse 1.1035743951797485\n","BEST ----> \n","LSTM Epoch 3, train_loss 1.802, val_loss 1.054, val_mrrmse 1.0305917263031006\n","LSTM Epoch 4, train_loss 1.699, val_loss 1.025, val_mrrmse 1.039034128189087\n","LSTM Epoch 5, train_loss 1.635, val_loss 1.284, val_mrrmse 1.0844759941101074\n","BEST ----> \n","LSTM Epoch 6, train_loss 1.745, val_loss 0.965, val_mrrmse 1.0019617080688477\n","LSTM Epoch 7, train_loss 1.513, val_loss 1.111, val_mrrmse 1.0536718368530273\n","LSTM Epoch 8, train_loss 1.612, val_loss 0.998, val_mrrmse 1.0428119897842407\n","LSTM Epoch 9, train_loss 1.366, val_loss 0.995, val_mrrmse 1.0119282007217407\n","BEST ----> \n","LSTM Epoch 10, train_loss 1.36, val_loss 0.951, val_mrrmse 0.9901007413864136\n","LSTM Epoch 11, train_loss 1.467, val_loss 1.213, val_mrrmse 1.0836100578308105\n","LSTM Epoch 12, train_loss 1.421, val_loss 1.022, val_mrrmse 1.013846755027771\n","LSTM Epoch 13, train_loss 1.286, val_loss 0.958, val_mrrmse 1.0018857717514038\n","LSTM Epoch 14, train_loss 1.335, val_loss 1.093, val_mrrmse 1.0595802068710327\n","LSTM Epoch 15, train_loss 1.247, val_loss 1.044, val_mrrmse 1.0371826887130737\n","BEST ----> \n","LSTM Epoch 16, train_loss 1.205, val_loss 0.861, val_mrrmse 0.9615983963012695\n","LSTM Epoch 17, train_loss 1.167, val_loss 0.944, val_mrrmse 0.9864622354507446\n","LSTM Epoch 18, train_loss 1.262, val_loss 0.976, val_mrrmse 0.9888136386871338\n","LSTM Epoch 19, train_loss 1.223, val_loss 0.894, val_mrrmse 0.9687144160270691\n","BEST ----> \n","Conv Epoch 0, train_loss 2.096, val_loss 1.298, val_mrrmse 1.0866442918777466\n","Conv Epoch 1, train_loss 2.023, val_loss 1.485, val_mrrmse 1.109106183052063\n","BEST ----> \n","Conv Epoch 2, train_loss 1.828, val_loss 1.243, val_mrrmse 1.0696600675582886\n","BEST ----> \n","Conv Epoch 3, train_loss 1.817, val_loss 1.09, val_mrrmse 1.0581064224243164\n","Conv Epoch 4, train_loss 1.79, val_loss 1.256, val_mrrmse 1.082633376121521\n","Conv Epoch 5, train_loss 1.676, val_loss 1.323, val_mrrmse 1.1048468351364136\n","Conv Epoch 6, train_loss 1.771, val_loss 1.216, val_mrrmse 1.070762038230896\n","Conv Epoch 7, train_loss 1.468, val_loss 1.244, val_mrrmse 1.0621904134750366\n","BEST ----> \n","Conv Epoch 8, train_loss 1.453, val_loss 0.955, val_mrrmse 0.9974595308303833\n","Conv Epoch 9, train_loss 1.52, val_loss 0.97, val_mrrmse 1.0005061626434326\n","Conv Epoch 10, train_loss 1.432, val_loss 0.949, val_mrrmse 0.997696042060852\n","BEST ----> \n","Conv Epoch 11, train_loss 1.255, val_loss 0.89, val_mrrmse 0.9639442563056946\n","Conv Epoch 12, train_loss 1.314, val_loss 0.875, val_mrrmse 0.9832863807678223\n","BEST ----> \n","Conv Epoch 13, train_loss 1.295, val_loss 0.862, val_mrrmse 0.9557961225509644\n","BEST ----> \n","Conv Epoch 14, train_loss 1.133, val_loss 0.807, val_mrrmse 0.9144167900085449\n","BEST ----> \n","Conv Epoch 15, train_loss 1.113, val_loss 0.796, val_mrrmse 0.8920249342918396\n","Conv Epoch 16, train_loss 1.145, val_loss 0.78, val_mrrmse 0.903491199016571\n","Conv Epoch 17, train_loss 1.061, val_loss 0.742, val_mrrmse 0.8932021856307983\n","Conv Epoch 18, train_loss 1.099, val_loss 0.781, val_mrrmse 0.9124653339385986\n","BEST ----> \n","Conv Epoch 19, train_loss 0.992, val_loss 0.746, val_mrrmse 0.8867465853691101\n","lr 0.0003\n","BEST ----> \n","GRU Epoch 0, train_loss 2.067, val_loss 1.283, val_mrrmse 1.0772913694381714\n","GRU Epoch 1, train_loss 1.748, val_loss 1.331, val_mrrmse 1.096205472946167\n","GRU Epoch 2, train_loss 1.648, val_loss 1.388, val_mrrmse 1.0975687503814697\n","BEST ----> \n","GRU Epoch 3, train_loss 1.634, val_loss 1.167, val_mrrmse 1.056107759475708\n","BEST ----> \n","GRU Epoch 4, train_loss 1.509, val_loss 0.995, val_mrrmse 0.9956886768341064\n","GRU Epoch 5, train_loss 1.511, val_loss 1.068, val_mrrmse 1.0033504962921143\n","GRU Epoch 6, train_loss 1.32, val_loss 1.139, val_mrrmse 1.0201828479766846\n","BEST ----> \n","GRU Epoch 7, train_loss 1.23, val_loss 0.946, val_mrrmse 0.9748424887657166\n","GRU Epoch 8, train_loss 1.233, val_loss 1.051, val_mrrmse 0.9981458187103271\n","BEST ----> \n","GRU Epoch 9, train_loss 1.22, val_loss 0.848, val_mrrmse 0.9331164360046387\n","GRU Epoch 10, train_loss 1.091, val_loss 0.904, val_mrrmse 0.9477131366729736\n","BEST ----> \n","GRU Epoch 11, train_loss 1.114, val_loss 0.835, val_mrrmse 0.9216037392616272\n","BEST ----> \n","GRU Epoch 12, train_loss 1.036, val_loss 0.827, val_mrrmse 0.9118829965591431\n","GRU Epoch 13, train_loss 0.972, val_loss 0.847, val_mrrmse 0.9182339906692505\n","BEST ----> \n","GRU Epoch 14, train_loss 0.976, val_loss 0.744, val_mrrmse 0.8946810364723206\n","GRU Epoch 15, train_loss 0.958, val_loss 0.731, val_mrrmse 0.8965818881988525\n","GRU Epoch 16, train_loss 0.852, val_loss 0.775, val_mrrmse 0.901712954044342\n","GRU Epoch 17, train_loss 0.905, val_loss 0.765, val_mrrmse 0.9048507809638977\n","GRU Epoch 18, train_loss 1.001, val_loss 0.81, val_mrrmse 0.9252338409423828\n","BEST ----> \n","GRU Epoch 19, train_loss 0.842, val_loss 0.712, val_mrrmse 0.8732649087905884\n","\n","Split 4/5...\n","BEST ----> \n","LSTM Epoch 0, train_loss 1.896, val_loss 1.925, val_mrrmse 1.3549684286117554\n","LSTM Epoch 1, train_loss 1.707, val_loss 1.853, val_mrrmse 1.3683907985687256\n","LSTM Epoch 2, train_loss 1.688, val_loss 2.864, val_mrrmse 1.5704615116119385\n","BEST ----> \n","LSTM Epoch 3, train_loss 1.862, val_loss 1.689, val_mrrmse 1.3251073360443115\n","LSTM Epoch 4, train_loss 1.697, val_loss 2.047, val_mrrmse 1.3833041191101074\n","LSTM Epoch 5, train_loss 1.59, val_loss 1.736, val_mrrmse 1.3455196619033813\n","LSTM Epoch 6, train_loss 1.639, val_loss 1.786, val_mrrmse 1.4227527379989624\n","BEST ----> \n","LSTM Epoch 7, train_loss 1.593, val_loss 1.531, val_mrrmse 1.286039113998413\n","LSTM Epoch 8, train_loss 1.493, val_loss 2.252, val_mrrmse 1.430404543876648\n","LSTM Epoch 9, train_loss 1.496, val_loss 1.733, val_mrrmse 1.358534336090088\n","LSTM Epoch 10, train_loss 1.794, val_loss 1.796, val_mrrmse 1.3398090600967407\n","LSTM Epoch 11, train_loss 1.521, val_loss 2.067, val_mrrmse 1.3854608535766602\n","LSTM Epoch 12, train_loss 1.657, val_loss 1.725, val_mrrmse 1.3304684162139893\n","LSTM Epoch 13, train_loss 1.446, val_loss 1.583, val_mrrmse 1.2993290424346924\n","BEST ----> \n","LSTM Epoch 14, train_loss 1.382, val_loss 1.523, val_mrrmse 1.282426357269287\n","LSTM Epoch 15, train_loss 1.294, val_loss 1.939, val_mrrmse 1.3927152156829834\n","LSTM Epoch 16, train_loss 1.207, val_loss 1.919, val_mrrmse 1.373504638671875\n","LSTM Epoch 17, train_loss 1.286, val_loss 1.828, val_mrrmse 1.3401172161102295\n","LSTM Epoch 18, train_loss 1.201, val_loss 1.817, val_mrrmse 1.3392388820648193\n","LSTM Epoch 19, train_loss 1.142, val_loss 1.885, val_mrrmse 1.364588737487793\n","BEST ----> \n","Conv Epoch 0, train_loss 1.976, val_loss 2.069, val_mrrmse 1.4072781801223755\n","BEST ----> \n","Conv Epoch 1, train_loss 1.723, val_loss 1.886, val_mrrmse 1.3376469612121582\n","Conv Epoch 2, train_loss 1.719, val_loss 2.038, val_mrrmse 1.38633131980896\n","Conv Epoch 3, train_loss 1.617, val_loss 2.616, val_mrrmse 1.5164250135421753\n","Conv Epoch 4, train_loss 1.606, val_loss 1.969, val_mrrmse 1.3815066814422607\n","Conv Epoch 5, train_loss 1.63, val_loss 2.228, val_mrrmse 1.420519471168518\n","Conv Epoch 6, train_loss 1.396, val_loss 1.836, val_mrrmse 1.3423032760620117\n","Conv Epoch 7, train_loss 1.33, val_loss 2.49, val_mrrmse 1.4743828773498535\n","Conv Epoch 8, train_loss 1.402, val_loss 2.242, val_mrrmse 1.4257287979125977\n","Conv Epoch 9, train_loss 1.286, val_loss 1.928, val_mrrmse 1.3619732856750488\n","Conv Epoch 10, train_loss 1.225, val_loss 2.041, val_mrrmse 1.3736543655395508\n","Conv Epoch 11, train_loss 1.194, val_loss 1.825, val_mrrmse 1.3427579402923584\n","BEST ----> \n","Conv Epoch 12, train_loss 1.142, val_loss 1.686, val_mrrmse 1.2984287738800049\n","Conv Epoch 13, train_loss 1.054, val_loss 1.856, val_mrrmse 1.3333081007003784\n","Conv Epoch 14, train_loss 1.05, val_loss 2.082, val_mrrmse 1.3585784435272217\n","Conv Epoch 15, train_loss 1.12, val_loss 1.742, val_mrrmse 1.300229549407959\n","BEST ----> \n","Conv Epoch 16, train_loss 0.995, val_loss 1.655, val_mrrmse 1.2611535787582397\n","Conv Epoch 17, train_loss 0.851, val_loss 1.827, val_mrrmse 1.2691277265548706\n","Conv Epoch 18, train_loss 0.932, val_loss 1.908, val_mrrmse 1.3437600135803223\n","Conv Epoch 19, train_loss 0.934, val_loss 1.761, val_mrrmse 1.2816771268844604\n","lr 0.0003\n","BEST ----> \n","GRU Epoch 0, train_loss 1.843, val_loss 1.925, val_mrrmse 1.3810313940048218\n","BEST ----> \n","GRU Epoch 1, train_loss 1.616, val_loss 1.936, val_mrrmse 1.3796828985214233\n","BEST ----> \n","GRU Epoch 2, train_loss 1.589, val_loss 1.644, val_mrrmse 1.3068989515304565\n","GRU Epoch 3, train_loss 1.495, val_loss 2.001, val_mrrmse 1.3851672410964966\n","GRU Epoch 4, train_loss 1.399, val_loss 1.765, val_mrrmse 1.3254752159118652\n","BEST ----> \n","GRU Epoch 5, train_loss 1.289, val_loss 1.78, val_mrrmse 1.3005318641662598\n","GRU Epoch 6, train_loss 1.172, val_loss 2.013, val_mrrmse 1.3846766948699951\n","GRU Epoch 7, train_loss 1.071, val_loss 1.825, val_mrrmse 1.3376381397247314\n","BEST ----> \n","GRU Epoch 8, train_loss 1.079, val_loss 1.527, val_mrrmse 1.2421259880065918\n","GRU Epoch 9, train_loss 0.99, val_loss 1.529, val_mrrmse 1.2510381937026978\n","GRU Epoch 10, train_loss 0.985, val_loss 1.919, val_mrrmse 1.3489750623703003\n","GRU Epoch 11, train_loss 0.959, val_loss 1.862, val_mrrmse 1.3288007974624634\n","BEST ----> \n","GRU Epoch 12, train_loss 0.962, val_loss 1.527, val_mrrmse 1.2296037673950195\n","GRU Epoch 13, train_loss 0.836, val_loss 1.763, val_mrrmse 1.3043510913848877\n","GRU Epoch 14, train_loss 0.936, val_loss 1.559, val_mrrmse 1.2530007362365723\n","GRU Epoch 15, train_loss 0.803, val_loss 1.542, val_mrrmse 1.2341071367263794\n","GRU Epoch 16, train_loss 0.796, val_loss 1.71, val_mrrmse 1.2804298400878906\n","BEST ----> \n","GRU Epoch 17, train_loss 0.771, val_loss 1.433, val_mrrmse 1.2072827816009521\n","GRU Epoch 18, train_loss 0.76, val_loss 1.595, val_mrrmse 1.2294480800628662\n","GRU Epoch 19, train_loss 0.703, val_loss 1.689, val_mrrmse 1.2580783367156982\n","\n","Split 5/5...\n","BEST ----> \n","LSTM Epoch 0, train_loss 2.027, val_loss 1.612, val_mrrmse 1.3011093139648438\n","BEST ----> \n","LSTM Epoch 1, train_loss 2.12, val_loss 1.386, val_mrrmse 1.1025763750076294\n","LSTM Epoch 2, train_loss 2.316, val_loss 1.583, val_mrrmse 1.1303143501281738\n","LSTM Epoch 3, train_loss 2.24, val_loss 1.537, val_mrrmse 1.1199702024459839\n","LSTM Epoch 4, train_loss 2.142, val_loss 1.571, val_mrrmse 1.1232589483261108\n","LSTM Epoch 5, train_loss 2.344, val_loss 1.565, val_mrrmse 1.1204768419265747\n","LSTM Epoch 6, train_loss 2.298, val_loss 1.56, val_mrrmse 1.1185243129730225\n","LSTM Epoch 7, train_loss 2.359, val_loss 1.556, val_mrrmse 1.1173717975616455\n","LSTM Epoch 8, train_loss 2.277, val_loss 1.553, val_mrrmse 1.1168155670166016\n","LSTM Epoch 9, train_loss 2.272, val_loss 1.551, val_mrrmse 1.1165571212768555\n","LSTM Epoch 10, train_loss 2.351, val_loss 1.549, val_mrrmse 1.116574764251709\n","LSTM Epoch 11, train_loss 2.276, val_loss 1.547, val_mrrmse 1.1168437004089355\n","LSTM Epoch 12, train_loss 2.311, val_loss 1.546, val_mrrmse 1.1171021461486816\n","LSTM Epoch 13, train_loss 2.298, val_loss 1.545, val_mrrmse 1.1176939010620117\n","LSTM Epoch 14, train_loss 2.406, val_loss 1.544, val_mrrmse 1.118155837059021\n","LSTM Epoch 15, train_loss 2.275, val_loss 1.543, val_mrrmse 1.1190263032913208\n","LSTM Epoch 16, train_loss 2.266, val_loss 1.542, val_mrrmse 1.1195706129074097\n","LSTM Epoch 17, train_loss 2.26, val_loss 1.542, val_mrrmse 1.1201469898223877\n","LSTM Epoch 18, train_loss 2.264, val_loss 1.541, val_mrrmse 1.120850920677185\n","LSTM Epoch 19, train_loss 2.292, val_loss 1.541, val_mrrmse 1.121309757232666\n","BEST ----> \n","Conv Epoch 0, train_loss 1.914, val_loss 1.358, val_mrrmse 1.0909178256988525\n","BEST ----> \n","Conv Epoch 1, train_loss 1.928, val_loss 1.167, val_mrrmse 1.0201314687728882\n","Conv Epoch 2, train_loss 2.095, val_loss 1.21, val_mrrmse 1.0639472007751465\n","Conv Epoch 3, train_loss 1.834, val_loss 1.2, val_mrrmse 1.0587595701217651\n","Conv Epoch 4, train_loss 1.905, val_loss 1.153, val_mrrmse 1.027022123336792\n","BEST ----> \n","Conv Epoch 5, train_loss 1.677, val_loss 1.094, val_mrrmse 0.996688723564148\n","Conv Epoch 6, train_loss 1.585, val_loss 1.185, val_mrrmse 1.0169477462768555\n","Conv Epoch 7, train_loss 1.686, val_loss 1.202, val_mrrmse 1.049451470375061\n","Conv Epoch 8, train_loss 1.596, val_loss 1.227, val_mrrmse 1.022580862045288\n","Conv Epoch 9, train_loss 1.444, val_loss 1.107, val_mrrmse 1.0049458742141724\n","Conv Epoch 10, train_loss 1.545, val_loss 1.128, val_mrrmse 1.007799744606018\n","Conv Epoch 11, train_loss 1.454, val_loss 1.198, val_mrrmse 1.0051603317260742\n","Conv Epoch 12, train_loss 1.253, val_loss 1.164, val_mrrmse 0.997728705406189\n","BEST ----> \n","Conv Epoch 13, train_loss 1.306, val_loss 1.097, val_mrrmse 0.9959997534751892\n","BEST ----> \n","Conv Epoch 14, train_loss 1.215, val_loss 1.172, val_mrrmse 0.9936215877532959\n","BEST ----> \n","Conv Epoch 15, train_loss 1.051, val_loss 1.104, val_mrrmse 0.9799036979675293\n","BEST ----> \n","Conv Epoch 16, train_loss 1.108, val_loss 1.102, val_mrrmse 0.9746490716934204\n","Conv Epoch 17, train_loss 0.959, val_loss 1.207, val_mrrmse 0.9935142993927002\n","BEST ----> \n","Conv Epoch 18, train_loss 0.945, val_loss 1.018, val_mrrmse 0.9631251096725464\n","Conv Epoch 19, train_loss 0.912, val_loss 1.024, val_mrrmse 0.9633156657218933\n","lr 0.0003\n","BEST ----> \n","GRU Epoch 0, train_loss 2.124, val_loss 1.267, val_mrrmse 1.1053547859191895\n","BEST ----> \n","GRU Epoch 1, train_loss 1.781, val_loss 1.486, val_mrrmse 1.0978267192840576\n","BEST ----> \n","GRU Epoch 2, train_loss 1.795, val_loss 1.376, val_mrrmse 1.0899600982666016\n","BEST ----> \n","GRU Epoch 3, train_loss 1.609, val_loss 1.171, val_mrrmse 1.0332478284835815\n","BEST ----> \n","GRU Epoch 4, train_loss 1.491, val_loss 1.162, val_mrrmse 1.0233122110366821\n","BEST ----> \n","GRU Epoch 5, train_loss 1.443, val_loss 1.134, val_mrrmse 1.010227918624878\n","GRU Epoch 6, train_loss 1.347, val_loss 1.196, val_mrrmse 1.0237823724746704\n","BEST ----> \n","GRU Epoch 7, train_loss 1.214, val_loss 1.079, val_mrrmse 1.0002168416976929\n","BEST ----> \n","GRU Epoch 8, train_loss 1.176, val_loss 1.163, val_mrrmse 0.998329222202301\n","BEST ----> \n","GRU Epoch 9, train_loss 1.123, val_loss 1.02, val_mrrmse 0.9744031429290771\n","BEST ----> \n","GRU Epoch 10, train_loss 1.057, val_loss 1.023, val_mrrmse 0.9581214785575867\n","GRU Epoch 11, train_loss 0.967, val_loss 1.142, val_mrrmse 0.9825289249420166\n","GRU Epoch 12, train_loss 0.972, val_loss 1.077, val_mrrmse 0.9666417837142944\n","BEST ----> \n","GRU Epoch 13, train_loss 0.95, val_loss 0.965, val_mrrmse 0.9374008774757385\n","GRU Epoch 14, train_loss 0.942, val_loss 0.986, val_mrrmse 0.9417909383773804\n","GRU Epoch 15, train_loss 0.83, val_loss 1.153, val_mrrmse 0.9791126251220703\n","GRU Epoch 16, train_loss 0.813, val_loss 1.071, val_mrrmse 0.9672213792800903\n","GRU Epoch 17, train_loss 0.86, val_loss 0.95, val_mrrmse 0.9382750988006592\n","GRU Epoch 18, train_loss 0.753, val_loss 1.031, val_mrrmse 0.948849618434906\n","GRU Epoch 19, train_loss 0.704, val_loss 1.063, val_mrrmse 0.9552549123764038\n","-----Seed Set!-----\n","\n","Split 1/5...\n","BEST ----> \n","LSTM Epoch 0, train_loss 2.06, val_loss 1.203, val_mrrmse 1.0205302238464355\n","BEST ----> \n","LSTM Epoch 1, train_loss 2.153, val_loss 1.219, val_mrrmse 1.017703652381897\n","BEST ----> \n","LSTM Epoch 2, train_loss 1.829, val_loss 1.183, val_mrrmse 0.9917479753494263\n","LSTM Epoch 3, train_loss 1.959, val_loss 1.19, val_mrrmse 1.033761739730835\n","LSTM Epoch 4, train_loss 1.989, val_loss 1.303, val_mrrmse 1.026785135269165\n","LSTM Epoch 5, train_loss 1.729, val_loss 1.473, val_mrrmse 1.064424991607666\n","LSTM Epoch 6, train_loss 1.577, val_loss 1.704, val_mrrmse 1.0690221786499023\n","LSTM Epoch 7, train_loss 1.568, val_loss 1.262, val_mrrmse 1.0142523050308228\n","LSTM Epoch 8, train_loss 1.673, val_loss 1.343, val_mrrmse 1.015714406967163\n","LSTM Epoch 9, train_loss 1.524, val_loss 1.954, val_mrrmse 1.1238670349121094\n","LSTM Epoch 10, train_loss 1.455, val_loss 1.447, val_mrrmse 1.04445481300354\n","LSTM Epoch 11, train_loss 1.383, val_loss 1.922, val_mrrmse 1.0848814249038696\n","LSTM Epoch 12, train_loss 1.237, val_loss 1.888, val_mrrmse 1.0883440971374512\n","LSTM Epoch 13, train_loss 1.372, val_loss 1.446, val_mrrmse 1.0246524810791016\n","BEST ----> \n","LSTM Epoch 14, train_loss 1.17, val_loss 1.143, val_mrrmse 0.9730993509292603\n","BEST ----> \n","LSTM Epoch 15, train_loss 1.15, val_loss 1.076, val_mrrmse 0.9556869268417358\n","LSTM Epoch 16, train_loss 1.149, val_loss 1.118, val_mrrmse 0.9581472873687744\n","LSTM Epoch 17, train_loss 1.124, val_loss 1.218, val_mrrmse 0.9686876535415649\n","BEST ----> \n","LSTM Epoch 18, train_loss 1.014, val_loss 1.0, val_mrrmse 0.9367139339447021\n","LSTM Epoch 19, train_loss 1.153, val_loss 1.27, val_mrrmse 0.974470853805542\n","BEST ----> \n","Conv Epoch 0, train_loss 2.043, val_loss 1.38, val_mrrmse 1.034895420074463\n","BEST ----> \n","Conv Epoch 1, train_loss 1.828, val_loss 1.214, val_mrrmse 1.0317294597625732\n","BEST ----> \n","Conv Epoch 2, train_loss 1.977, val_loss 1.136, val_mrrmse 0.9903311729431152\n","BEST ----> \n","Conv Epoch 3, train_loss 1.932, val_loss 1.07, val_mrrmse 0.9765198826789856\n","BEST ----> \n","Conv Epoch 4, train_loss 1.66, val_loss 0.985, val_mrrmse 0.9434719085693359\n","Conv Epoch 5, train_loss 1.662, val_loss 1.118, val_mrrmse 0.9745221734046936\n","Conv Epoch 6, train_loss 1.505, val_loss 1.416, val_mrrmse 1.0165098905563354\n","Conv Epoch 7, train_loss 1.541, val_loss 1.027, val_mrrmse 0.9450395703315735\n","Conv Epoch 8, train_loss 1.46, val_loss 1.227, val_mrrmse 0.9837236404418945\n","Conv Epoch 9, train_loss 1.382, val_loss 1.206, val_mrrmse 0.9737916588783264\n","Conv Epoch 10, train_loss 1.287, val_loss 1.165, val_mrrmse 0.9690803289413452\n","BEST ----> \n","Conv Epoch 11, train_loss 1.287, val_loss 0.977, val_mrrmse 0.9337135553359985\n","Conv Epoch 12, train_loss 1.105, val_loss 1.642, val_mrrmse 1.049323320388794\n","Conv Epoch 13, train_loss 1.043, val_loss 1.511, val_mrrmse 1.0050327777862549\n","Conv Epoch 14, train_loss 1.17, val_loss 1.161, val_mrrmse 0.9651457667350769\n","Conv Epoch 15, train_loss 1.075, val_loss 1.226, val_mrrmse 0.9709869623184204\n","BEST ----> \n","Conv Epoch 16, train_loss 1.084, val_loss 1.001, val_mrrmse 0.915210485458374\n","BEST ----> \n","Conv Epoch 17, train_loss 1.065, val_loss 0.919, val_mrrmse 0.8959458470344543\n","Conv Epoch 18, train_loss 0.912, val_loss 0.952, val_mrrmse 0.9020018577575684\n","BEST ----> \n","Conv Epoch 19, train_loss 0.955, val_loss 0.796, val_mrrmse 0.870554506778717\n","lr 0.0003\n","BEST ----> \n","GRU Epoch 0, train_loss 2.027, val_loss 1.178, val_mrrmse 0.9973829984664917\n","BEST ----> \n","GRU Epoch 1, train_loss 1.801, val_loss 1.153, val_mrrmse 0.9876611232757568\n","GRU Epoch 2, train_loss 1.611, val_loss 1.176, val_mrrmse 0.9889461994171143\n","GRU Epoch 3, train_loss 1.448, val_loss 1.156, val_mrrmse 0.9899129271507263\n","GRU Epoch 4, train_loss 1.415, val_loss 1.615, val_mrrmse 1.048531174659729\n","BEST ----> \n","GRU Epoch 5, train_loss 1.387, val_loss 1.06, val_mrrmse 0.9460622072219849\n","GRU Epoch 6, train_loss 1.259, val_loss 1.12, val_mrrmse 0.9607353806495667\n","GRU Epoch 7, train_loss 1.17, val_loss 1.163, val_mrrmse 0.9608696699142456\n","GRU Epoch 8, train_loss 1.076, val_loss 1.177, val_mrrmse 0.9655765295028687\n","BEST ----> \n","GRU Epoch 9, train_loss 1.017, val_loss 1.047, val_mrrmse 0.9404782056808472\n","BEST ----> \n","GRU Epoch 10, train_loss 0.985, val_loss 0.968, val_mrrmse 0.918134868144989\n","BEST ----> \n","GRU Epoch 11, train_loss 0.924, val_loss 1.003, val_mrrmse 0.910967230796814\n","BEST ----> \n","GRU Epoch 12, train_loss 0.861, val_loss 0.871, val_mrrmse 0.8854812383651733\n","BEST ----> \n","GRU Epoch 13, train_loss 0.86, val_loss 0.835, val_mrrmse 0.8833929896354675\n","GRU Epoch 14, train_loss 0.893, val_loss 0.913, val_mrrmse 0.9024473428726196\n","GRU Epoch 15, train_loss 0.785, val_loss 0.93, val_mrrmse 0.9052290916442871\n","BEST ----> \n","GRU Epoch 16, train_loss 0.755, val_loss 0.816, val_mrrmse 0.8761188983917236\n","GRU Epoch 17, train_loss 0.737, val_loss 1.076, val_mrrmse 0.914873480796814\n","GRU Epoch 18, train_loss 0.753, val_loss 0.91, val_mrrmse 0.9032822251319885\n","GRU Epoch 19, train_loss 0.733, val_loss 1.103, val_mrrmse 0.9198868870735168\n","\n","Split 2/5...\n","BEST ----> \n","LSTM Epoch 0, train_loss 1.502, val_loss 3.005, val_mrrmse 1.476876139640808\n","BEST ----> \n","LSTM Epoch 1, train_loss 1.417, val_loss 3.201, val_mrrmse 1.4411895275115967\n","BEST ----> \n","LSTM Epoch 2, train_loss 1.236, val_loss 2.934, val_mrrmse 1.37950599193573\n","LSTM Epoch 3, train_loss 1.208, val_loss 3.152, val_mrrmse 1.4381519556045532\n","BEST ----> \n","LSTM Epoch 4, train_loss 1.101, val_loss 2.779, val_mrrmse 1.3741154670715332\n","LSTM Epoch 5, train_loss 1.041, val_loss 3.262, val_mrrmse 1.5965806245803833\n","LSTM Epoch 6, train_loss 1.095, val_loss 3.266, val_mrrmse 1.4343996047973633\n","LSTM Epoch 7, train_loss 1.001, val_loss 3.463, val_mrrmse 1.481343150138855\n","LSTM Epoch 8, train_loss 1.002, val_loss 2.994, val_mrrmse 1.4034053087234497\n","LSTM Epoch 9, train_loss 0.851, val_loss 3.2, val_mrrmse 1.4134917259216309\n","LSTM Epoch 10, train_loss 0.892, val_loss 2.901, val_mrrmse 1.380255937576294\n","LSTM Epoch 11, train_loss 0.799, val_loss 3.2, val_mrrmse 1.4319608211517334\n","BEST ----> \n","LSTM Epoch 12, train_loss 0.802, val_loss 2.657, val_mrrmse 1.3153135776519775\n","LSTM Epoch 13, train_loss 0.717, val_loss 2.791, val_mrrmse 1.3546411991119385\n","LSTM Epoch 14, train_loss 0.71, val_loss 2.594, val_mrrmse 1.3213849067687988\n","LSTM Epoch 15, train_loss 0.745, val_loss 2.666, val_mrrmse 1.3364152908325195\n","LSTM Epoch 16, train_loss 0.679, val_loss 2.735, val_mrrmse 1.3356914520263672\n","LSTM Epoch 17, train_loss 0.673, val_loss 3.097, val_mrrmse 1.3799481391906738\n","BEST ----> \n","LSTM Epoch 18, train_loss 0.69, val_loss 2.455, val_mrrmse 1.2748441696166992\n","LSTM Epoch 19, train_loss 0.656, val_loss 2.952, val_mrrmse 1.364941954612732\n","BEST ----> \n","Conv Epoch 0, train_loss 1.539, val_loss 2.603, val_mrrmse 1.3611860275268555\n","Conv Epoch 1, train_loss 1.387, val_loss 2.853, val_mrrmse 1.3778109550476074\n","Conv Epoch 2, train_loss 1.351, val_loss 2.732, val_mrrmse 1.3741841316223145\n","Conv Epoch 3, train_loss 1.438, val_loss 3.413, val_mrrmse 1.4641550779342651\n","Conv Epoch 4, train_loss 1.325, val_loss 2.685, val_mrrmse 1.3660341501235962\n","Conv Epoch 5, train_loss 1.264, val_loss 3.132, val_mrrmse 1.4595210552215576\n","Conv Epoch 6, train_loss 1.258, val_loss 3.066, val_mrrmse 1.4207203388214111\n","BEST ----> \n","Conv Epoch 7, train_loss 1.181, val_loss 2.87, val_mrrmse 1.3593281507492065\n","Conv Epoch 8, train_loss 1.127, val_loss 2.916, val_mrrmse 1.3803824186325073\n","Conv Epoch 9, train_loss 1.161, val_loss 2.885, val_mrrmse 1.3797067403793335\n","Conv Epoch 10, train_loss 1.242, val_loss 2.896, val_mrrmse 1.3891353607177734\n","Conv Epoch 11, train_loss 1.036, val_loss 2.944, val_mrrmse 1.3851656913757324\n","Conv Epoch 12, train_loss 1.088, val_loss 2.965, val_mrrmse 1.3986784219741821\n","Conv Epoch 13, train_loss 1.026, val_loss 2.857, val_mrrmse 1.3643518686294556\n","BEST ----> \n","Conv Epoch 14, train_loss 0.993, val_loss 2.908, val_mrrmse 1.35843825340271\n","BEST ----> \n","Conv Epoch 15, train_loss 0.9, val_loss 2.704, val_mrrmse 1.3259273767471313\n","Conv Epoch 16, train_loss 0.866, val_loss 2.82, val_mrrmse 1.3529911041259766\n","Conv Epoch 17, train_loss 0.834, val_loss 2.96, val_mrrmse 1.3490304946899414\n","BEST ----> \n","Conv Epoch 18, train_loss 0.879, val_loss 2.747, val_mrrmse 1.317091464996338\n","Conv Epoch 19, train_loss 0.886, val_loss 2.733, val_mrrmse 1.3255667686462402\n","lr 0.0003\n","BEST ----> \n","GRU Epoch 0, train_loss 1.527, val_loss 2.974, val_mrrmse 1.4463222026824951\n","BEST ----> \n","GRU Epoch 1, train_loss 1.362, val_loss 3.059, val_mrrmse 1.4116742610931396\n","BEST ----> \n","GRU Epoch 2, train_loss 1.231, val_loss 3.068, val_mrrmse 1.4081275463104248\n","GRU Epoch 3, train_loss 1.124, val_loss 3.481, val_mrrmse 1.4794434309005737\n","BEST ----> \n","GRU Epoch 4, train_loss 1.051, val_loss 2.907, val_mrrmse 1.3764976263046265\n","GRU Epoch 5, train_loss 0.964, val_loss 2.885, val_mrrmse 1.3926236629486084\n","GRU Epoch 6, train_loss 0.92, val_loss 3.059, val_mrrmse 1.3795255422592163\n","BEST ----> \n","GRU Epoch 7, train_loss 0.86, val_loss 2.792, val_mrrmse 1.3586550951004028\n","GRU Epoch 8, train_loss 0.851, val_loss 3.19, val_mrrmse 1.4261009693145752\n","BEST ----> \n","GRU Epoch 9, train_loss 0.823, val_loss 2.647, val_mrrmse 1.3012319803237915\n","GRU Epoch 10, train_loss 0.794, val_loss 2.665, val_mrrmse 1.307902216911316\n","BEST ----> \n","GRU Epoch 11, train_loss 0.711, val_loss 2.602, val_mrrmse 1.2744004726409912\n","GRU Epoch 12, train_loss 0.708, val_loss 2.788, val_mrrmse 1.3170970678329468\n","GRU Epoch 13, train_loss 0.704, val_loss 2.764, val_mrrmse 1.297943115234375\n","BEST ----> \n","GRU Epoch 14, train_loss 0.697, val_loss 2.559, val_mrrmse 1.2539074420928955\n","BEST ----> \n","GRU Epoch 15, train_loss 0.655, val_loss 2.501, val_mrrmse 1.2447397708892822\n","GRU Epoch 16, train_loss 0.668, val_loss 2.655, val_mrrmse 1.2680277824401855\n","GRU Epoch 17, train_loss 0.645, val_loss 2.697, val_mrrmse 1.2810404300689697\n","BEST ----> \n","GRU Epoch 18, train_loss 0.641, val_loss 2.476, val_mrrmse 1.2197120189666748\n","GRU Epoch 19, train_loss 0.634, val_loss 2.603, val_mrrmse 1.2622976303100586\n","\n","Split 3/5...\n","BEST ----> \n","LSTM Epoch 0, train_loss 2.015, val_loss 0.978, val_mrrmse 1.0091619491577148\n","LSTM Epoch 1, train_loss 1.909, val_loss 2.151, val_mrrmse 1.6021482944488525\n","LSTM Epoch 2, train_loss 1.787, val_loss 1.139, val_mrrmse 1.0542311668395996\n","LSTM Epoch 3, train_loss 1.525, val_loss 1.379, val_mrrmse 1.0980414152145386\n","LSTM Epoch 4, train_loss 1.567, val_loss 1.45, val_mrrmse 1.104589581489563\n","LSTM Epoch 5, train_loss 1.449, val_loss 1.063, val_mrrmse 1.0171020030975342\n","LSTM Epoch 6, train_loss 1.417, val_loss 1.099, val_mrrmse 1.0381920337677002\n","LSTM Epoch 7, train_loss 1.396, val_loss 1.012, val_mrrmse 1.0155329704284668\n","BEST ----> \n","LSTM Epoch 8, train_loss 1.215, val_loss 0.969, val_mrrmse 0.9820921421051025\n","LSTM Epoch 9, train_loss 1.223, val_loss 1.006, val_mrrmse 0.9959532618522644\n","BEST ----> \n","LSTM Epoch 10, train_loss 1.252, val_loss 0.894, val_mrrmse 0.9643030166625977\n","LSTM Epoch 11, train_loss 1.152, val_loss 1.257, val_mrrmse 1.026794672012329\n","BEST ----> \n","LSTM Epoch 12, train_loss 1.111, val_loss 0.898, val_mrrmse 0.9577008485794067\n","BEST ----> \n","LSTM Epoch 13, train_loss 1.087, val_loss 0.865, val_mrrmse 0.9468174576759338\n","LSTM Epoch 14, train_loss 0.88, val_loss 0.927, val_mrrmse 0.9639129638671875\n","LSTM Epoch 15, train_loss 0.929, val_loss 0.939, val_mrrmse 0.958025336265564\n","BEST ----> \n","LSTM Epoch 16, train_loss 0.889, val_loss 0.808, val_mrrmse 0.9182475805282593\n","LSTM Epoch 17, train_loss 0.912, val_loss 0.832, val_mrrmse 0.931037962436676\n","LSTM Epoch 18, train_loss 0.915, val_loss 0.799, val_mrrmse 0.9193403720855713\n","LSTM Epoch 19, train_loss 0.781, val_loss 0.841, val_mrrmse 0.919501543045044\n","BEST ----> \n","Conv Epoch 0, train_loss 2.048, val_loss 1.038, val_mrrmse 1.0264875888824463\n","Conv Epoch 1, train_loss 1.916, val_loss 1.328, val_mrrmse 1.0917508602142334\n","BEST ----> \n","Conv Epoch 2, train_loss 1.803, val_loss 1.09, val_mrrmse 1.021684169769287\n","BEST ----> \n","Conv Epoch 3, train_loss 1.738, val_loss 1.056, val_mrrmse 1.0138850212097168\n","BEST ----> \n","Conv Epoch 4, train_loss 1.641, val_loss 0.998, val_mrrmse 1.0018881559371948\n","Conv Epoch 5, train_loss 1.718, val_loss 1.21, val_mrrmse 1.0635840892791748\n","Conv Epoch 6, train_loss 1.637, val_loss 1.004, val_mrrmse 1.0089623928070068\n","Conv Epoch 7, train_loss 1.682, val_loss 1.038, val_mrrmse 1.0116755962371826\n","Conv Epoch 8, train_loss 1.576, val_loss 1.145, val_mrrmse 1.0329477787017822\n","BEST ----> \n","Conv Epoch 9, train_loss 1.651, val_loss 0.979, val_mrrmse 0.9978341460227966\n","Conv Epoch 10, train_loss 1.639, val_loss 1.195, val_mrrmse 1.043190598487854\n","BEST ----> \n","Conv Epoch 11, train_loss 1.498, val_loss 0.987, val_mrrmse 0.9915603399276733\n","Conv Epoch 12, train_loss 1.455, val_loss 1.036, val_mrrmse 1.0101029872894287\n","Conv Epoch 13, train_loss 1.379, val_loss 1.094, val_mrrmse 1.0353962182998657\n","Conv Epoch 14, train_loss 1.396, val_loss 1.125, val_mrrmse 1.0223414897918701\n","BEST ----> \n","Conv Epoch 15, train_loss 1.376, val_loss 0.981, val_mrrmse 0.9898335337638855\n","Conv Epoch 16, train_loss 1.352, val_loss 1.095, val_mrrmse 1.0250787734985352\n","Conv Epoch 17, train_loss 1.212, val_loss 1.163, val_mrrmse 1.0231832265853882\n","BEST ----> \n","Conv Epoch 18, train_loss 1.197, val_loss 1.035, val_mrrmse 0.9873818755149841\n","BEST ----> \n","Conv Epoch 19, train_loss 1.08, val_loss 1.011, val_mrrmse 0.9758800864219666\n","lr 0.0003\n","BEST ----> \n","GRU Epoch 0, train_loss 2.077, val_loss 1.475, val_mrrmse 1.112239122390747\n","GRU Epoch 1, train_loss 1.8, val_loss 1.824, val_mrrmse 1.1531763076782227\n","BEST ----> \n","GRU Epoch 2, train_loss 1.68, val_loss 1.499, val_mrrmse 1.098404884338379\n","BEST ----> \n","GRU Epoch 3, train_loss 1.576, val_loss 1.297, val_mrrmse 1.063279628753662\n","BEST ----> \n","GRU Epoch 4, train_loss 1.446, val_loss 1.199, val_mrrmse 1.0459760427474976\n","BEST ----> \n","GRU Epoch 5, train_loss 1.32, val_loss 0.959, val_mrrmse 0.9724313020706177\n","BEST ----> \n","GRU Epoch 6, train_loss 1.244, val_loss 0.934, val_mrrmse 0.970453679561615\n","BEST ----> \n","GRU Epoch 7, train_loss 1.253, val_loss 0.939, val_mrrmse 0.9676165580749512\n","GRU Epoch 8, train_loss 1.184, val_loss 0.992, val_mrrmse 0.9795593023300171\n","BEST ----> \n","GRU Epoch 9, train_loss 1.077, val_loss 0.904, val_mrrmse 0.954290509223938\n","BEST ----> \n","GRU Epoch 10, train_loss 1.042, val_loss 0.873, val_mrrmse 0.9297295808792114\n","BEST ----> \n","GRU Epoch 11, train_loss 1.066, val_loss 0.766, val_mrrmse 0.8935595750808716\n","GRU Epoch 12, train_loss 0.951, val_loss 0.825, val_mrrmse 0.9207247495651245\n","BEST ----> \n","GRU Epoch 13, train_loss 0.937, val_loss 0.739, val_mrrmse 0.8851413726806641\n","GRU Epoch 14, train_loss 0.934, val_loss 0.786, val_mrrmse 0.9060367345809937\n","BEST ----> \n","GRU Epoch 15, train_loss 0.867, val_loss 0.737, val_mrrmse 0.8716639876365662\n","GRU Epoch 16, train_loss 0.818, val_loss 0.777, val_mrrmse 0.8932087421417236\n","GRU Epoch 17, train_loss 0.797, val_loss 0.762, val_mrrmse 0.8993200063705444\n","BEST ----> \n","GRU Epoch 18, train_loss 0.783, val_loss 0.719, val_mrrmse 0.8700928688049316\n","BEST ----> \n","GRU Epoch 19, train_loss 0.78, val_loss 0.68, val_mrrmse 0.8578301072120667\n","\n","Split 4/5...\n","BEST ----> \n","LSTM Epoch 0, train_loss 1.899, val_loss 2.473, val_mrrmse 1.4771623611450195\n","LSTM Epoch 1, train_loss 1.781, val_loss 2.528, val_mrrmse 1.4825023412704468\n","BEST ----> \n","LSTM Epoch 2, train_loss 1.634, val_loss 1.778, val_mrrmse 1.3737512826919556\n","BEST ----> \n","LSTM Epoch 3, train_loss 1.559, val_loss 1.951, val_mrrmse 1.356343150138855\n","LSTM Epoch 4, train_loss 1.441, val_loss 2.199, val_mrrmse 1.422310709953308\n","LSTM Epoch 5, train_loss 1.392, val_loss 1.954, val_mrrmse 1.3689180612564087\n","BEST ----> \n","LSTM Epoch 6, train_loss 1.365, val_loss 1.709, val_mrrmse 1.3143582344055176\n","LSTM Epoch 7, train_loss 1.294, val_loss 1.718, val_mrrmse 1.3185369968414307\n","LSTM Epoch 8, train_loss 1.278, val_loss 2.195, val_mrrmse 1.4195499420166016\n","LSTM Epoch 9, train_loss 1.12, val_loss 2.18, val_mrrmse 1.405118703842163\n","BEST ----> \n","LSTM Epoch 10, train_loss 1.103, val_loss 1.511, val_mrrmse 1.2488157749176025\n","LSTM Epoch 11, train_loss 1.106, val_loss 1.647, val_mrrmse 1.284546971321106\n","LSTM Epoch 12, train_loss 1.014, val_loss 1.579, val_mrrmse 1.263179898262024\n","BEST ----> \n","LSTM Epoch 13, train_loss 0.972, val_loss 1.491, val_mrrmse 1.2458834648132324\n","LSTM Epoch 14, train_loss 0.907, val_loss 2.619, val_mrrmse 1.498772144317627\n","LSTM Epoch 15, train_loss 0.891, val_loss 2.07, val_mrrmse 1.3761045932769775\n","LSTM Epoch 16, train_loss 0.871, val_loss 2.062, val_mrrmse 1.3906047344207764\n","LSTM Epoch 17, train_loss 0.898, val_loss 1.989, val_mrrmse 1.3557348251342773\n","LSTM Epoch 18, train_loss 0.824, val_loss 1.583, val_mrrmse 1.2966820001602173\n","LSTM Epoch 19, train_loss 0.966, val_loss 1.943, val_mrrmse 1.3550379276275635\n","BEST ----> \n","Conv Epoch 0, train_loss 1.843, val_loss 1.678, val_mrrmse 1.3057674169540405\n","Conv Epoch 1, train_loss 1.645, val_loss 2.314, val_mrrmse 1.4427704811096191\n","Conv Epoch 2, train_loss 1.654, val_loss 2.423, val_mrrmse 1.4616888761520386\n","Conv Epoch 3, train_loss 1.64, val_loss 1.972, val_mrrmse 1.363368272781372\n","Conv Epoch 4, train_loss 1.396, val_loss 1.753, val_mrrmse 1.3296195268630981\n","Conv Epoch 5, train_loss 1.533, val_loss 2.086, val_mrrmse 1.3832064867019653\n","BEST ----> \n","Conv Epoch 6, train_loss 1.496, val_loss 1.713, val_mrrmse 1.3051170110702515\n","Conv Epoch 7, train_loss 1.405, val_loss 2.396, val_mrrmse 1.452243447303772\n","Conv Epoch 8, train_loss 1.375, val_loss 1.847, val_mrrmse 1.327854871749878\n","BEST ----> \n","Conv Epoch 9, train_loss 1.353, val_loss 1.625, val_mrrmse 1.2830405235290527\n","Conv Epoch 10, train_loss 1.214, val_loss 1.8, val_mrrmse 1.314168095588684\n","Conv Epoch 11, train_loss 1.317, val_loss 1.764, val_mrrmse 1.3225079774856567\n","Conv Epoch 12, train_loss 1.197, val_loss 1.832, val_mrrmse 1.3306540250778198\n","Conv Epoch 13, train_loss 1.158, val_loss 1.693, val_mrrmse 1.2866290807724\n","BEST ----> \n","Conv Epoch 14, train_loss 1.101, val_loss 1.554, val_mrrmse 1.2679380178451538\n","Conv Epoch 15, train_loss 1.136, val_loss 2.05, val_mrrmse 1.364563226699829\n","BEST ----> \n","Conv Epoch 16, train_loss 1.029, val_loss 1.395, val_mrrmse 1.2164565324783325\n","Conv Epoch 17, train_loss 0.94, val_loss 1.596, val_mrrmse 1.254377841949463\n","Conv Epoch 18, train_loss 1.036, val_loss 1.699, val_mrrmse 1.2835958003997803\n","Conv Epoch 19, train_loss 0.937, val_loss 1.768, val_mrrmse 1.2799211740493774\n","lr 0.0003\n","BEST ----> \n","GRU Epoch 0, train_loss 1.864, val_loss 2.109, val_mrrmse 1.3977299928665161\n","BEST ----> \n","GRU Epoch 1, train_loss 1.609, val_loss 1.895, val_mrrmse 1.3680111169815063\n","GRU Epoch 2, train_loss 1.52, val_loss 2.19, val_mrrmse 1.4079951047897339\n","BEST ----> \n","GRU Epoch 3, train_loss 1.348, val_loss 1.843, val_mrrmse 1.3432326316833496\n","GRU Epoch 4, train_loss 1.278, val_loss 2.283, val_mrrmse 1.4174160957336426\n","BEST ----> \n","GRU Epoch 5, train_loss 1.265, val_loss 1.878, val_mrrmse 1.3276904821395874\n","GRU Epoch 6, train_loss 1.184, val_loss 1.955, val_mrrmse 1.3455544710159302\n","BEST ----> \n","GRU Epoch 7, train_loss 1.086, val_loss 1.842, val_mrrmse 1.3267472982406616\n","BEST ----> \n","GRU Epoch 8, train_loss 1.035, val_loss 1.655, val_mrrmse 1.2753369808197021\n","GRU Epoch 9, train_loss 1.01, val_loss 1.834, val_mrrmse 1.319082498550415\n","BEST ----> \n","GRU Epoch 10, train_loss 0.938, val_loss 1.594, val_mrrmse 1.2489259243011475\n","GRU Epoch 11, train_loss 0.963, val_loss 1.84, val_mrrmse 1.313501238822937\n","GRU Epoch 12, train_loss 0.884, val_loss 1.735, val_mrrmse 1.277045488357544\n","GRU Epoch 13, train_loss 0.833, val_loss 1.705, val_mrrmse 1.27146315574646\n","GRU Epoch 14, train_loss 0.816, val_loss 1.635, val_mrrmse 1.2530701160430908\n","BEST ----> \n","GRU Epoch 15, train_loss 0.766, val_loss 1.469, val_mrrmse 1.1884567737579346\n","GRU Epoch 16, train_loss 0.73, val_loss 1.895, val_mrrmse 1.301218867301941\n","GRU Epoch 17, train_loss 0.769, val_loss 1.573, val_mrrmse 1.230719804763794\n","GRU Epoch 18, train_loss 0.704, val_loss 1.546, val_mrrmse 1.228627324104309\n","GRU Epoch 19, train_loss 0.721, val_loss 1.65, val_mrrmse 1.223021388053894\n","\n","Split 5/5...\n","BEST ----> \n","LSTM Epoch 0, train_loss 2.052, val_loss 1.477, val_mrrmse 1.1009747982025146\n","BEST ----> \n","LSTM Epoch 1, train_loss 1.938, val_loss 1.189, val_mrrmse 1.0325202941894531\n","LSTM Epoch 2, train_loss 1.798, val_loss 1.177, val_mrrmse 1.0529100894927979\n","LSTM Epoch 3, train_loss 1.769, val_loss 1.524, val_mrrmse 1.1181668043136597\n","LSTM Epoch 4, train_loss 1.768, val_loss 1.155, val_mrrmse 1.044053077697754\n","LSTM Epoch 5, train_loss 1.551, val_loss 1.445, val_mrrmse 1.1108074188232422\n","BEST ----> \n","LSTM Epoch 6, train_loss 1.485, val_loss 1.234, val_mrrmse 1.021904468536377\n","LSTM Epoch 7, train_loss 1.578, val_loss 1.117, val_mrrmse 1.0407648086547852\n","LSTM Epoch 8, train_loss 1.428, val_loss 1.301, val_mrrmse 1.046618938446045\n","BEST ----> \n","LSTM Epoch 9, train_loss 1.3, val_loss 1.053, val_mrrmse 0.9857574701309204\n","LSTM Epoch 10, train_loss 1.297, val_loss 1.155, val_mrrmse 1.0017082691192627\n","LSTM Epoch 11, train_loss 1.145, val_loss 1.052, val_mrrmse 1.0047571659088135\n","LSTM Epoch 12, train_loss 1.081, val_loss 1.173, val_mrrmse 1.015715479850769\n","LSTM Epoch 13, train_loss 1.111, val_loss 1.15, val_mrrmse 1.0043742656707764\n","LSTM Epoch 14, train_loss 1.065, val_loss 1.07, val_mrrmse 1.0081841945648193\n","LSTM Epoch 15, train_loss 1.015, val_loss 1.032, val_mrrmse 0.9995582103729248\n","LSTM Epoch 16, train_loss 0.903, val_loss 1.144, val_mrrmse 0.9983003735542297\n","LSTM Epoch 17, train_loss 0.939, val_loss 1.092, val_mrrmse 1.0046178102493286\n","BEST ----> \n","LSTM Epoch 18, train_loss 0.931, val_loss 0.985, val_mrrmse 0.946397066116333\n","LSTM Epoch 19, train_loss 0.837, val_loss 1.057, val_mrrmse 0.9795464277267456\n","BEST ----> \n","Conv Epoch 0, train_loss 1.88, val_loss 1.155, val_mrrmse 1.0303277969360352\n","Conv Epoch 1, train_loss 1.906, val_loss 1.304, val_mrrmse 1.0497431755065918\n","BEST ----> \n","Conv Epoch 2, train_loss 1.819, val_loss 1.16, val_mrrmse 1.0251214504241943\n","Conv Epoch 3, train_loss 1.734, val_loss 1.12, val_mrrmse 1.0252058506011963\n","Conv Epoch 4, train_loss 1.515, val_loss 1.459, val_mrrmse 1.0620900392532349\n","Conv Epoch 5, train_loss 1.641, val_loss 1.24, val_mrrmse 1.0433202981948853\n","BEST ----> \n","Conv Epoch 6, train_loss 1.687, val_loss 1.236, val_mrrmse 1.017188549041748\n","Conv Epoch 7, train_loss 1.568, val_loss 1.349, val_mrrmse 1.0614274740219116\n","BEST ----> \n","Conv Epoch 8, train_loss 1.735, val_loss 1.133, val_mrrmse 1.0008881092071533\n","Conv Epoch 9, train_loss 1.481, val_loss 1.248, val_mrrmse 1.0237129926681519\n","BEST ----> \n","Conv Epoch 10, train_loss 1.407, val_loss 1.162, val_mrrmse 0.9952090978622437\n","BEST ----> \n","Conv Epoch 11, train_loss 1.217, val_loss 1.146, val_mrrmse 0.9817382097244263\n","Conv Epoch 12, train_loss 1.126, val_loss 1.157, val_mrrmse 0.9914374351501465\n","BEST ----> \n","Conv Epoch 13, train_loss 1.144, val_loss 1.045, val_mrrmse 0.9628510475158691\n","Conv Epoch 14, train_loss 0.987, val_loss 1.182, val_mrrmse 0.9841745495796204\n","BEST ----> \n","Conv Epoch 15, train_loss 1.133, val_loss 0.99, val_mrrmse 0.9444884657859802\n","Conv Epoch 16, train_loss 1.01, val_loss 1.026, val_mrrmse 0.947136402130127\n","Conv Epoch 17, train_loss 0.963, val_loss 1.032, val_mrrmse 0.9549742937088013\n","Conv Epoch 18, train_loss 0.946, val_loss 1.034, val_mrrmse 0.9471980929374695\n","Conv Epoch 19, train_loss 0.946, val_loss 1.019, val_mrrmse 0.9452431201934814\n","lr 0.0003\n","BEST ----> \n","GRU Epoch 0, train_loss 2.056, val_loss 1.234, val_mrrmse 1.0523029565811157\n","BEST ----> \n","GRU Epoch 1, train_loss 1.735, val_loss 1.168, val_mrrmse 1.0477614402770996\n","BEST ----> \n","GRU Epoch 2, train_loss 1.631, val_loss 1.116, val_mrrmse 1.0207197666168213\n","BEST ----> \n","GRU Epoch 3, train_loss 1.524, val_loss 1.16, val_mrrmse 1.0100936889648438\n","BEST ----> \n","GRU Epoch 4, train_loss 1.411, val_loss 1.122, val_mrrmse 1.0088067054748535\n","BEST ----> \n","GRU Epoch 5, train_loss 1.269, val_loss 1.067, val_mrrmse 0.984320878982544\n","GRU Epoch 6, train_loss 1.224, val_loss 1.329, val_mrrmse 1.0234307050704956\n","GRU Epoch 7, train_loss 1.193, val_loss 1.125, val_mrrmse 0.9869928359985352\n","BEST ----> \n","GRU Epoch 8, train_loss 1.02, val_loss 1.079, val_mrrmse 0.9748948812484741\n","GRU Epoch 9, train_loss 1.021, val_loss 1.097, val_mrrmse 0.9781320095062256\n","BEST ----> \n","GRU Epoch 10, train_loss 0.955, val_loss 1.094, val_mrrmse 0.9577150344848633\n","BEST ----> \n","GRU Epoch 11, train_loss 0.935, val_loss 1.006, val_mrrmse 0.9497748613357544\n","BEST ----> \n","GRU Epoch 12, train_loss 0.908, val_loss 0.971, val_mrrmse 0.937136173248291\n","BEST ----> \n","GRU Epoch 13, train_loss 0.881, val_loss 0.945, val_mrrmse 0.93642258644104\n","GRU Epoch 14, train_loss 0.8, val_loss 1.007, val_mrrmse 0.9367234706878662\n","BEST ----> \n","GRU Epoch 15, train_loss 0.769, val_loss 1.004, val_mrrmse 0.9275541305541992\n","BEST ----> \n","GRU Epoch 16, train_loss 0.742, val_loss 0.911, val_mrrmse 0.9122669696807861\n","GRU Epoch 17, train_loss 0.728, val_loss 0.958, val_mrrmse 0.917995810508728\n","GRU Epoch 18, train_loss 0.776, val_loss 0.983, val_mrrmse 0.925781786441803\n","GRU Epoch 19, train_loss 0.736, val_loss 0.909, val_mrrmse 0.9139504432678223\n","-----Seed Set!-----\n","\n","Split 1/5...\n","BEST ----> \n","LSTM Epoch 0, train_loss 2.136, val_loss 1.224, val_mrrmse 1.0161433219909668\n","LSTM Epoch 1, train_loss 1.848, val_loss 1.254, val_mrrmse 1.020930290222168\n","LSTM Epoch 2, train_loss 1.785, val_loss 1.253, val_mrrmse 1.0170460939407349\n","BEST ----> \n","LSTM Epoch 3, train_loss 1.886, val_loss 1.201, val_mrrmse 1.0069355964660645\n","BEST ----> \n","LSTM Epoch 4, train_loss 1.586, val_loss 1.307, val_mrrmse 1.0044171810150146\n","LSTM Epoch 5, train_loss 1.508, val_loss 1.244, val_mrrmse 1.0081899166107178\n","LSTM Epoch 6, train_loss 1.39, val_loss 1.95, val_mrrmse 1.0822062492370605\n","BEST ----> \n","LSTM Epoch 7, train_loss 1.441, val_loss 1.346, val_mrrmse 0.9954822659492493\n","LSTM Epoch 8, train_loss 1.356, val_loss 1.339, val_mrrmse 1.0011951923370361\n","LSTM Epoch 9, train_loss 1.278, val_loss 1.289, val_mrrmse 1.012598991394043\n","LSTM Epoch 10, train_loss 1.265, val_loss 1.753, val_mrrmse 1.056172490119934\n","BEST ----> \n","LSTM Epoch 11, train_loss 1.177, val_loss 1.326, val_mrrmse 0.98360276222229\n","LSTM Epoch 12, train_loss 1.249, val_loss 1.259, val_mrrmse 0.9843796491622925\n","BEST ----> \n","LSTM Epoch 13, train_loss 1.064, val_loss 1.049, val_mrrmse 0.9348973035812378\n","LSTM Epoch 14, train_loss 1.121, val_loss 1.374, val_mrrmse 0.9953820705413818\n","LSTM Epoch 15, train_loss 1.077, val_loss 1.243, val_mrrmse 0.9645874500274658\n","LSTM Epoch 16, train_loss 1.014, val_loss 1.599, val_mrrmse 1.0312036275863647\n","LSTM Epoch 17, train_loss 0.994, val_loss 1.179, val_mrrmse 0.9543003439903259\n","LSTM Epoch 18, train_loss 0.997, val_loss 1.46, val_mrrmse 0.9989299774169922\n","LSTM Epoch 19, train_loss 0.955, val_loss 1.638, val_mrrmse 1.019940733909607\n","BEST ----> \n","Conv Epoch 0, train_loss 2.174, val_loss 1.19, val_mrrmse 1.0110435485839844\n","Conv Epoch 1, train_loss 1.847, val_loss 1.612, val_mrrmse 1.0700112581253052\n","BEST ----> \n","Conv Epoch 2, train_loss 1.972, val_loss 1.182, val_mrrmse 0.9905226230621338\n","BEST ----> \n","Conv Epoch 3, train_loss 1.762, val_loss 1.156, val_mrrmse 0.9870405197143555\n","Conv Epoch 4, train_loss 1.736, val_loss 1.242, val_mrrmse 1.0103310346603394\n","BEST ----> \n","Conv Epoch 5, train_loss 1.678, val_loss 1.058, val_mrrmse 0.9701983332633972\n","Conv Epoch 6, train_loss 1.691, val_loss 1.185, val_mrrmse 0.9947307109832764\n","Conv Epoch 7, train_loss 1.711, val_loss 1.206, val_mrrmse 0.9747377634048462\n","BEST ----> \n","Conv Epoch 8, train_loss 1.614, val_loss 1.154, val_mrrmse 0.9653182029724121\n","BEST ----> \n","Conv Epoch 9, train_loss 1.466, val_loss 1.025, val_mrrmse 0.946792721748352\n","Conv Epoch 10, train_loss 1.378, val_loss 1.211, val_mrrmse 0.9720250368118286\n","BEST ----> \n","Conv Epoch 11, train_loss 1.545, val_loss 0.983, val_mrrmse 0.9428167939186096\n","BEST ----> \n","Conv Epoch 12, train_loss 1.214, val_loss 1.036, val_mrrmse 0.9310182929039001\n","Conv Epoch 13, train_loss 1.129, val_loss 1.224, val_mrrmse 0.9890493154525757\n","Conv Epoch 14, train_loss 1.059, val_loss 1.041, val_mrrmse 0.9343225955963135\n","Conv Epoch 15, train_loss 1.145, val_loss 0.951, val_mrrmse 0.932330310344696\n","BEST ----> \n","Conv Epoch 16, train_loss 1.072, val_loss 0.934, val_mrrmse 0.8987874984741211\n","Conv Epoch 17, train_loss 1.017, val_loss 1.019, val_mrrmse 0.914169192314148\n","BEST ----> \n","Conv Epoch 18, train_loss 0.985, val_loss 0.904, val_mrrmse 0.8901728391647339\n","Conv Epoch 19, train_loss 0.97, val_loss 0.973, val_mrrmse 0.9245576858520508\n","lr 0.0003\n","BEST ----> \n","GRU Epoch 0, train_loss 2.084, val_loss 1.161, val_mrrmse 0.9961094260215759\n","GRU Epoch 1, train_loss 1.78, val_loss 1.237, val_mrrmse 1.0113551616668701\n","GRU Epoch 2, train_loss 1.626, val_loss 1.196, val_mrrmse 1.0029346942901611\n","GRU Epoch 3, train_loss 1.562, val_loss 1.199, val_mrrmse 1.000533103942871\n","BEST ----> \n","GRU Epoch 4, train_loss 1.488, val_loss 1.14, val_mrrmse 0.9768314957618713\n","GRU Epoch 5, train_loss 1.421, val_loss 1.198, val_mrrmse 0.9850291013717651\n","BEST ----> \n","GRU Epoch 6, train_loss 1.244, val_loss 1.027, val_mrrmse 0.9386153221130371\n","BEST ----> \n","GRU Epoch 7, train_loss 1.214, val_loss 0.941, val_mrrmse 0.923878014087677\n","BEST ----> \n","GRU Epoch 8, train_loss 1.091, val_loss 0.982, val_mrrmse 0.9234346151351929\n","GRU Epoch 9, train_loss 1.067, val_loss 1.169, val_mrrmse 0.9495507478713989\n","BEST ----> \n","GRU Epoch 10, train_loss 1.047, val_loss 0.903, val_mrrmse 0.8978766798973083\n","GRU Epoch 11, train_loss 0.933, val_loss 0.902, val_mrrmse 0.8988725543022156\n","GRU Epoch 12, train_loss 0.907, val_loss 1.07, val_mrrmse 0.9613090753555298\n","GRU Epoch 13, train_loss 0.906, val_loss 0.885, val_mrrmse 0.8982571959495544\n","GRU Epoch 14, train_loss 0.866, val_loss 0.963, val_mrrmse 0.8987535834312439\n","GRU Epoch 15, train_loss 0.848, val_loss 1.104, val_mrrmse 0.9338567852973938\n","BEST ----> \n","GRU Epoch 16, train_loss 0.87, val_loss 1.002, val_mrrmse 0.8969610929489136\n","GRU Epoch 17, train_loss 0.815, val_loss 1.159, val_mrrmse 0.9182342290878296\n","GRU Epoch 18, train_loss 0.8, val_loss 1.091, val_mrrmse 0.9230539798736572\n","GRU Epoch 19, train_loss 0.77, val_loss 1.111, val_mrrmse 0.9231881499290466\n","\n","Split 2/5...\n","BEST ----> \n","LSTM Epoch 0, train_loss 1.684, val_loss 3.058, val_mrrmse 1.478121280670166\n","BEST ----> \n","LSTM Epoch 1, train_loss 1.402, val_loss 3.213, val_mrrmse 1.4456168413162231\n","BEST ----> \n","LSTM Epoch 2, train_loss 1.363, val_loss 3.089, val_mrrmse 1.4175479412078857\n","LSTM Epoch 3, train_loss 1.346, val_loss 3.568, val_mrrmse 1.501183032989502\n","BEST ----> \n","LSTM Epoch 4, train_loss 1.286, val_loss 2.852, val_mrrmse 1.3865094184875488\n","LSTM Epoch 5, train_loss 1.26, val_loss 2.78, val_mrrmse 1.388277530670166\n","LSTM Epoch 6, train_loss 1.24, val_loss 3.151, val_mrrmse 1.4736645221710205\n","LSTM Epoch 7, train_loss 1.123, val_loss 3.135, val_mrrmse 1.454211950302124\n","LSTM Epoch 8, train_loss 0.953, val_loss 3.011, val_mrrmse 1.4264110326766968\n","LSTM Epoch 9, train_loss 0.924, val_loss 3.076, val_mrrmse 1.4462833404541016\n","LSTM Epoch 10, train_loss 0.939, val_loss 3.157, val_mrrmse 1.4166653156280518\n","LSTM Epoch 11, train_loss 0.923, val_loss 3.043, val_mrrmse 1.410529613494873\n","LSTM Epoch 12, train_loss 0.874, val_loss 3.126, val_mrrmse 1.4050041437149048\n","BEST ----> \n","LSTM Epoch 13, train_loss 0.855, val_loss 2.969, val_mrrmse 1.3774060010910034\n","BEST ----> \n","LSTM Epoch 14, train_loss 0.769, val_loss 3.004, val_mrrmse 1.3683162927627563\n","LSTM Epoch 15, train_loss 0.754, val_loss 3.172, val_mrrmse 1.4044049978256226\n","LSTM Epoch 16, train_loss 0.821, val_loss 3.034, val_mrrmse 1.3908095359802246\n","LSTM Epoch 17, train_loss 0.791, val_loss 3.152, val_mrrmse 1.3949576616287231\n","LSTM Epoch 18, train_loss 0.793, val_loss 2.988, val_mrrmse 1.3865565061569214\n","BEST ----> \n","LSTM Epoch 19, train_loss 0.772, val_loss 2.826, val_mrrmse 1.3654271364212036\n","BEST ----> \n","Conv Epoch 0, train_loss 1.667, val_loss 2.801, val_mrrmse 1.3935463428497314\n","Conv Epoch 1, train_loss 1.404, val_loss 2.927, val_mrrmse 1.43779718875885\n","Conv Epoch 2, train_loss 1.482, val_loss 2.903, val_mrrmse 1.406468152999878\n","BEST ----> \n","Conv Epoch 3, train_loss 1.377, val_loss 2.911, val_mrrmse 1.3727476596832275\n","Conv Epoch 4, train_loss 1.33, val_loss 3.048, val_mrrmse 1.4063811302185059\n","BEST ----> \n","Conv Epoch 5, train_loss 1.322, val_loss 2.728, val_mrrmse 1.3547598123550415\n","Conv Epoch 6, train_loss 1.374, val_loss 2.857, val_mrrmse 1.3657400608062744\n","Conv Epoch 7, train_loss 1.164, val_loss 2.951, val_mrrmse 1.37786865234375\n","Conv Epoch 8, train_loss 1.178, val_loss 2.815, val_mrrmse 1.3609693050384521\n","Conv Epoch 9, train_loss 1.123, val_loss 2.868, val_mrrmse 1.3629870414733887\n","BEST ----> \n","Conv Epoch 10, train_loss 1.121, val_loss 2.829, val_mrrmse 1.3443214893341064\n","Conv Epoch 11, train_loss 1.039, val_loss 2.695, val_mrrmse 1.3468108177185059\n","BEST ----> \n","Conv Epoch 12, train_loss 1.06, val_loss 2.555, val_mrrmse 1.3194279670715332\n","Conv Epoch 13, train_loss 1.0, val_loss 2.835, val_mrrmse 1.3333938121795654\n","Conv Epoch 14, train_loss 0.972, val_loss 3.154, val_mrrmse 1.3737019300460815\n","BEST ----> \n","Conv Epoch 15, train_loss 0.938, val_loss 2.508, val_mrrmse 1.2721494436264038\n","Conv Epoch 16, train_loss 0.89, val_loss 2.87, val_mrrmse 1.3068819046020508\n","BEST ----> \n","Conv Epoch 17, train_loss 0.812, val_loss 2.507, val_mrrmse 1.2695603370666504\n","BEST ----> \n","Conv Epoch 18, train_loss 0.822, val_loss 2.579, val_mrrmse 1.2440013885498047\n","Conv Epoch 19, train_loss 0.764, val_loss 2.559, val_mrrmse 1.250403642654419\n","lr 0.0003\n","BEST ----> \n","GRU Epoch 0, train_loss 1.53, val_loss 2.92, val_mrrmse 1.41990327835083\n","BEST ----> \n","GRU Epoch 1, train_loss 1.35, val_loss 2.949, val_mrrmse 1.4189965724945068\n","BEST ----> \n","GRU Epoch 2, train_loss 1.241, val_loss 2.9, val_mrrmse 1.393294095993042\n","GRU Epoch 3, train_loss 1.154, val_loss 3.256, val_mrrmse 1.4568840265274048\n","BEST ----> \n","GRU Epoch 4, train_loss 1.132, val_loss 2.907, val_mrrmse 1.390535831451416\n","BEST ----> \n","GRU Epoch 5, train_loss 1.019, val_loss 2.816, val_mrrmse 1.3727421760559082\n","GRU Epoch 6, train_loss 0.919, val_loss 2.94, val_mrrmse 1.3729791641235352\n","BEST ----> \n","GRU Epoch 7, train_loss 0.918, val_loss 2.819, val_mrrmse 1.3482213020324707\n","BEST ----> \n","GRU Epoch 8, train_loss 0.862, val_loss 2.547, val_mrrmse 1.284814715385437\n","BEST ----> \n","GRU Epoch 9, train_loss 0.808, val_loss 2.48, val_mrrmse 1.2812741994857788\n","GRU Epoch 10, train_loss 0.795, val_loss 2.553, val_mrrmse 1.2979748249053955\n","GRU Epoch 11, train_loss 0.789, val_loss 2.681, val_mrrmse 1.3018991947174072\n","BEST ----> \n","GRU Epoch 12, train_loss 0.764, val_loss 2.43, val_mrrmse 1.2477068901062012\n","BEST ----> \n","GRU Epoch 13, train_loss 0.737, val_loss 2.47, val_mrrmse 1.2389709949493408\n","GRU Epoch 14, train_loss 0.696, val_loss 2.564, val_mrrmse 1.2722594738006592\n","BEST ----> \n","GRU Epoch 15, train_loss 0.691, val_loss 2.404, val_mrrmse 1.2195160388946533\n","GRU Epoch 16, train_loss 0.671, val_loss 2.391, val_mrrmse 1.2201974391937256\n","GRU Epoch 17, train_loss 0.681, val_loss 2.476, val_mrrmse 1.2377140522003174\n","BEST ----> \n","GRU Epoch 18, train_loss 0.625, val_loss 2.439, val_mrrmse 1.2162439823150635\n","GRU Epoch 19, train_loss 0.654, val_loss 2.517, val_mrrmse 1.219139814376831\n","\n","Split 3/5...\n","BEST ----> \n","LSTM Epoch 0, train_loss 2.109, val_loss 1.119, val_mrrmse 1.0500226020812988\n","LSTM Epoch 1, train_loss 1.858, val_loss 1.234, val_mrrmse 1.0757761001586914\n","LSTM Epoch 2, train_loss 1.766, val_loss 1.153, val_mrrmse 1.059360146522522\n","LSTM Epoch 3, train_loss 1.879, val_loss 1.725, val_mrrmse 1.1717736721038818\n","LSTM Epoch 4, train_loss 1.736, val_loss 1.601, val_mrrmse 1.1141879558563232\n","BEST ----> \n","LSTM Epoch 5, train_loss 1.672, val_loss 1.015, val_mrrmse 1.0188825130462646\n","LSTM Epoch 6, train_loss 1.453, val_loss 1.108, val_mrrmse 1.0509945154190063\n","LSTM Epoch 7, train_loss 1.466, val_loss 1.064, val_mrrmse 1.0520522594451904\n","BEST ----> \n","LSTM Epoch 8, train_loss 1.463, val_loss 0.949, val_mrrmse 0.992477536201477\n","BEST ----> \n","LSTM Epoch 9, train_loss 1.393, val_loss 0.901, val_mrrmse 0.9718694090843201\n","LSTM Epoch 10, train_loss 1.282, val_loss 0.963, val_mrrmse 0.995191216468811\n","LSTM Epoch 11, train_loss 1.315, val_loss 1.067, val_mrrmse 1.0113556385040283\n","LSTM Epoch 12, train_loss 1.228, val_loss 1.056, val_mrrmse 1.0078253746032715\n","LSTM Epoch 13, train_loss 1.174, val_loss 1.098, val_mrrmse 1.0032634735107422\n","BEST ----> \n","LSTM Epoch 14, train_loss 1.103, val_loss 0.87, val_mrrmse 0.948735237121582\n","BEST ----> \n","LSTM Epoch 15, train_loss 1.048, val_loss 0.871, val_mrrmse 0.9438896775245667\n","BEST ----> \n","LSTM Epoch 16, train_loss 1.051, val_loss 0.786, val_mrrmse 0.9198921322822571\n","LSTM Epoch 17, train_loss 0.938, val_loss 0.77, val_mrrmse 0.9309379458427429\n","LSTM Epoch 18, train_loss 0.919, val_loss 0.782, val_mrrmse 0.9455322623252869\n","LSTM Epoch 19, train_loss 1.005, val_loss 0.871, val_mrrmse 0.9395691156387329\n","BEST ----> \n","Conv Epoch 0, train_loss 1.942, val_loss 2.069, val_mrrmse 1.2082624435424805\n","BEST ----> \n","Conv Epoch 1, train_loss 2.071, val_loss 1.019, val_mrrmse 1.01510751247406\n","Conv Epoch 2, train_loss 1.822, val_loss 1.338, val_mrrmse 1.0823962688446045\n","Conv Epoch 3, train_loss 1.786, val_loss 1.081, val_mrrmse 1.028026819229126\n","Conv Epoch 4, train_loss 1.723, val_loss 1.276, val_mrrmse 1.0607572793960571\n","Conv Epoch 5, train_loss 1.678, val_loss 1.247, val_mrrmse 1.058178424835205\n","Conv Epoch 6, train_loss 1.566, val_loss 1.11, val_mrrmse 1.0332887172698975\n","Conv Epoch 7, train_loss 1.636, val_loss 1.165, val_mrrmse 1.0435444116592407\n","BEST ----> \n","Conv Epoch 8, train_loss 1.399, val_loss 1.039, val_mrrmse 1.009207844734192\n","Conv Epoch 9, train_loss 1.408, val_loss 1.276, val_mrrmse 1.0657917261123657\n","BEST ----> \n","Conv Epoch 10, train_loss 1.339, val_loss 1.052, val_mrrmse 1.0005420446395874\n","Conv Epoch 11, train_loss 1.336, val_loss 1.099, val_mrrmse 1.0126756429672241\n","Conv Epoch 12, train_loss 1.504, val_loss 1.065, val_mrrmse 1.0089346170425415\n","Conv Epoch 13, train_loss 1.176, val_loss 1.258, val_mrrmse 1.0284550189971924\n","BEST ----> \n","Conv Epoch 14, train_loss 1.199, val_loss 0.905, val_mrrmse 0.9585861563682556\n","BEST ----> \n","Conv Epoch 15, train_loss 1.153, val_loss 0.892, val_mrrmse 0.9578150510787964\n","Conv Epoch 16, train_loss 1.189, val_loss 1.009, val_mrrmse 0.9817931652069092\n","BEST ----> \n","Conv Epoch 17, train_loss 1.178, val_loss 0.91, val_mrrmse 0.9560979604721069\n","BEST ----> \n","Conv Epoch 18, train_loss 1.077, val_loss 0.875, val_mrrmse 0.9495663046836853\n","BEST ----> \n","Conv Epoch 19, train_loss 1.048, val_loss 0.828, val_mrrmse 0.9308909773826599\n","lr 0.0003\n","BEST ----> \n","GRU Epoch 0, train_loss 2.242, val_loss 1.288, val_mrrmse 1.0754929780960083\n","GRU Epoch 1, train_loss 1.971, val_loss 1.258, val_mrrmse 1.088362216949463\n","GRU Epoch 2, train_loss 1.715, val_loss 1.464, val_mrrmse 1.1146631240844727\n","BEST ----> \n","GRU Epoch 3, train_loss 1.601, val_loss 1.029, val_mrrmse 1.0256582498550415\n","GRU Epoch 4, train_loss 1.46, val_loss 1.152, val_mrrmse 1.046292781829834\n","BEST ----> \n","GRU Epoch 5, train_loss 1.388, val_loss 0.963, val_mrrmse 0.9984747171401978\n","BEST ----> \n","GRU Epoch 6, train_loss 1.242, val_loss 0.94, val_mrrmse 0.9835019707679749\n","BEST ----> \n","GRU Epoch 7, train_loss 1.156, val_loss 0.862, val_mrrmse 0.9519409537315369\n","GRU Epoch 8, train_loss 1.091, val_loss 1.045, val_mrrmse 0.9949049353599548\n","GRU Epoch 9, train_loss 1.022, val_loss 0.902, val_mrrmse 0.9706423878669739\n","BEST ----> \n","GRU Epoch 10, train_loss 0.996, val_loss 0.795, val_mrrmse 0.9196407794952393\n","GRU Epoch 11, train_loss 0.983, val_loss 0.746, val_mrrmse 0.9238101243972778\n","BEST ----> \n","GRU Epoch 12, train_loss 1.04, val_loss 0.782, val_mrrmse 0.9042956233024597\n","BEST ----> \n","GRU Epoch 13, train_loss 0.899, val_loss 0.75, val_mrrmse 0.9041939973831177\n","BEST ----> \n","GRU Epoch 14, train_loss 0.886, val_loss 0.716, val_mrrmse 0.8799760937690735\n","GRU Epoch 15, train_loss 0.86, val_loss 0.734, val_mrrmse 0.9003324508666992\n","GRU Epoch 16, train_loss 0.847, val_loss 0.703, val_mrrmse 0.8836315274238586\n","BEST ----> \n","GRU Epoch 17, train_loss 0.795, val_loss 0.693, val_mrrmse 0.8691886067390442\n","BEST ----> \n","GRU Epoch 18, train_loss 0.818, val_loss 0.698, val_mrrmse 0.8641997575759888\n","BEST ----> \n","GRU Epoch 19, train_loss 0.74, val_loss 0.679, val_mrrmse 0.8596481680870056\n","\n","Split 4/5...\n","BEST ----> \n","LSTM Epoch 0, train_loss 1.792, val_loss 1.771, val_mrrmse 1.3482732772827148\n","LSTM Epoch 1, train_loss 1.769, val_loss 2.357, val_mrrmse 1.4484792947769165\n","LSTM Epoch 2, train_loss 1.637, val_loss 2.523, val_mrrmse 1.4743629693984985\n","BEST ----> \n","LSTM Epoch 3, train_loss 1.631, val_loss 1.722, val_mrrmse 1.3171000480651855\n","LSTM Epoch 4, train_loss 1.526, val_loss 1.847, val_mrrmse 1.3612087965011597\n","BEST ----> \n","LSTM Epoch 5, train_loss 1.478, val_loss 1.745, val_mrrmse 1.3064066171646118\n","LSTM Epoch 6, train_loss 1.416, val_loss 1.909, val_mrrmse 1.3548989295959473\n","LSTM Epoch 7, train_loss 1.362, val_loss 2.032, val_mrrmse 1.375016212463379\n","BEST ----> \n","LSTM Epoch 8, train_loss 1.238, val_loss 1.609, val_mrrmse 1.291467308998108\n","LSTM Epoch 9, train_loss 1.198, val_loss 1.987, val_mrrmse 1.3677374124526978\n","LSTM Epoch 10, train_loss 1.134, val_loss 1.826, val_mrrmse 1.3174853324890137\n","LSTM Epoch 11, train_loss 1.076, val_loss 1.807, val_mrrmse 1.3330018520355225\n","LSTM Epoch 12, train_loss 0.99, val_loss 1.953, val_mrrmse 1.3624635934829712\n","LSTM Epoch 13, train_loss 0.977, val_loss 2.086, val_mrrmse 1.3761942386627197\n","LSTM Epoch 14, train_loss 1.064, val_loss 1.798, val_mrrmse 1.3217580318450928\n","BEST ----> \n","LSTM Epoch 15, train_loss 0.889, val_loss 1.556, val_mrrmse 1.253491759300232\n","LSTM Epoch 16, train_loss 0.951, val_loss 2.043, val_mrrmse 1.3768150806427002\n","LSTM Epoch 17, train_loss 0.909, val_loss 1.714, val_mrrmse 1.2998747825622559\n","LSTM Epoch 18, train_loss 0.825, val_loss 1.591, val_mrrmse 1.2705423831939697\n","BEST ----> \n","LSTM Epoch 19, train_loss 0.877, val_loss 1.499, val_mrrmse 1.2456730604171753\n","BEST ----> \n","Conv Epoch 0, train_loss 1.734, val_loss 1.805, val_mrrmse 1.350396990776062\n","Conv Epoch 1, train_loss 1.666, val_loss 2.11, val_mrrmse 1.3940373659133911\n","BEST ----> \n","Conv Epoch 2, train_loss 1.734, val_loss 1.841, val_mrrmse 1.3428500890731812\n","BEST ----> \n","Conv Epoch 3, train_loss 1.711, val_loss 1.781, val_mrrmse 1.3213449716567993\n","Conv Epoch 4, train_loss 1.57, val_loss 1.775, val_mrrmse 1.359484076499939\n","Conv Epoch 5, train_loss 1.532, val_loss 2.234, val_mrrmse 1.3968513011932373\n","Conv Epoch 6, train_loss 1.68, val_loss 1.95, val_mrrmse 1.346184492111206\n","Conv Epoch 7, train_loss 1.483, val_loss 2.54, val_mrrmse 1.4762544631958008\n","Conv Epoch 8, train_loss 1.494, val_loss 2.099, val_mrrmse 1.3938366174697876\n","Conv Epoch 9, train_loss 1.432, val_loss 1.89, val_mrrmse 1.3380513191223145\n","Conv Epoch 10, train_loss 1.316, val_loss 1.924, val_mrrmse 1.3630818128585815\n","Conv Epoch 11, train_loss 1.215, val_loss 3.102, val_mrrmse 1.6108369827270508\n","Conv Epoch 12, train_loss 1.251, val_loss 1.959, val_mrrmse 1.326480507850647\n","Conv Epoch 13, train_loss 1.15, val_loss 2.221, val_mrrmse 1.4172412157058716\n","Conv Epoch 14, train_loss 1.032, val_loss 2.019, val_mrrmse 1.3735584020614624\n","Conv Epoch 15, train_loss 1.093, val_loss 2.504, val_mrrmse 1.452410101890564\n","BEST ----> \n","Conv Epoch 16, train_loss 1.103, val_loss 1.764, val_mrrmse 1.2922263145446777\n","BEST ----> \n","Conv Epoch 17, train_loss 1.016, val_loss 1.575, val_mrrmse 1.2697064876556396\n","Conv Epoch 18, train_loss 0.903, val_loss 1.924, val_mrrmse 1.3259379863739014\n","BEST ----> \n","Conv Epoch 19, train_loss 1.034, val_loss 1.627, val_mrrmse 1.264555811882019\n","lr 0.0003\n","BEST ----> \n","GRU Epoch 0, train_loss 1.857, val_loss 1.987, val_mrrmse 1.367096185684204\n","GRU Epoch 1, train_loss 1.601, val_loss 2.007, val_mrrmse 1.3842201232910156\n","BEST ----> \n","GRU Epoch 2, train_loss 1.536, val_loss 1.856, val_mrrmse 1.3491417169570923\n","GRU Epoch 3, train_loss 1.432, val_loss 1.971, val_mrrmse 1.3901013135910034\n","GRU Epoch 4, train_loss 1.356, val_loss 1.98, val_mrrmse 1.3869996070861816\n","GRU Epoch 5, train_loss 1.274, val_loss 2.163, val_mrrmse 1.408591866493225\n","GRU Epoch 6, train_loss 1.174, val_loss 1.981, val_mrrmse 1.3669465780258179\n","GRU Epoch 7, train_loss 1.14, val_loss 1.913, val_mrrmse 1.3544126749038696\n","BEST ----> \n","GRU Epoch 8, train_loss 1.045, val_loss 1.624, val_mrrmse 1.270564079284668\n","BEST ----> \n","GRU Epoch 9, train_loss 1.023, val_loss 1.57, val_mrrmse 1.2567591667175293\n","GRU Epoch 10, train_loss 0.998, val_loss 1.804, val_mrrmse 1.3126611709594727\n","GRU Epoch 11, train_loss 0.943, val_loss 1.679, val_mrrmse 1.287401556968689\n","BEST ----> \n","GRU Epoch 12, train_loss 0.867, val_loss 1.509, val_mrrmse 1.2283650636672974\n","GRU Epoch 13, train_loss 0.862, val_loss 1.697, val_mrrmse 1.290041446685791\n","GRU Epoch 14, train_loss 0.85, val_loss 1.873, val_mrrmse 1.305968999862671\n","GRU Epoch 15, train_loss 0.819, val_loss 1.603, val_mrrmse 1.262811303138733\n","GRU Epoch 16, train_loss 0.796, val_loss 1.725, val_mrrmse 1.2854149341583252\n","GRU Epoch 17, train_loss 0.792, val_loss 1.964, val_mrrmse 1.3424478769302368\n","GRU Epoch 18, train_loss 0.757, val_loss 1.628, val_mrrmse 1.251800537109375\n","GRU Epoch 19, train_loss 0.779, val_loss 1.617, val_mrrmse 1.2410515546798706\n","\n","Split 5/5...\n","BEST ----> \n","LSTM Epoch 0, train_loss 2.057, val_loss 1.191, val_mrrmse 1.0390546321868896\n","LSTM Epoch 1, train_loss 1.853, val_loss 1.514, val_mrrmse 1.1154074668884277\n","LSTM Epoch 2, train_loss 1.823, val_loss 1.237, val_mrrmse 1.0691214799880981\n","LSTM Epoch 3, train_loss 1.677, val_loss 1.261, val_mrrmse 1.0732412338256836\n","LSTM Epoch 4, train_loss 1.625, val_loss 1.324, val_mrrmse 1.0876399278640747\n","BEST ----> \n","LSTM Epoch 5, train_loss 1.62, val_loss 1.12, val_mrrmse 1.0234276056289673\n","LSTM Epoch 6, train_loss 1.407, val_loss 1.313, val_mrrmse 1.03886079788208\n","LSTM Epoch 7, train_loss 1.399, val_loss 1.253, val_mrrmse 1.0422601699829102\n","LSTM Epoch 8, train_loss 1.333, val_loss 1.212, val_mrrmse 1.0469481945037842\n","LSTM Epoch 9, train_loss 1.241, val_loss 1.327, val_mrrmse 1.045654535293579\n","BEST ----> \n","LSTM Epoch 10, train_loss 1.234, val_loss 1.236, val_mrrmse 1.012776255607605\n","BEST ----> \n","LSTM Epoch 11, train_loss 1.14, val_loss 1.13, val_mrrmse 1.0015636682510376\n","LSTM Epoch 12, train_loss 1.131, val_loss 1.119, val_mrrmse 1.0087428092956543\n","LSTM Epoch 13, train_loss 1.056, val_loss 1.198, val_mrrmse 1.0213713645935059\n","BEST ----> \n","LSTM Epoch 14, train_loss 1.094, val_loss 1.152, val_mrrmse 0.9945660829544067\n","BEST ----> \n","LSTM Epoch 15, train_loss 1.071, val_loss 1.049, val_mrrmse 0.9933146238327026\n","BEST ----> \n","LSTM Epoch 16, train_loss 1.014, val_loss 1.011, val_mrrmse 0.9544228315353394\n","LSTM Epoch 17, train_loss 0.87, val_loss 1.004, val_mrrmse 0.9567216634750366\n","BEST ----> \n","LSTM Epoch 18, train_loss 0.946, val_loss 1.004, val_mrrmse 0.9508320689201355\n","LSTM Epoch 19, train_loss 0.902, val_loss 1.122, val_mrrmse 1.0068354606628418\n","BEST ----> \n","Conv Epoch 0, train_loss 2.04, val_loss 1.322, val_mrrmse 1.0706360340118408\n","BEST ----> \n","Conv Epoch 1, train_loss 1.825, val_loss 1.23, val_mrrmse 1.0422606468200684\n","Conv Epoch 2, train_loss 1.915, val_loss 1.257, val_mrrmse 1.0713589191436768\n","BEST ----> \n","Conv Epoch 3, train_loss 1.832, val_loss 1.121, val_mrrmse 0.9992920160293579\n","BEST ----> \n","Conv Epoch 4, train_loss 1.891, val_loss 1.088, val_mrrmse 0.997850775718689\n","Conv Epoch 5, train_loss 1.817, val_loss 1.266, val_mrrmse 1.0493152141571045\n","Conv Epoch 6, train_loss 1.633, val_loss 1.282, val_mrrmse 1.0460023880004883\n","BEST ----> \n","Conv Epoch 7, train_loss 1.594, val_loss 1.052, val_mrrmse 0.9779311418533325\n","Conv Epoch 8, train_loss 1.559, val_loss 1.126, val_mrrmse 0.9928697943687439\n","BEST ----> \n","Conv Epoch 9, train_loss 1.481, val_loss 1.004, val_mrrmse 0.9577763080596924\n","Conv Epoch 10, train_loss 1.384, val_loss 1.088, val_mrrmse 0.9738484621047974\n","Conv Epoch 11, train_loss 1.378, val_loss 1.212, val_mrrmse 1.0230655670166016\n","Conv Epoch 12, train_loss 1.19, val_loss 1.071, val_mrrmse 0.976556658744812\n","Conv Epoch 13, train_loss 1.213, val_loss 1.206, val_mrrmse 1.0194053649902344\n","Conv Epoch 14, train_loss 1.142, val_loss 1.044, val_mrrmse 0.9618014693260193\n","Conv Epoch 15, train_loss 1.012, val_loss 1.352, val_mrrmse 1.0628836154937744\n","BEST ----> \n","Conv Epoch 16, train_loss 1.026, val_loss 1.008, val_mrrmse 0.9499788284301758\n","Conv Epoch 17, train_loss 1.07, val_loss 1.117, val_mrrmse 0.9634580612182617\n","Conv Epoch 18, train_loss 1.064, val_loss 1.23, val_mrrmse 1.021655797958374\n","BEST ----> \n","Conv Epoch 19, train_loss 1.083, val_loss 1.034, val_mrrmse 0.9435595870018005\n","lr 0.0003\n","BEST ----> \n","GRU Epoch 0, train_loss 2.062, val_loss 1.184, val_mrrmse 1.0254771709442139\n","GRU Epoch 1, train_loss 1.784, val_loss 1.214, val_mrrmse 1.052533745765686\n","BEST ----> \n","GRU Epoch 2, train_loss 1.652, val_loss 1.173, val_mrrmse 1.012404203414917\n","BEST ----> \n","GRU Epoch 3, train_loss 1.489, val_loss 1.143, val_mrrmse 0.9962220191955566\n","BEST ----> \n","GRU Epoch 4, train_loss 1.426, val_loss 1.107, val_mrrmse 0.9949368238449097\n","GRU Epoch 5, train_loss 1.385, val_loss 1.256, val_mrrmse 1.0469454526901245\n","BEST ----> \n","GRU Epoch 6, train_loss 1.181, val_loss 1.066, val_mrrmse 0.9925942420959473\n","GRU Epoch 7, train_loss 1.113, val_loss 1.214, val_mrrmse 1.014566421508789\n","BEST ----> \n","GRU Epoch 8, train_loss 1.036, val_loss 1.005, val_mrrmse 0.9549412727355957\n","GRU Epoch 9, train_loss 1.024, val_loss 1.149, val_mrrmse 0.9837768077850342\n","GRU Epoch 10, train_loss 0.955, val_loss 1.082, val_mrrmse 0.9729114174842834\n","GRU Epoch 11, train_loss 0.946, val_loss 1.069, val_mrrmse 0.9642799496650696\n","BEST ----> \n","GRU Epoch 12, train_loss 0.898, val_loss 1.029, val_mrrmse 0.9501791000366211\n","GRU Epoch 13, train_loss 0.888, val_loss 1.0, val_mrrmse 0.9552497863769531\n","BEST ----> \n","GRU Epoch 14, train_loss 0.868, val_loss 0.955, val_mrrmse 0.9306607842445374\n","GRU Epoch 15, train_loss 0.803, val_loss 0.988, val_mrrmse 0.9399339556694031\n","BEST ----> \n","GRU Epoch 16, train_loss 0.81, val_loss 0.992, val_mrrmse 0.9252336025238037\n","GRU Epoch 17, train_loss 0.756, val_loss 1.08, val_mrrmse 0.9779074192047119\n","BEST ----> \n","GRU Epoch 18, train_loss 0.767, val_loss 0.978, val_mrrmse 0.9154019355773926\n","GRU Epoch 19, train_loss 0.751, val_loss 0.98, val_mrrmse 0.9238914251327515\n"]}],"source":["trained_models = reproduce(epochs=250)"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T09:00:56.011801Z","iopub.status.busy":"2024-04-25T09:00:56.011349Z","iopub.status.idle":"2024-04-25T09:01:43.286384Z","shell.execute_reply":"2024-04-25T09:01:43.285232Z","shell.execute_reply.started":"2024-04-25T09:00:56.011759Z"},"trusted":true},"outputs":[],"source":["pred1 = average_prediction(test_vec, trained_models['initial'])\n","pred2 = weighted_average_prediction(test_vec, trained_models['initial'],\\\n","                                    model_wise=[0.29, 0.33, 0.38], fold_wise=[0.25, 0.15, 0.2, 0.15, 0.25])"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T09:01:43.288300Z","iopub.status.busy":"2024-04-25T09:01:43.287952Z","iopub.status.idle":"2024-04-25T09:02:28.947189Z","shell.execute_reply":"2024-04-25T09:02:28.946078Z","shell.execute_reply.started":"2024-04-25T09:01:43.288270Z"},"trusted":true},"outputs":[],"source":["pred3 = average_prediction(test_vec_light, trained_models['light'])\n","pred4 = weighted_average_prediction(test_vec_light, trained_models['light'],\\\n","                                    model_wise=[0.29, 0.33, 0.38], fold_wise=[0.25, 0.15, 0.2, 0.15, 0.25])"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T09:02:28.949034Z","iopub.status.busy":"2024-04-25T09:02:28.948697Z","iopub.status.idle":"2024-04-25T09:03:33.255773Z","shell.execute_reply":"2024-04-25T09:03:33.249783Z","shell.execute_reply.started":"2024-04-25T09:02:28.949004Z"},"trusted":true},"outputs":[],"source":["pred5 = average_prediction(test_vec_heavy, trained_models['heavy'])\n","pred6 = weighted_average_prediction(test_vec_heavy, trained_models['heavy'],\\\n","                                    model_wise=[0.29, 0.33, 0.38], fold_wise=[0.25, 0.15, 0.2, 0.15, 0.25])"]},{"cell_type":"markdown","metadata":{},"source":["## Read Submission Sample File"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T09:03:33.258632Z","iopub.status.busy":"2024-04-25T09:03:33.258167Z","iopub.status.idle":"2024-04-25T09:03:33.288955Z","shell.execute_reply":"2024-04-25T09:03:33.287835Z","shell.execute_reply.started":"2024-04-25T09:03:33.258589Z"},"trusted":true},"outputs":[],"source":["col = list(de_train.columns[5:])\n","submission = sample_submission.copy()"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T09:03:33.292564Z","iopub.status.busy":"2024-04-25T09:03:33.292300Z","iopub.status.idle":"2024-04-25T09:03:40.052441Z","shell.execute_reply":"2024-04-25T09:03:40.051635Z","shell.execute_reply.started":"2024-04-25T09:03:33.292541Z"},"trusted":true},"outputs":[],"source":["submission[col] = 0.18*pred1 + 0.15*pred2 + 0.23*pred3 + 0.15*pred4 + 0.15*pred5 + 0.14*pred6\n","df1 = submission.copy()"]},{"cell_type":"markdown","metadata":{},"source":["## Ensemble Prediction"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T09:03:40.053839Z","iopub.status.busy":"2024-04-25T09:03:40.053546Z","iopub.status.idle":"2024-04-25T09:03:48.530437Z","shell.execute_reply":"2024-04-25T09:03:48.529502Z","shell.execute_reply.started":"2024-04-25T09:03:40.053813Z"},"trusted":true},"outputs":[],"source":["submission[col] =  0.23*pred1 + 0.15*pred2 + 0.13*pred3 + 0.15*pred4 + 0.20*pred5 + 0.14*pred6\n","df2 = submission.copy()"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T09:03:48.531874Z","iopub.status.busy":"2024-04-25T09:03:48.531586Z","iopub.status.idle":"2024-04-25T09:03:56.733353Z","shell.execute_reply":"2024-04-25T09:03:56.732578Z","shell.execute_reply.started":"2024-04-25T09:03:48.531849Z"},"trusted":true},"outputs":[],"source":["submission[col] = 0.17*pred1 + 0.16*pred2 + 0.17*pred3 + 0.16*pred4 + 0.18*pred5 + 0.16*pred6\n","df3 = submission.copy()"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T09:03:56.734807Z","iopub.status.busy":"2024-04-25T09:03:56.734518Z","iopub.status.idle":"2024-04-25T09:03:56.767362Z","shell.execute_reply":"2024-04-25T09:03:56.766488Z","shell.execute_reply.started":"2024-04-25T09:03:56.734782Z"},"trusted":true},"outputs":[],"source":["df_sub = 0.34*df1 + 0.33*df2 + 0.33*df3"]},{"cell_type":"markdown","metadata":{},"source":["## Save Submission Dataframe"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T09:03:56.768704Z","iopub.status.busy":"2024-04-25T09:03:56.768445Z","iopub.status.idle":"2024-04-25T09:04:07.961389Z","shell.execute_reply":"2024-04-25T09:04:07.960547Z","shell.execute_reply.started":"2024-04-25T09:03:56.768681Z"},"trusted":true},"outputs":[],"source":["df_sub.to_csv('submission.csv')"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T09:04:07.962816Z","iopub.status.busy":"2024-04-25T09:04:07.962524Z","iopub.status.idle":"2024-04-25T09:04:08.002049Z","shell.execute_reply":"2024-04-25T09:04:08.001214Z","shell.execute_reply.started":"2024-04-25T09:04:07.962790Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>A1BG</th>\n","      <th>A1BG-AS1</th>\n","      <th>A2M</th>\n","      <th>A2M-AS1</th>\n","      <th>A2MP1</th>\n","      <th>A4GALT</th>\n","      <th>AAAS</th>\n","      <th>AACS</th>\n","      <th>AAGAB</th>\n","      <th>AAK1</th>\n","      <th>...</th>\n","      <th>ZUP1</th>\n","      <th>ZW10</th>\n","      <th>ZWILCH</th>\n","      <th>ZWINT</th>\n","      <th>ZXDA</th>\n","      <th>ZXDB</th>\n","      <th>ZXDC</th>\n","      <th>ZYG11B</th>\n","      <th>ZYX</th>\n","      <th>ZZEF1</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.431220</td>\n","      <td>0.204304</td>\n","      <td>0.235907</td>\n","      <td>0.553206</td>\n","      <td>1.276552</td>\n","      <td>0.908929</td>\n","      <td>-1.207080e-02</td>\n","      <td>0.289966</td>\n","      <td>-0.085586</td>\n","      <td>0.379022</td>\n","      <td>...</td>\n","      <td>-0.211065</td>\n","      <td>0.047183</td>\n","      <td>-0.042464</td>\n","      <td>0.123361</td>\n","      <td>0.477829</td>\n","      <td>0.362787</td>\n","      <td>0.264231</td>\n","      <td>0.282896</td>\n","      <td>-0.276190</td>\n","      <td>0.040589</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.126678</td>\n","      <td>0.084022</td>\n","      <td>0.110098</td>\n","      <td>0.191143</td>\n","      <td>0.411099</td>\n","      <td>0.281977</td>\n","      <td>-2.764451e-02</td>\n","      <td>0.130964</td>\n","      <td>-0.040344</td>\n","      <td>0.125686</td>\n","      <td>...</td>\n","      <td>-0.066286</td>\n","      <td>0.013244</td>\n","      <td>-0.074683</td>\n","      <td>0.087673</td>\n","      <td>0.174893</td>\n","      <td>0.136845</td>\n","      <td>0.147960</td>\n","      <td>0.100047</td>\n","      <td>-0.096026</td>\n","      <td>-0.066539</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.453972</td>\n","      <td>0.205501</td>\n","      <td>0.245085</td>\n","      <td>0.578844</td>\n","      <td>1.445402</td>\n","      <td>0.941471</td>\n","      <td>-1.485819e-02</td>\n","      <td>0.240209</td>\n","      <td>0.000047</td>\n","      <td>0.306720</td>\n","      <td>...</td>\n","      <td>-0.109657</td>\n","      <td>0.081212</td>\n","      <td>-0.088158</td>\n","      <td>0.234756</td>\n","      <td>0.453756</td>\n","      <td>0.293844</td>\n","      <td>0.244291</td>\n","      <td>0.199781</td>\n","      <td>-0.174123</td>\n","      <td>-0.031649</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.089317</td>\n","      <td>0.069120</td>\n","      <td>0.085070</td>\n","      <td>0.148146</td>\n","      <td>0.304212</td>\n","      <td>0.213113</td>\n","      <td>-3.698963e-02</td>\n","      <td>0.117031</td>\n","      <td>-0.057705</td>\n","      <td>0.108603</td>\n","      <td>...</td>\n","      <td>-0.068455</td>\n","      <td>0.000701</td>\n","      <td>-0.066211</td>\n","      <td>0.064782</td>\n","      <td>0.148242</td>\n","      <td>0.119782</td>\n","      <td>0.136939</td>\n","      <td>0.091471</td>\n","      <td>-0.096721</td>\n","      <td>-0.066936</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.131595</td>\n","      <td>0.079445</td>\n","      <td>0.119654</td>\n","      <td>0.214682</td>\n","      <td>0.406764</td>\n","      <td>0.304155</td>\n","      <td>-4.093058e-02</td>\n","      <td>0.149494</td>\n","      <td>-0.065292</td>\n","      <td>0.167606</td>\n","      <td>...</td>\n","      <td>-0.095546</td>\n","      <td>-0.006776</td>\n","      <td>-0.036900</td>\n","      <td>0.067380</td>\n","      <td>0.172304</td>\n","      <td>0.152857</td>\n","      <td>0.173854</td>\n","      <td>0.128512</td>\n","      <td>-0.151712</td>\n","      <td>-0.048244</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>250</th>\n","      <td>0.061724</td>\n","      <td>0.021368</td>\n","      <td>-0.059040</td>\n","      <td>0.027223</td>\n","      <td>0.207163</td>\n","      <td>0.109053</td>\n","      <td>-1.895083e-02</td>\n","      <td>0.040701</td>\n","      <td>-0.075486</td>\n","      <td>0.057375</td>\n","      <td>...</td>\n","      <td>-0.067656</td>\n","      <td>-0.038587</td>\n","      <td>-0.128493</td>\n","      <td>-0.061276</td>\n","      <td>0.112277</td>\n","      <td>0.068606</td>\n","      <td>0.057628</td>\n","      <td>0.038314</td>\n","      <td>-0.090064</td>\n","      <td>-0.082396</td>\n","    </tr>\n","    <tr>\n","      <th>251</th>\n","      <td>0.201037</td>\n","      <td>0.056008</td>\n","      <td>-0.120832</td>\n","      <td>0.230680</td>\n","      <td>1.060937</td>\n","      <td>0.549361</td>\n","      <td>4.504498e-02</td>\n","      <td>0.088583</td>\n","      <td>-0.071042</td>\n","      <td>0.095157</td>\n","      <td>...</td>\n","      <td>-0.243144</td>\n","      <td>-0.136677</td>\n","      <td>-0.158875</td>\n","      <td>-0.253272</td>\n","      <td>0.342076</td>\n","      <td>0.079660</td>\n","      <td>0.025801</td>\n","      <td>0.055652</td>\n","      <td>-0.278458</td>\n","      <td>-0.005996</td>\n","    </tr>\n","    <tr>\n","      <th>252</th>\n","      <td>0.100357</td>\n","      <td>0.036045</td>\n","      <td>-0.068380</td>\n","      <td>0.081245</td>\n","      <td>0.430884</td>\n","      <td>0.234329</td>\n","      <td>-5.991821e-07</td>\n","      <td>0.065840</td>\n","      <td>-0.074628</td>\n","      <td>0.067284</td>\n","      <td>...</td>\n","      <td>-0.112692</td>\n","      <td>-0.051849</td>\n","      <td>-0.124976</td>\n","      <td>-0.101464</td>\n","      <td>0.180059</td>\n","      <td>0.066586</td>\n","      <td>0.045893</td>\n","      <td>0.044741</td>\n","      <td>-0.143338</td>\n","      <td>-0.065524</td>\n","    </tr>\n","    <tr>\n","      <th>253</th>\n","      <td>0.817711</td>\n","      <td>0.717661</td>\n","      <td>-1.944665</td>\n","      <td>0.675907</td>\n","      <td>3.911817</td>\n","      <td>2.452285</td>\n","      <td>3.335526e-01</td>\n","      <td>-0.077506</td>\n","      <td>-0.094431</td>\n","      <td>0.657112</td>\n","      <td>...</td>\n","      <td>-0.420474</td>\n","      <td>-0.160090</td>\n","      <td>-1.857235</td>\n","      <td>-0.448571</td>\n","      <td>0.793861</td>\n","      <td>0.078360</td>\n","      <td>-0.703141</td>\n","      <td>0.102289</td>\n","      <td>0.100296</td>\n","      <td>0.042162</td>\n","    </tr>\n","    <tr>\n","      <th>254</th>\n","      <td>0.241203</td>\n","      <td>0.074507</td>\n","      <td>-0.192164</td>\n","      <td>0.266639</td>\n","      <td>1.278402</td>\n","      <td>0.666074</td>\n","      <td>4.925980e-02</td>\n","      <td>0.087027</td>\n","      <td>-0.085354</td>\n","      <td>0.079783</td>\n","      <td>...</td>\n","      <td>-0.248821</td>\n","      <td>-0.132069</td>\n","      <td>-0.210158</td>\n","      <td>-0.259788</td>\n","      <td>0.424278</td>\n","      <td>0.104969</td>\n","      <td>-0.008336</td>\n","      <td>0.017144</td>\n","      <td>-0.278316</td>\n","      <td>-0.004139</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>255 rows × 18211 columns</p>\n","</div>"],"text/plain":["         A1BG  A1BG-AS1       A2M   A2M-AS1     A2MP1    A4GALT          AAAS  \\\n","id                                                                              \n","0    0.431220  0.204304  0.235907  0.553206  1.276552  0.908929 -1.207080e-02   \n","1    0.126678  0.084022  0.110098  0.191143  0.411099  0.281977 -2.764451e-02   \n","2    0.453972  0.205501  0.245085  0.578844  1.445402  0.941471 -1.485819e-02   \n","3    0.089317  0.069120  0.085070  0.148146  0.304212  0.213113 -3.698963e-02   \n","4    0.131595  0.079445  0.119654  0.214682  0.406764  0.304155 -4.093058e-02   \n","..        ...       ...       ...       ...       ...       ...           ...   \n","250  0.061724  0.021368 -0.059040  0.027223  0.207163  0.109053 -1.895083e-02   \n","251  0.201037  0.056008 -0.120832  0.230680  1.060937  0.549361  4.504498e-02   \n","252  0.100357  0.036045 -0.068380  0.081245  0.430884  0.234329 -5.991821e-07   \n","253  0.817711  0.717661 -1.944665  0.675907  3.911817  2.452285  3.335526e-01   \n","254  0.241203  0.074507 -0.192164  0.266639  1.278402  0.666074  4.925980e-02   \n","\n","         AACS     AAGAB      AAK1  ...      ZUP1      ZW10    ZWILCH  \\\n","id                                 ...                                 \n","0    0.289966 -0.085586  0.379022  ... -0.211065  0.047183 -0.042464   \n","1    0.130964 -0.040344  0.125686  ... -0.066286  0.013244 -0.074683   \n","2    0.240209  0.000047  0.306720  ... -0.109657  0.081212 -0.088158   \n","3    0.117031 -0.057705  0.108603  ... -0.068455  0.000701 -0.066211   \n","4    0.149494 -0.065292  0.167606  ... -0.095546 -0.006776 -0.036900   \n","..        ...       ...       ...  ...       ...       ...       ...   \n","250  0.040701 -0.075486  0.057375  ... -0.067656 -0.038587 -0.128493   \n","251  0.088583 -0.071042  0.095157  ... -0.243144 -0.136677 -0.158875   \n","252  0.065840 -0.074628  0.067284  ... -0.112692 -0.051849 -0.124976   \n","253 -0.077506 -0.094431  0.657112  ... -0.420474 -0.160090 -1.857235   \n","254  0.087027 -0.085354  0.079783  ... -0.248821 -0.132069 -0.210158   \n","\n","        ZWINT      ZXDA      ZXDB      ZXDC    ZYG11B       ZYX     ZZEF1  \n","id                                                                         \n","0    0.123361  0.477829  0.362787  0.264231  0.282896 -0.276190  0.040589  \n","1    0.087673  0.174893  0.136845  0.147960  0.100047 -0.096026 -0.066539  \n","2    0.234756  0.453756  0.293844  0.244291  0.199781 -0.174123 -0.031649  \n","3    0.064782  0.148242  0.119782  0.136939  0.091471 -0.096721 -0.066936  \n","4    0.067380  0.172304  0.152857  0.173854  0.128512 -0.151712 -0.048244  \n","..        ...       ...       ...       ...       ...       ...       ...  \n","250 -0.061276  0.112277  0.068606  0.057628  0.038314 -0.090064 -0.082396  \n","251 -0.253272  0.342076  0.079660  0.025801  0.055652 -0.278458 -0.005996  \n","252 -0.101464  0.180059  0.066586  0.045893  0.044741 -0.143338 -0.065524  \n","253 -0.448571  0.793861  0.078360 -0.703141  0.102289  0.100296  0.042162  \n","254 -0.259788  0.424278  0.104969 -0.008336  0.017144 -0.278316 -0.004139  \n","\n","[255 rows x 18211 columns]"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["df_sub"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T09:04:08.003394Z","iopub.status.busy":"2024-04-25T09:04:08.003126Z","iopub.status.idle":"2024-04-25T09:07:21.493932Z","shell.execute_reply":"2024-04-25T09:07:21.493122Z","shell.execute_reply.started":"2024-04-25T09:04:08.003370Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","25\n","50\n","75\n","100\n","125\n","150\n","175\n","200\n","225\n","250\n"]}],"source":["for index, compound_gene_pred in submission.iterrows():\n","    if index % 25 == 0:\n","        print(index)\n","    abs_compound_mean = abs(compound_gene_pred).mean()\n","    \n","    compound_gene_pred *= min(abs_compound_mean**0.6, 1)\n","    submission.loc[index] = compound_gene_pred  \n","    \n","    abs_compound_mean = abs(compound_gene_pred).mean()"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T09:52:37.783041Z","iopub.status.busy":"2024-04-25T09:52:37.782746Z","iopub.status.idle":"2024-04-25T09:52:38.490016Z","shell.execute_reply":"2024-04-25T09:52:38.489136Z","shell.execute_reply.started":"2024-04-25T09:52:37.783016Z"},"trusted":true},"outputs":[],"source":["submission *= 1.2"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":7010844,"sourceId":59094,"sourceType":"competition"},{"sourceId":143494741,"sourceType":"kernelVersion"}],"dockerImageVersionId":30588,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
